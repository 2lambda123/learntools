{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn what a **categorical variable** is, along with three approaches for handling this type of data.\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "A **categorical variable** takes only a limited number of values.  \n",
    "\n",
    "- Consider a survey that asks how often you eat breakfast and provides four options: \"Never\", \"Rarely\", \"Most days\", or \"Every day\".  In this case, the data is categorical, because responses fall into a fixed set of categories.\n",
    "- If people responded to a survey about which what brand of car they owned, the responses would fall into categories like \"Honda\", \"Toyota\", and \"Ford\".  In this case, the data is also categorical.\n",
    "\n",
    "You will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first.  In this tutorial, we'll compare three approaches that you can use to prepare your categorical data.\n",
    "\n",
    "# Three Approaches\n",
    "\n",
    "### 1) Drop Categorical Variables\n",
    "\n",
    "The easiest approach to dealing with categorical variables is to simply remove them from the dataset.  This approach will only work well if the columns did not contain useful information.\n",
    "\n",
    "### 2) Label Encoding\n",
    "\n",
    "**Label encoding** assigns each unique value to a different integer.\n",
    "\n",
    "![label encoding simple example](./images/tut2_labelencode.png)\n",
    "\n",
    "This approach assumes an ordering of the categories: \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\n",
    "\n",
    "This assumption makes sense in this example, because there is an indisputable ranking to the categories.  Not all categorical variables have a clear ordering in the values, but we refer to those that do as **ordinal variables**.  For tree-based models (like decision trees and random forests), you can expect label encoding to work well with ordinal variables. \n",
    "\n",
    "### 3) One-Hot Encoding\n",
    "\n",
    "**One-hot encoding** creates new columns indicating the presence (or absence) of each possible value in the original data.  To understand this, we'll work through an example.\n",
    "\n",
    "![one-hot encoding simple example](./images/tut2_onehot.png)\n",
    "\n",
    "In the original dataset, \"Color\" is a categorical variable with three categories: \"Red\", \"Yellow\", and \"Green\".  The corresponding one-hot encoding contains one column for each possible value, and one row for each row in the original dataset.  Wherever the original value was \"Red\", we put a 1 in the \"Red\" column; if the original value was \"Yellow\", we put a 1 in the \"Yellow\" column, and so on.  \n",
    "\n",
    "In contrast to label encoding, one-hot encoding *does not* assume an ordering of the categories.  Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., \"Red\" is neither _more_ nor _less_ than \"Yellow\").  We refer to categorical variables without an intrinsic ranking as **nominal variables**.\n",
    "\n",
    "One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values). \n",
    "\n",
    "# Example\n",
    "\n",
    "We'll work with a dataset containing housing characteristics and skip the basic data set-up code.  The end result is:\n",
    "- The housing characteristics for the training data are stored in a DataFrame `train_predictors`.  We'll use it to predict home prices in a Series called `target`.  \n",
    "- The housing characteristics for the test data are stored in a DataFrame `test_predictors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "#$HIDE$\n",
    "import pandas as pd\n",
    "\n",
    "# read the data\n",
    "train_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "test_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n",
    "\n",
    "# separate predictors from target\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "target = train_data.SalePrice\n",
    "\n",
    "# drop columns with missing values (simplest approach)\n",
    "cols_with_missing = [col for col in train_data.columns \n",
    "                                 if train_data[col].isnull().any()]                                  \n",
    "candidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n",
    "candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n",
    "\n",
    "# \"cardinality\" means the number of unique values in a column.\n",
    "# select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].nunique() < 10 and\n",
    "                                candidate_train_predictors[cname].dtype == \"object\"]\n",
    "\n",
    "# select numeric columns\n",
    "numeric_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# define train and test predictors with selected columns\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "train_predictors = candidate_train_predictors[my_cols]\n",
    "test_predictors = candidate_test_predictors[my_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a peek at the training predictors with the `head` method below.  Many of the first several columns in the DataFrame are categorical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we obtain a list of all of the categorical variables in the training data.\n",
    "\n",
    "We do this by checking the data type (or **dtype**) of each column.  The `object` dtype indicates a column has text (there are other things it could theoretically be, but that's unimportant for our purposes).  For this dataset, the columns with text indicate categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of categorical variables\n",
    "s = (train_predictors.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Measure Quality of Each Approach\n",
    "\n",
    "We define a function `score_dataset_cv` to compare the three different approaches to dealing with categorical variables. This function reports the [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) from a random forest model.  Better approaches will have lower MAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE$\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def score_dataset_cv(X, y):\n",
    "    # multiply by -1 to get positive MAE score instead of neg value returned as sklearn convention\n",
    "    # (you'll learn about cross-validation in a later tutorial)\n",
    "    return -1 * cross_val_score(RandomForestRegressor(50, random_state=0), \n",
    "                                X, y, \n",
    "                                scoring = 'neg_mean_absolute_error').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score from Approach 1 (Drop Categorical Variables)\n",
    "\n",
    "We drop the `object` columns with the [`select_dtypes`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html) method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "drop_train_predictors = train_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset_cv(drop_train_predictors, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score from Approach 2 (Label Encoding)\n",
    "\n",
    "Scikit-learn has a [`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) class that can be used to get label encodings.  We loop over the categorical variables and apply the label encoder separately to each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# make copy to avoid changing original data \n",
    "label_train_predictors = train_predictors.copy()\n",
    "\n",
    "# apply label encoder to each column with categorical data\n",
    "for col in object_cols:\n",
    "    label_train_predictors[col] = LabelEncoder().fit_transform(train_predictors[col])\n",
    "\n",
    "print(\"MAE from Approach 2 (Label Encoding):\") \n",
    "print(score_dataset_cv(label_train_predictors, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score from Approach 3 (One-Hot Encoding)\n",
    "\n",
    "Pandas offers a convenient function called [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) to get one-hot encodings.  Note that we can pass the entire `train_predictors` DataFrame to the function, which automatically detects and encodes the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "# one-hot encode categorical data\n",
    "OH_train_predictors = pd.get_dummies(train_predictors)\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset_cv(OH_train_predictors, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, since the returned MAE scores are close in value, there doesn't appear to be any meaningful benefit to one approach over the other.\n",
    "\n",
    "In general, one-hot encoding (**Approach 3**) will typically perform best, and dropping the categorical columns (**Approach 1**) typically performs worst, but it varies on a case-by-case basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Note: Applying to Multiple Files\n",
    "\n",
    "So far, you've one-hot-encoded your training data.  What about when you have multiple files (e.g., a test dataset, or some other data that you'd like to use to make predictions)?  \n",
    "\n",
    "Scikit-learn is sensitive to the ordering of columns, so if the training dataset and test datasets get misaligned, your results will be nonsense.  This could happen if a categorical variable had a different number of values in the training data vs the test data.  Thankfully, we can quickly ensure the test data is encoded in the same manner as the training data with the `align` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OH_test_predictors = pd.get_dummies(test_predictors)\n",
    "final_train, final_test = OH_train_predictors.align(OH_test_predictors,\n",
    "                                                    join='left', \n",
    "                                                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `align` command makes sure the columns show up in the same order in both datasets: it uses column names to identify which columns line up in each dataset.  \n",
    "- The argument `join='left'` ensures that if there are columns that show up in one dataset and not the other, we will keep exactly the columns from our training data. \n",
    "- The argument `join='inner'` keeps only the columns that appear in both datasets. That's also a sensible choice.\n",
    "\n",
    "If you're familiar with [SQL](https://en.wikipedia.org/wiki/SQL), you may have noticed that `join=\"left\"` is the equivalent of SQL's left join, and `join='inner'` is the equivalent of SQL's inner join.  If you are not familiar with SQL, **_after completing this mini-course_**, you can learn more through [this mini-course](https://www.kaggle.com/learn/sql)!\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "The world is filled with categorical data. You will be a much more effective data scientist if you know how to use this common data type!\n",
    "\n",
    "# Your Turn\n",
    "\n",
    "hi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
