{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn what a **categorical variable** is, along with the most common approach for handling this type of data.\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "**Categorical data** is data that takes only a limited number of values.  \n",
    "\n",
    "For example, if you people responded to a survey about which what brand of car they owned, the result would be categorical (because the answers would be things like _Honda_,  _Toyota_, _Ford_, _None_, etc.). Responses fall into\n",
    "a fixed set of categories.\n",
    "\n",
    "You will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first.  In this tutorial, we'll explore the most popular method for handling categorical variables.\n",
    "\n",
    "# One-Hot Encoding: The Standard Approach for Categorical Data\n",
    "\n",
    "**One hot encoding** is the most widespread approach for categorical data.  It creates new (binary) columns, indicating the presence of each possible value in the original data.  To understand this, we'll work through an example.\n",
    "\n",
    "![One-hot encoding simple example](https://i.imgur.com/mtimFxh.png)\n",
    "\n",
    "The values in the original data are _Red_, _Yellow_, and _Green_.  The corresponding one-hot encoding contains separate columns for each possible value.  Wherever the original value was _Red_, we put a 1 in the _Red_ column; if the original value was _Yellow_, we put a 1 in the _Yellow_ column, and so on.  \n",
    "\n",
    "One-hot encoding works very well, unless your categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values).\n",
    "\n",
    "# Calculating One-Hot Encodings\n",
    "\n",
    "Let's see this in code.  We'll work with a dataset containing housing characteristics and skip the basic data set-up code.  The end result is:\n",
    "- The housing characteristics for the training data are stored in a DataFrame `train_predictors`.  We'll use it to predict home prices in a Series called `target`.  \n",
    "- The housing characteristics for the test data are stored in a DataFrame `test_predictors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the data\n",
    "train_data = pd.read_csv('../input/train.csv')\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "\n",
    "# separate predictors from target\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "target = train_data.SalePrice\n",
    "\n",
    "# drop columns with missing values (simplest approach)\n",
    "cols_with_missing = [col for col in train_data.columns \n",
    "                                 if train_data[col].isnull().any()]                                  \n",
    "candidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n",
    "candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n",
    "\n",
    "# \"cardinality\" means the number of unique values in a column.\n",
    "# select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].nunique() < 10 and\n",
    "                                candidate_train_predictors[cname].dtype == \"object\"]\n",
    "\n",
    "# select numeric columns\n",
    "numeric_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# define train and test predictors with selected columns\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "train_predictors = candidate_train_predictors[my_cols]\n",
    "test_predictors = candidate_test_predictors[my_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas assigns a data type (called a dtype) to each column or Series.  Let's see a random sample of dtypes from our prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_predictors.dtypes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Object** indicates a column has text (there are other things it could be theoretically be, but that's unimportant for our purposes). It's most common to one-hot encode these \"object\" columns, since they can't be plugged directly into most models.  \n",
    "\n",
    "Pandas offers a convenient function called [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) to get one-hot encodings. Call it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "\n",
    "# print the first five rows of the one-hot encoding\n",
    "one_hot_encoded_training_predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Approaches?\n",
    "\n",
    "Alternatively, we could have dropped the columns with categorical data. To see how the approaches compare, we can calculate the mean absolute error (MAE) of models built with two alternative sets of predictors:\n",
    "1. One-hot encoded categoricals as well as numeric predictors\n",
    "2. Numerical predictors, where we drop categoricals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def get_mae(X, y):\n",
    "    # multiply by -1 to get positive MAE score instead of neg value returned as sklearn convention\n",
    "    return -1 * cross_val_score(RandomForestRegressor(50), \n",
    "                                X, y, \n",
    "                                scoring = 'neg_mean_absolute_error').mean()\n",
    "\n",
    "predictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "mae_without_categoricals = get_mae(predictors_without_categoricals, target)\n",
    "\n",
    "mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n",
    "\n",
    "print('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\n",
    "print('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding usually helps, but it varies on a case-by-case basis.  In this case, there doesn't appear to be any meaningful benefit from using the one-hot encoded variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying to Multiple Files\n",
    "\n",
    "So far, you've one-hot-encoded your training data.  What about when you have multiple files (e.g., a test dataset, or some other data that you'd like to use to make predictions)?  \n",
    "\n",
    "Scikit-learn is sensitive to the ordering of columns, so if the training dataset and test datasets get misaligned, your results will be nonsense.  This could happen if a categorical variable had a different number of values in the training data vs the test data.  Thankfully, we can quickly ensure the test data is encoded in the same manner as the training data with the `align` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n",
    "final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n",
    "                                                                    join='left', \n",
    "                                                                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `align` command makes sure the columns show up in the same order in both datasets: it uses column names to identify which columns line up in each dataset.  \n",
    "- The argument `join='left'` specifies that we will do the equivalent of SQL's _left join_.  In this case, if there are columns that show up in one dataset and not the other, we will keep exactly the columns from our training data. \n",
    "- The argument `join='inner'` would do the equivalent of SQL's _inner join_, keeping only the columns that appear in both datasets.  That's also a sensible choice.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "The world is filled with categorical data. You will be a much more effective data scientist if you know how to use this common data type!\n",
    "\n",
    "# Your Turn\n",
    "\n",
    "hi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
