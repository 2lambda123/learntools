{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn three approaches to dealing with missing values. You will then learn to compare the effectiveness of these approaches on any given dataset.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "There are many ways data can end up with missing values. For example,\n",
    "- A 2 bedroom house won't include a value for the size of a third bedroom.\n",
    "- A survey respondent may choose to not share his income.\n",
    "\n",
    "Most libraries (_including scikit-learn_) will give you an error if you try to build a model using data with missing values. So you'll need to choose one of the strategies below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Approaches\n",
    "\n",
    "\n",
    "### 1) A Simple Option: Drop Columns with Missing Values\n",
    "\n",
    "A simple option is to drop columns with missing values. If your data is in a DataFrame called `original_data`, you can do that as follows:\n",
    "\n",
    "```python\n",
    "reduced_data = original_data.dropna(axis=1)\n",
    "```\n",
    "\n",
    "In many cases, you'll have both a training dataset (`original_train_data`) and a test dataset (`original_test_data`).  You will want to drop the same columns in both DataFrames. In that case, you would write:\n",
    "\n",
    "```python\n",
    "# get names of columns with missing values\n",
    "cols_with_missing = [col for col in original_data.columns \n",
    "                                 if original_data[col].isnull().any()]\n",
    "# drop columns in train and test DataFrames\n",
    "reduced_train_data = original_train_data.drop(cols_with_missing, axis=1)\n",
    "reduced_test_data = original_test_data.drop(cols_with_missing, axis=1)\n",
    "```\n",
    "\n",
    "If those columns had useful information _in the entries that were not missing_, your model loses access to this information when the column is dropped. Also, if your test data has missing values in places where your training data did not, this will result in an error.  \n",
    "\n",
    "So, it's usually not the best solution. However, it can be useful when most values in a column are missing.\n",
    "\n",
    "### 2) A Better Option: Imputation\n",
    "\n",
    "**Imputation** fills in the missing values with some number. This approach usually gives more accurate models than dropping columns entirely, and it is demonstrated in the pseudocode below:\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_data = my_imputer.fit_transform(original_data)\n",
    "```\n",
    "\n",
    "The default behavior fills in the mean value for imputation.  Statisticians have researched more complex strategies, but those complex strategies typically give no benefit once you plug the results into sophisticated machine learning models.\n",
    "\n",
    "### 3) An Extension To Imputation\n",
    "\n",
    "Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing.  Here's how it might look:\n",
    "\n",
    "```python\n",
    "# make copy to avoid changing original data (when imputing)\n",
    "new_data = original_data.copy()\n",
    "\n",
    "# make new columns indicating what will be imputed\n",
    "cols_with_missing = (col for col in new_data.columns \n",
    "                                 if new_data[col].isnull().any())\n",
    "for col in cols_with_missing:\n",
    "    new_data[col + '_was_missing'] = new_data[col].isnull()\n",
    "\n",
    "# imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_data = pd.DataFrame(my_imputer.fit_transform(new_data))\n",
    "imputed_data.columns = original_data.columns\n",
    "```\n",
    "\n",
    "In some cases this approach will meaningfully improve results. In other cases, it doesn't help at all.\n",
    "\n",
    "# Example (Comparing All Solutions)\n",
    "\n",
    "We will see an example predicting housing prices from the Melbourne Housing data.  \n",
    "\n",
    "### Set-up\n",
    "\n",
    "`melb_target`, `melb_numeric_predictors`\n",
    "\n",
    "We divide our data into training and test subsets with [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the data\n",
    "melb_data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n",
    "\n",
    "# some comment here\n",
    "melb_target = melb_data.Price\n",
    "\n",
    "# to keep things simple, we'll use only numeric predictors\n",
    "melb_predictors = melb_data.drop(['Price'], axis=1)\n",
    "melb_numeric_predictors = melb_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "# divide data into training and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, \n",
    "                                                    melb_target,\n",
    "                                                    train_size=0.7, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Function to Measure Quality of Each Approach\n",
    "\n",
    "We define a function `score_dataset` to compare different approaches to dealing with missing values. This function reports the [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) from a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# function for comparing different approaches\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score from Approach 1 (Dropping Columns with Missing Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns \n",
    "                                 if X_train[col].isnull().any()]\n",
    "\n",
    "# drop columns in train and test DataFrames\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_test  = X_test.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print(\"MAE from Approach 1 (Dropping columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score from Approach 2 (Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = my_imputer.fit_transform(X_train)\n",
    "imputed_X_test = my_imputer.transform(X_test)\n",
    "\n",
    "print(\"MAE from Approach 2 (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score from Approach 3 (An Extension to Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy to avoid changing original data (when imputing)\n",
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "\n",
    "# make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n",
    "    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n",
    "\n",
    "# imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\n",
    "imputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n",
    "\n",
    "print(\"MAE from Approach 3 (An Extension to Imputation):\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "As is common, imputing missing values (in **Approach 2**) yielded better results, relative to when we simply dropped columns with missing values (in **Approach 1**).  We got an additional boost by tracking what values had been imputed (in **Approach 3**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep Going\n",
    "Once you've added the Imputer and included columns with missing values, you are ready to [add categorical variables](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding), which is non-numeric data representing categories (like the name of the neighborhood a house is in)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
