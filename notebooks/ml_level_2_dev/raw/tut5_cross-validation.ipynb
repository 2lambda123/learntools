{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn how to use **cross-validation** for better measures of model performance.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Machine learning is an iterative process. \n",
    "\n",
    "You will face choices about what predictive variables to use, what types of models to use, what arguments to supply to those models, etc. We make these choices in a data-driven way by measuring model quality of various alternatives.  \n",
    "\n",
    "# Shortcomings of Train-Test Split\n",
    "\n",
    "Imagine you have a dataset with 5000 rows.  The `train_test_split` function has an argument for `test_size` that you can use to decide how many rows go to the training set and how many go to the test set. \n",
    "\n",
    "You will typically keep about 20% of the data as a test dataset (`test_size = 0.2`).  But even with 1000 rows in the test set, there's some random chance in determining model scores.  A model might do well on one set of 1000 rows, even if it would be inaccurate on a different 1000 rows.  The larger the test set, the less randomness (aka \"noise\") there is in our measure of model quality (and the more reliable it will be!).\n",
    "\n",
    "At an extreme, you could imagine having only 1 row of data in the test set.  If you compare alternative models, which one makes the best predictions on a single data point will be mostly a matter of luck.  \n",
    "\n",
    "But we can only get a large test set by removing data from our training data, and smaller training datasets mean worse models.  In fact, the ideal modeling decisions on a small dataset typically aren't the best modeling decisions on large datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is cross-validation?\n",
    "\n",
    "In **cross-validation**, we run our modeling process on different subsets of the data to get multiple measures of model quality. \n",
    "\n",
    "For example, we could begin by dividing the data into 5 pieces, each being 20% of the full dataset.  In this case, we say that we have broken the data into 5 \"**folds**\".  \n",
    "\n",
    "![cross-validation-graphic](https://i.stack.imgur.com/1fXzJ.png)\n",
    "\n",
    "Procedure:\n",
    "- We run an experiment called experiment 1 which uses the first fold as a holdout set, and everything else as training data. This gives us a measure of model quality based on a 20% holdout set, much as we got from using the simple train-test split.  \n",
    "- We then run a second experiment, where we hold out data from the second fold (using everything except the 2nd fold for training the model.) This gives us a second estimate of model quality.\n",
    "- We repeat this process, using every fold once as the holdout.  Putting this together, 100% of the data is used as a holdout at some point.  \n",
    "\n",
    "Returning to our example above from train-test split, if we have 5000 rows of data, we end up with a measure of model quality based on 5000 rows of holdout (even if we don't use all 5000 rows simultaneously).\n",
    "\n",
    "# Trade-offs Between Cross-Validation and Train-Test Split\n",
    "\n",
    "Cross-validation gives a more accurate measure of model quality, which is especially important if you are making a lot of modeling decisions.  However, it can take longer to run, because it estimates multiple models (one for each **fold**).  \n",
    "\n",
    "So, given these tradeoffs, when should you use each approach?\n",
    "- _For small datasets_, where extra computational burden isn't a big deal, you should run cross-validation.\n",
    "- _For larger datasets_, a simple train-test split is sufficient.  It will run faster, and you may have enough data that there's little need to re-use some of it for holdout.\n",
    "\n",
    "There's no simple threshold for what constitutes a large vs. small dataset.  But if your model takes a couple minutes or less to run, it's probably worth switching to cross-validation.  \n",
    "\n",
    "Alternatively, you can run cross-validation and see if the scores for each experiment seem close.  If each experiment yields the same results, a train-test split is probably sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "We'll work with the same data as in the previous tutorial.  We load the input data in `X` and the output data in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the data\n",
    "data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n",
    "\n",
    "# select subset of predictors\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "\n",
    "# select target\n",
    "y = data.Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a **pipeline** that uses an **imputer** to fill in missing values and a **random forest** model to make predictions.  \n",
    "\n",
    "While it's _possible_ to do cross-validation without pipelines, it is quite difficult!  Using a pipeline will make the code remarkably straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = make_pipeline(SimpleImputer(), RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the cross-validation scores with just a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(my_pipeline, X, y,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores:\")\n",
    "# multiply by -1 (since scikit-learn calculates *negative* MAE)\n",
    "print(-1 * scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify an argument for the `scoring` parameter to choose a measure of model quality to report: in this case, we chose negative mean absolute error (MAE).  The docs for scikit-learn show a [list of options](http://scikit-learn.org/stable/modules/model_evaluation.html).  \n",
    "\n",
    "It is a little surprising that we specify *negative* MAE. Scikit-learn has a convention where all metrics are defined so a high number is better.  Using negatives here allows them to be consistent with that convention, though negative MAE is almost unheard of elsewhere. \n",
    "\n",
    "We typically want a single measure of model quality to compare alternative models.  So we take the average across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average MAE score (across experiments):\")\n",
    "print(-1 * scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Using cross-validation yields a much better measure of model quality, with the added benefit of cleaning up our code: note that we no longer needing to keep track of separate train and test sets.  So, especially for small datasets, it's a good improvement!\n",
    "\n",
    "# Your Turn\n",
    "\n",
    "hi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
