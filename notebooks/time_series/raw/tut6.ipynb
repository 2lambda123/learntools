{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forced-meeting",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In this lesson, we'll learn how to address some of the unique challenges that come with forecasting.\n",
    "\n",
    "In this lesson, we'll learn how to define and evaluate forecasts.\n",
    "\n",
    "- *Electricity Demand*\n",
    "\n",
    "# Defining the Forecasting Goal #\n",
    "\n",
    "Let's review some terminology commonly used in forecasting.\n",
    "\n",
    "- why did we wait until now? now we have the ability to make good forecasts, so now let's learn how to evaluate them\n",
    "- be more precise about what defines a forecast\n",
    "\n",
    "- Forecast Origin\n",
    "  - time the forecast is made\n",
    "  - generally, the final point in the training period\n",
    "- Lead Time\n",
    "  - how long until the forecast starts\n",
    "  - aka: gap, delay, embargo\n",
    "  - forecasts with a lead time greater than one step are called multi-step *ahead* forecasts\n",
    "- Forecast Horizon\n",
    "  - the time period for which forecasts are made\n",
    "  - aka: test period\n",
    "- One-step Forecast and Multi-step Forecast\n",
    "  - how many time steps are forecast for a given origin\n",
    "  - the length of the forecast horizon\n",
    "\n",
    "<img>forecast definition diagram</img>\n",
    "\n",
    "We need to understand the circumstances of the problem. What is the goal? What are the constraints?\n",
    "\n",
    "Note that there is a difference between a single multi-step forecast and multiple single-step forecasts.\n",
    "\n",
    "<img>single-step / multi-step table</img>\n",
    "\n",
    "The forecast horizon (the number of steps) is equal to the number of outputs the model should produce.\n",
    "\n",
    "A multi-step forecast means you predict values for multiple times in the future all from the same forecast origin -- each row in the dataframe has several target values.\n",
    "\n",
    "Some machine learning algorithms are capable of producing multiple outputs (including linear regression and neural nets), but there are workarounds for those that can't. You might start with scikit-learn's [`MultiOutputRegressor`](https://scikit-learn.org/stable/modules/multiclass.html#multioutput-regression), but also see [this discussion](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/151927) from the *M5 Forecasting* competition on the so-called recursive and direct methods.\n",
    "\n",
    "# Cross Validation #\n",
    "\n",
    "What we really care about is how our model performs on *new* data (that is, the out-of-sample or test data).\n",
    "\n",
    "Performance evaluation is an essential part of the machine learning workflow. Two techniques you might be familiar with are *the hold-out method* and *cross-validation*. (For a refresher, see our [lesson on cross-validation](https://www.kaggle.com/alexisbcook/cross-validation) in the *Intermediate ML* course.) We can use both of these evaluation techniques in forecasting, but the time- and serial-dependence in time series means we have to use some special care.\n",
    "\n",
    "How you evaluate a machine learning model should reflect, as much as possible, how you intend to use it. This is the basic principal of model evaluation. The biggest trouble with time series is that their statistical properties can be time dependent: the very thing that your model is trying to learn can shift over time.\n",
    "\n",
    "Whichever method you're using, in these cases it's best to have the training set come chronologically prior to the validation set.\n",
    "\n",
    "When using the hold-out method, the strategy is to choose a validation set from a time that resembles as much as possible the forecasting period. If you had a 30-day forecast horizon, you might choose for a hold-out validation set the 30 days just before the horizon. On the other hand, if your forecast horizon was the last two weeks of December (holiday season in the many parts of the world), it might be better to use the last two weeks of December from the previous year.\n",
    "\n",
    "<img>time series hold-out</img>\n",
    "\n",
    "Cross-validation is similar, with a fixed-size validation set and a training set made of prior data. The training set can either take the form of a *rolling window* or an *expanding* window:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/NFnQLwe.png\" width=400, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "You could use a rolling window to see how the model changes as the training data changes, and an expanding window to see how robust your model is to overfitting due to insufficient training data.\n",
    "\n",
    "Time-series cross-validation of this form is sometimes called **backtesting**, especially when the performance is considered over time instead of averaged together into a single number. Backtesting a forecasting model can help you understand how it performs as the behavior of the time series evolves or during extraordinary events. You might backtest forecasts for a utility-demand model to see how it performs under severe weather conditions or as the service population grows.\n",
    "\n",
    "<img>backtesting</img>\n",
    "\n",
    "<note>Delete the next paragraph?</note>\n",
    "Time series that are *not* time dependent, whose statistical properties stay the same, are known as **stationary** series. With stationary series, there is much less to worry about. You can even use a validation set that comes prior to your training set since, statistically, the data behaves the same way at all times. The only trouble with stationary series is that they can still have serial dependence. For technical reasons, this puts your error estimates at risk of being too optimistic. It turns out though that with a sufficiently powerful forecasting model even this isn't a danger. (We'll talk about how to create such models in the next lesson.)\n",
    "\n",
    "In summary:\n",
    "- In general, order your training and validation splits *chronologically*: training data first, validation data second.\n",
    "- The *hold-out* strategy is a good default. Try to choose a validation set that looks like the forecasting period -- the time period just before, perhaps.\n",
    "- Use *Time-series cross-validation* or *backtesting* to test how your model behaves under different historical conditions.\n",
    "\n",
    "<blockquote>\n",
    "It's worth noting that most top-ranking competitors in Kaggle's recent forecasting competitions have used the hold-out method, which suggests that just using a hold-out validation set should be fine if you're only making a single forecast.\n",
    "</blockquote>\n",
    "\n",
    "# Example - Electricity Demand #\n",
    "\n",
    "The *Electricity Demand* dataset contains hourly demand for electricity.\n",
    "\n",
    "The hidden cell defines some utility functions from the previous lessons and loads the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-visiting",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "from pathlib import Path\n",
    "from warnings import simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.deterministic import (CalendarFourier,\n",
    "                                           DeterministicProcess)\n",
    "\n",
    "simplefilter(\"ignore\")\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True, figsize=(11, 5))\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# Load data\n",
    "data_dir = Path(\"../input/ts-course-data\")\n",
    "elecdemand = pd.read_csv(data_dir / \"elecdemand.csv\", parse_dates=[\"Datetime\"])\n",
    "elecdemand = elecdemand.set_index(\"Datetime\").to_period(\"H\")\n",
    "\n",
    "# Create features\n",
    "\n",
    "# Data is hourly. There are 168 hours per week, so `fourier` creates\n",
    "# about half as many features (42 * 2) as indicators would (168 - 1).\n",
    "fourier = CalendarFourier(freq=\"W\", order=42)\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "    index=elecdemand.index,\n",
    "    constant=True,               # level\n",
    "    order=2,                     # trend (order 1 means linear)\n",
    "    seasonal=True,               # daily seasonality (indicators)\n",
    "    additional_terms=[fourier],  # weekly seasonality (fourier)\n",
    "    drop=True,                   # drop terms to avoid collinearity\n",
    ")\n",
    "\n",
    "X = dp.in_sample()\n",
    "y = elecdemand.Demand.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-least",
   "metadata": {},
   "source": [
    "First we'll use holdout validation.\n",
    "\n",
    "We can use `train_test_split` from scikit-learn to create our data splits. It's important to set `shuffle=False` or else the test set will be sampled at random dates instead of taken as a continuous block at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=24 * 14,  # 14 days\n",
    "    shuffle=False,      # time series should not be shuffled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-wholesale",
   "metadata": {},
   "source": [
    "Now we'll create the predictions and look at the train and test error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_fit = pd.Series(\n",
    "    model.predict(X_train),\n",
    "    index=X_train.index,\n",
    ")\n",
    "y_pred = pd.Series(\n",
    "    model.predict(X_test),\n",
    "    index=y_test.index,\n",
    ")\n",
    "\n",
    "train_rmse = mean_squared_error(y_train, y_fit, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print((f\"Train RMSE: {train_rmse:.2f}\\n\" f\"Test RMSE: {test_rmse:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-metallic",
   "metadata": {},
   "source": [
    "With timeseries validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "\n",
    "\n",
    "def rollingcv(n, train_size, test_size, gap=0):\n",
    "    n_splits = (n - train_size - gap) // test_size\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits,\n",
    "                         max_train_size=train_size,\n",
    "                         gap=gap,\n",
    "                         test_size=test_size)\n",
    "    return cv\n",
    "\n",
    "\n",
    "cv = rollingcv(\n",
    "    n=len(y),\n",
    "    train_size=24*28,  # 28 days\n",
    "    test_size=24*14,  # 14 days\n",
    "    gap=14  # 1 day\n",
    ")\n",
    "\n",
    "cv_rmse = (-1) * cross_val_score(\n",
    "    LinearRegression(fit_intercept=False),\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    ")\n",
    "cv_rmse = np.sqrt(cv_rmse.mean())\n",
    "\n",
    "print(\"Backtest RMSE: \", cv_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-electronics",
   "metadata": {},
   "source": [
    "Forecasting future demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit model to entire training set\n",
    "model.fit(X, y)\n",
    "\n",
    "# create features for forecast\n",
    "X_oos = dp.out_of_sample(steps=24 * 14)\n",
    "\n",
    "y_forecast = pd.Series(\n",
    "    model.predict(X_oos),\n",
    "    index=X_oos.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-findings",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ax = y.plot()\n",
    "_ = y_forecast.plot(ax=ax, color='C3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-implement",
   "metadata": {},
   "source": [
    "# Your Turn #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
