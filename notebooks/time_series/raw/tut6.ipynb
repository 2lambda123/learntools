{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "voluntary-envelope",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction #\n",
    "\n",
    "Now we come to our final lesson of the course -- on *multivariate* time series. In previous lessons, you learned the basics of forecasting a single time series. In this lesson, you'll learn how to make forecasts on *collections* of time series.\n",
    "\n",
    "Forecasting strategies pt. 2:\n",
    "- stacking forecasters\n",
    "- multioutput, direct, recursive\n",
    "\n",
    "# Stacking Forecasters and Examining Residuals #\n",
    "\n",
    "We'll look at a kind of ensembling method called \"stacking\" that's especially common in forecasting.\n",
    "\n",
    "Let's review what the residuals are.\n",
    "\n",
    "The residuals are what you get when you subtract out the model's predictions of the target from the target itself, during the training period.\n",
    "\n",
    "- subtract from the target the values the model learned to predict\n",
    "- tells you how well the model fits the data in the training sample\n",
    "- can show you if the model is \"underspecified\" or not flexible enough\n",
    "- can show you if your model failed to learn any patterns or relationships in the training data\n",
    "- we look for residuals that resemble \"white noise\"\n",
    "\n",
    "Because of the complex and layered behavior in many real-world time series, *ensembling* has become especially common in forecasting.\n",
    "\n",
    "- a variation of the stacking method\n",
    "- training a second model on the residuals (or \"leftover part\") of the first\n",
    "\n",
    "Reasons and advantages:\n",
    "- capture more kinds of relationships\n",
    "- combine the strengths of several models\n",
    "\n",
    "Linear regression makes up for XGBoost's lack of extrapolation ability, while XGBoost makes up for linear regression's lack of non-linearity and deep interactions. With stacking, we can have the best of both.\n",
    "\n",
    "<blockquote>\n",
    "Several variations of stacking forecasters have appeared in past competitions.\n",
    "- M4 - exponential smoothing + neural nets (1st)\n",
    "- Restaurant (?) - linear regression + XGBoost (?st)\n",
    "- (???) - ARIMA + XGBoost\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-telling",
   "metadata": {},
   "source": [
    "Ensembles of decision trees (like `RandomForest` and `XGBoost`) excel at capturing nonlinear behavior and interactions. Decision trees, however, make predictions through *interpolation* -- they predict new values through averages. This means that they fail at *extrapolation*, making predictions for data outside the range of the training set.\n",
    "\n",
    "<img>xgboost failing to model trend</img>\n",
    "\n",
    "We can overcome this limitation by combining a tree-based model with a trend or seasonal model of the kinds we saw in Lessons 2 and 3.\n",
    "\n",
    "<img>detrending and learning residuals</img>\n",
    "\n",
    "Using different models to capture the different parts of a time series is especially common in forecasting.\n",
    "- M4 Competition winner (Smyl 2019, from Uber): combination of exponential smoothing and LSTM\n",
    "\n",
    "# Local Models and Global Models #\n",
    "\n",
    "Many real world datasets comprise hundreds or thousands of interelated time series. Websites keep records of the number of visits for each page. Retail companies keep records of the number of sales per store or per item. While we could just forecast each series individually (like we've learned already), it would be better if we could also take advantage of relationships among the series somehow.\n",
    "\n",
    "<img>hierarchy of series</img>\n",
    "\n",
    "In a multivariate setting, models that only forecast a single series at a time are called **local** models, while those applied to the entire collection are called **global** models.\n",
    "\n",
    "Successful approaches to multivariate forecasting often combine local and global models. The idea is that the local models will be more successful at capturing whatever makes each series unique, while the global model can capture relationships among the series. Similar to the stacked model we saw in Lesson 3, the approach we'll take in this course will be to use simple linear regression for the local models and a tree ensemble for the global model.\n",
    "\n",
    "<img>local + global</img>\n",
    "\n",
    "- *Wiki trends*\n",
    "\n",
    "- *Australian Arrivals*\n",
    "\n",
    "# Forecasting Strategies #\n",
    "\n",
    "- multioutput\n",
    "- direct\n",
    "- recursive\n",
    "- combinations\n",
    "\n",
    "# Example - Avocado Sales #\n",
    "\n",
    "The *Avocado Sales* dataset contains several years of weekly sales data for three kinds of avocado. It includes categorical features like the region of sale and whether the avocados were organic or not. We'll predict the volume of sales for each of the three varieties of avocado, both organic and conventional. This gives us six series in total for which we'll make forecasts.\n",
    "\n",
    "The hidden cell loads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "from pathlib import Path\n",
    "from warnings import simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "from statsmodels.tsa.deterministic import (CalendarFourier,\n",
    "                                           DeterministicProcess)\n",
    "\n",
    "\n",
    "simplefilter(\"ignore\")\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True, figsize=(11, 5))\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "data_dir = Path(\"../input/ts-course-data/\")\n",
    "avocados = pd.read_csv(\n",
    "    data_dir / \"avocados.csv\",\n",
    "    header=[0, 1],\n",
    "    index_col=0,\n",
    "    parse_dates=[0],\n",
    ").to_period(\"D\")\n",
    "\n",
    "avocados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-turtle",
   "metadata": {},
   "source": [
    "There will be a bit of fancy indexing with Pandas to keep all the time series aligned, so we'll be sure to go over what we're doing carefully.\n",
    "\n",
    "Let's take a look at our six series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "_ = avocados.plot(subplots=True, sharex=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-truck",
   "metadata": {},
   "source": [
    "Since XGBoost and linear regression each do best with certain kinds of feature engineering, we'll create two versions of our dataset: one for the local linear regression, and one for the global XGBoost.\n",
    "\n",
    "First, the local models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the local dataset\n",
    "yl = avocados.copy()\n",
    "Xl = pd.DataFrame(index=yl.index)\n",
    "Xl = add_trend(Xl)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the last 26 weeks as the test set\n",
    "Xl_train, Xl_test, yl_train, yl_test = train_test_split(\n",
    "    Xl, yl, test_size=26, shuffle=False\n",
    ")\n",
    "\n",
    "# Create dataframes to hold the predictions\n",
    "yl_fit = pd.DataFrame(index=yl_train.index)\n",
    "yl_pred = pd.DataFrame(index=yl_test.index)\n",
    "\n",
    "#  Make the local models by looping over the six time series\n",
    "for col in yl.columns:\n",
    "    model = LinearRegression()\n",
    "    model.fit(Xl_train, yl_train[col])\n",
    "    yl_fit[col] = model.predict(Xl_train)\n",
    "    yl_pred[col] = model.predict(Xl_test)\n",
    "\n",
    "# Melt the predictions into a single column\n",
    "yl_fit = yl_fit.melt(ignore_index=False).value\n",
    "yl_pred = yl_fit.melt(ignore_index=False).value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-anthropology",
   "metadata": {},
   "source": [
    "Preparing the data is almost the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `melt` method 'unpivots' a dataframe. We'll now have just a\n",
    "# single column of sales data with Variety and Type as categorical\n",
    "# features.\n",
    "X = df.melt(var_name=[\"Variety\", \"Type\"],\n",
    "            value_name=\"Sales\",\n",
    "            ignore_index=False)\n",
    "\n",
    "X[\"WeekOfYear\"] = X.index.weekofyear\n",
    "for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "y = X.pop(\"Sales\")\n",
    "\n",
    "# Use the last 26 weeks as the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=26, shuffle=False\n",
    ")\n",
    "y_train = y_train - yl_fit.melt(ignore_index=False).value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-father",
   "metadata": {},
   "source": [
    "Because we trained XGBoost on errors, errors are what XGBoost will predict. To get the complete time series, we add back in the predictions from the local models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_fit = xgb.predict(X_train) + yl_fit\n",
    "y_pred = xgb.predict(X_test) + yl_pred\n",
    "\n",
    "print(mean_squared_error(y_train, y_fit, squared=False))\n",
    "print(mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-valuation",
   "metadata": {},
   "source": [
    "The utility of the local models is demonstrated by a comparison to XGBoost alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "y_train = y.drop(idx_test)\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_fit = xgb.predict(X_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(mean_squared_error(y_train, y_fit, squared=False))\n",
    "print(mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-complexity",
   "metadata": {},
   "source": [
    "# Your Turn #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
