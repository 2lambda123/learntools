{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction #\n",
    "\n",
    "Now we come to our final lesson of the course -- on *multivariate* time series. In previous lessons, you learned the basics of forecasting a single time series. In this lesson, you'll learn how to make forecasts on *collections* of time series.\n",
    "\n",
    "# Stacking Forecasters #\n",
    "\n",
    "Ensembles of decision trees (like `RandomForest` and `XGBoost`) excel at capturing nonlinear behavior and interactions. Decision trees, however, make predictions through *interpolation* -- they predict new values through averages. This means that they fail at *extrapolation*, making predictions for data outside the range of the training set.\n",
    "\n",
    "<img>xgboost failing to model trend</img>\n",
    "\n",
    "We can overcome this limitation by combining a tree-based model with a trend or seasonal model of the kinds we saw in Lessons 2 and 3.\n",
    "\n",
    "<img>detrending and learning residuals</img>\n",
    "\n",
    "Using different models to capture the different parts of a time series is especially common in forecasting.\n",
    "- M4 Competition winner (Smyl 2019, from Uber): combination of exponential smoothing and LSTM\n",
    "\n",
    "# Local Models and Global Models #\n",
    "\n",
    "Many real world datasets comprise hundreds or thousands of interelated time series. Websites keep records of the number of visits for each page. Retail companies keep records of the number of sales per store or per item. While we could just forecast each series individually (like we've learned already), it would be better if we could also take advantage of relationships among the series somehow.\n",
    "\n",
    "<img>hierarchy of series</img>\n",
    "\n",
    "In a multivariate setting, models that only forecast a single series at a time are called **local** models, while those applied to the entire collection are called **global** models.\n",
    "\n",
    "Successful approaches to multivariate forecasting often combine local and global models. The idea is that the local models will be more successful at capturing whatever makes each series unique, while the global model can capture relationships among the series. Similar to the stacked model we saw in Lesson 3, the approach we'll take in this course will be to use simple linear regression for the local models and a tree ensemble for the global model.\n",
    "\n",
    "<img>local + global</img>\n",
    "\n",
    "- *Wiki trends*\n",
    "\n",
    "- *Australian Arrivals*\n",
    "\n",
    "# Aggregating Time Series #\n",
    "\n",
    "Feature engineering can help our model learn global properties of the data.\n",
    "\n",
    "Aggregating by groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out our course on [Feature Engineering]() for a more in depth look at these kinds of techniques.\n",
    "\n",
    "<blockquote>\n",
    "**Ideas for creating features**\n",
    "\n",
    "Here are some other ideas for features you might try:\n",
    "- time until / time after a special event\n",
    "- \n",
    "\n",
    "Winning solutions from forecasting competitions are another great source of ideas.\n",
    "- Rossman Store Sales\n",
    "- Walmart Sales\n",
    "- Recruit Restaurant\n",
    "- M5\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "# Example - Avocado Sales #\n",
    "\n",
    "The *Avocado Sales* dataset contains several years of weekly sales data for three kinds of avocado. It includes categorical features like the region of sale and whether the avocados were organic or not. We'll predict the volume of sales for each of the three varieties of avocado, both organic and conventional. This gives us six series in total for which we'll make forecasts.\n",
    "\n",
    "The hidden cell loads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "from pathlib import Path\n",
    "from warnings import simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "from statsmodels.tsa.deterministic import (CalendarFourier,\n",
    "                                           DeterministicProcess)\n",
    "\n",
    "\n",
    "simplefilter(\"ignore\")\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True, figsize=(11, 5))\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "data_dir = Path(\"../input/ts-course-data/\")\n",
    "avocados = pd.read_csv(\n",
    "    \"/home/jupyter/timeseries/input/ts-course-data/avocados.csv\",\n",
    "    header=[0, 1],\n",
    "    index_col=0,\n",
    "    parse_dates=[0],\n",
    ").to_period(\"D\")\n",
    "\n",
    "avocados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be a bit of fancy indexing with Pandas to keep all the time series aligned, so we'll be sure to go over what we're doing carefully.\n",
    "\n",
    "Let's take a look at our six series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "_ = avocados.plot(subplots=True, sharex=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since XGBoost and linear regression each do best with certain kinds of feature engineering, we'll create two versions of our dataset: one for the local linear regression, and one for the global XGBoost.\n",
    "\n",
    "First, the local models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the local dataset\n",
    "yl = avocados.copy()\n",
    "Xl = pd.DataFrame(index=yl.index)\n",
    "Xl = add_trend(Xl)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the last 26 weeks as the test set\n",
    "Xl_train, Xl_test, yl_train, yl_test = train_test_split(\n",
    "    Xl, yl, test_size=26, shuffle=False\n",
    ")\n",
    "\n",
    "# Create dataframes to hold the predictions\n",
    "yl_fit = pd.DataFrame(index=yl_train.index)\n",
    "yl_pred = pd.DataFrame(index=yl_test.index)\n",
    "\n",
    "#  Make the local models by looping over the six time series\n",
    "for col in yl.columns:\n",
    "    model = LinearRegression()\n",
    "    model.fit(Xl_train, yl_train[col])\n",
    "    yl_fit[col] = model.predict(Xl_train)\n",
    "    yl_pred[col] = model.predict(Xl_test)\n",
    "\n",
    "# Melt the predictions into a single column\n",
    "yl_fit = yl_fit.melt(ignore_index=False).value\n",
    "yl_pred = yl_fit.melt(ignore_index=False).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data is almost the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `melt` method 'unpivots' a dataframe. We'll now have just a\n",
    "# single column of sales data with Variety and Type as categorical\n",
    "# features.\n",
    "X = df.melt(var_name=[\"Variety\", \"Type\"],\n",
    "            value_name=\"Sales\",\n",
    "            ignore_index=False)\n",
    "\n",
    "X[\"WeekOfYear\"] = X.index.weekofyear\n",
    "for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "y = X.pop(\"Sales\")\n",
    "\n",
    "# Use the last 26 weeks as the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=26, shuffle=False\n",
    ")\n",
    "y_train = y.drop(idx_test) - yl_fit.melt(ignore_index=False).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we trained XGBoost on errors, errors are what XGBoost will predict. To get the complete time series, we add back in the predictions from the local models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_fit = xgb.predict(X_train) + yl_fit\n",
    "y_pred = xgb.predict(X_test) + yl_pred\n",
    "\n",
    "print(mean_squared_error(y_train, y_fit, squared=False))\n",
    "print(mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility of the local models is demonstrated by a comparison to XGBoost alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "y_train = y.drop(idx_test)\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_fit = xgb.predict(X_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(mean_squared_error(y_train, y_fit, squared=False))\n",
    "print(mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
