{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sixth-conducting",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-reasoning",
   "metadata": {},
   "source": [
    "We'll see how to stack XGBoost on top of a linear forecaster to make up for XGBoost's inability to extrapolate trends. This and similar combinations are widely used in winning solutions of Kaggle's forecasting competitions.\n",
    "\n",
    "# Time Series Residuals #\n",
    "\n",
    "The residuals are what you get when you subtract out the model's predictions during the training period of the target from the target itself -- the residual series is the difference between the actual curve and the fitted curve, in other words.\n",
    "\n",
    "<img>Residual series, top and bottom, from an earlier example</img>\n",
    "\n",
    "The residuals are useful because they show you what parts of a time series your model failed to learn a given feature. If the residuals look like noise when you plot them over *time*, you know that the model has learned all of the *time dependence* in the series.\n",
    "\n",
    "<img>time dependence</img>\n",
    "\n",
    "And similarly, if the lag plots of the residuals look like noise, you know the model has learned all of the *serial dependence* in the series.\n",
    "\n",
    "<img>serial dependence</img>\n",
    "\n",
    "If, however, you detect any patterns or relationships in the residuals, you know your model could still be improved. One way to capture these relationships could be through feature engineering -- by adding more or different seasonal features or indicators for holidays, say.\n",
    "\n",
    "Another way could be to use a different learning algorithm, one better suited to capturing those relationships; we might want an algorithm capable of learning non-linear relationships, for instance, like XGBoost. But instead of replacing the original model with another (which might have it's own limitations), we can combine the strengths of both through *stacking*.\n",
    "\n",
    "# Hybrid Forecasters #\n",
    "\n",
    "We can train a model on the residuals. (This is how GBDTs like XGBoost work, in fact, training decision trees on the residuals of earlier decision trees, over and over again. Which is why you sometimes hear the trick in this section referred to as a \"boosting\" technique.)\n",
    "\n",
    "The process looks like this:\n",
    "- train and predict with first model\n",
    "- train and predict second model on residuals\n",
    "- add predictions from second to predictions from first to get final predictions\n",
    "\n",
    "<img>training on residuals. top: series and fitted curve. middle: residuals and fitted curve. bottom: combined curve</img>\n",
    "\n",
    "The most common kind of hybrid combines a simple model to learn the main components of individual series with a complex model to learn interactions between series and non-linear effects. We'll combine the linear regression model we've learned about in previous lessons with XGBoost, a powerful decision-tree ensemble of a kind that has dominated tabular competitions on Kaggle.\n",
    "\n",
    "Ensembles of decision trees (like `RandomForest` and `XGBoost`) excel at capturing nonlinear behavior and interactions. Decision trees, however, make predictions through *interpolation* -- they predict new values through averages. Forecasting, however, is a matter of *extrapolation*, making predictions for data *outside* the range of the training set.\n",
    "\n",
    "We can overcome this limitation by combining a tree-based model with a trend or seasonal model of the kinds we saw in Lessons 2 and 3. \n",
    "\n",
    "<img>top: xgboost failing to model trend, middle: xgboost+hybrid succeeds</img>\n",
    "\n",
    "Linear regression makes up for XGBoost's lack of extrapolation ability, while XGBoost makes up for linear regression's lack of non-linearity and deep interactions. With stacking, we can have the best of both.\n",
    "\n",
    "<blockquote>\n",
    "Some notable hybrid models:\n",
    "- M4 - exponential smoothing -> neural nets (1st)\n",
    "- Restaurant (?) - linear regression -> XGBoost (?st)\n",
    "- (???) - ARIMA -> XGBoost\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-afghanistan",
   "metadata": {},
   "source": [
    "# Example - US Retail Sales #\n",
    "\n",
    "The [*US Retail Sales*](https://www.census.gov/retail/index.html) dataset contains monthly sales data for six retail industries from 1992 to 2019, as collected by the US Census Bureau. Our goal will be to forecast sales in the years 2016-2019 given sales in the earlier years. We'll see how a hybrid linear regression / XGBoost forecaster will outperform XGBoost alone on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "from pathlib import Path\n",
    "from warnings import simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import (cross_val_score,\n",
    "                                     TimeSeriesSplit,\n",
    "                                     train_test_split)\n",
    "from statsmodels.tsa.deterministic import (CalendarFourier,\n",
    "                                           DeterministicProcess)\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "simplefilter(\"ignore\")\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\n",
    "    \"figure\",\n",
    "    autolayout=True,\n",
    "    figsize=(12, 6),\n",
    "    titlesize=18,\n",
    "    titleweight='bold',\n",
    ")\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    ")\n",
    "\n",
    "data_dir = Path(\"../input/ts-course-data/\")\n",
    "industries = [\"BuildingMaterials\", \"Clothing\", \"FoodAndBeverage\"]\n",
    "retail = pd.read_csv(\n",
    "    data_dir / \"us-retail-sales.csv\",\n",
    "    usecols=['Month'] + industries,\n",
    "    parse_dates=['Month'],\n",
    "    index_col='Month',\n",
    ").to_period('D').reindex(columns=industries)\n",
    "retail = pd.concat({'Sales': retail}, names=[None, 'Industries'], axis=1)\n",
    "\n",
    "axs = retail.plot(\n",
    "    subplots=True,\n",
    "    sharex=True,\n",
    "    title=\"US Retail Sales\",\n",
    "    legend=False,\n",
    "    ** plot_params,\n",
    ")\n",
    "for ax, name in zip(axs, industries):\n",
    "    ax.legend([name], fontsize='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-relation",
   "metadata": {},
   "source": [
    "First let's use a linear regression model to learn the trend in each series. For demonstration, we'll use a quadratic (order 2) trend. The code here is just the same as in previous lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "y = retail.copy()\n",
    "\n",
    "# Create trend features\n",
    "dp = DeterministicProcess(\n",
    "    index=y.index,  # dates from the training data\n",
    "    constant=True,  # the intercept\n",
    "    order=2,        # quadratic trend\n",
    "    drop=True,      # drop terms to avoid collinearity\n",
    ")\n",
    "X = dp.in_sample()  # features for the training data\n",
    "\n",
    "# Test on the years 2016-2019. It will be easier for us later if we\n",
    "# split the date index instead of the dataframe directly.\n",
    "idx_train, idx_test = train_test_split(\n",
    "    y.index, test_size=12 * 4, shuffle=False,\n",
    ")\n",
    "X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\n",
    "y_train, y_test = y.loc[idx_train], y.loc[idx_test]\n",
    "\n",
    "# Fit trend model\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_fit = pd.DataFrame(\n",
    "    model.predict(X_train),\n",
    "    index=y_train.index,\n",
    "    columns=y_train.columns,\n",
    ")\n",
    "y_pred = pd.DataFrame(\n",
    "    model.predict(X_test),\n",
    "    index=y_test.index,\n",
    "    columns=y_test.columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-strain",
   "metadata": {},
   "source": [
    "Though the fit isn't perfect, it will be enough for our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "axs = y_train.plot(color='0.25', subplots=True, sharex=True)\n",
    "axs = y_test.plot(color='0.25', subplots=True, sharex=True, ax=axs)\n",
    "axs = y_fit.plot(color='C0', subplots=True, sharex=True, ax=axs)\n",
    "axs = y_pred.plot(color='C3', subplots=True, sharex=True, ax=axs)\n",
    "for ax in axs: ax.legend([])\n",
    "_ = plt.suptitle(\"Trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-disney",
   "metadata": {},
   "source": [
    "While the linear regression algorithm is capable of multi-output regression, the XGBoost algorithm is not. To predict multiple series at once with XGBoost, we'll instead convert these series from *wide* format, with one time series per column, to *long* format, with series indexed by categories along rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `stack` method converts column labels to row labels, pivoting\n",
    "# from wide format to long\n",
    "X = retail.stack()  # pivot dataset wide to long\n",
    "display(X.head())\n",
    "y = X.pop('Sales')  # grab target series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-atlas",
   "metadata": {},
   "source": [
    "So that XGBoost can learn to distinguish our three time series, we'll turn the row labels for `'Industries'` into a categorical feature with a label encoding. We'll also create a feature for annual seasonality by pulling the month numbers out of the time index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn row labels into categorical feature columns\n",
    "X = X.reset_index('Industries')\n",
    "# Label encoding for 'Industries' feature\n",
    "for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# Label encoding for annual seasonality\n",
    "X[\"Month\"] = X.index.month  # 1, 2, ..., 12\n",
    "\n",
    "# Create splits\n",
    "X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\n",
    "y_train, y_test = y.loc[idx_train], y.loc[idx_test]\n",
    "\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-freeware",
   "metadata": {},
   "source": [
    "Now we'll convert the trend predictions made earlier to long format and then subtract them from the original series. That will give us detrended (residual) series that XGBoost can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot wide to long (stack) and convert DataFrame to Series (squeeze)\n",
    "y_fit = y_fit.stack().squeeze()    # trend from training set\n",
    "y_pred = y_pred.stack().squeeze()  # trend from test set\n",
    "\n",
    "# Create residuals (the collection of detrended series) from the\n",
    "# training set\n",
    "y_resid = y_train - y_fit\n",
    "\n",
    "# Train XGBoost on the residuals\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_resid)\n",
    "\n",
    "# Add the predicted residuals onto the predicted trends\n",
    "y_pred_boosted = xgb.predict(X_train) + y_fit\n",
    "y_forecast_boosted = xgb.predict(X_test) + y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-scientist",
   "metadata": {},
   "source": [
    "The fit appears quite good, though we can see how the trend learned by XGBoost is only as good as the trend learned by our linear regression model -- in particular, XGBoost wasn't able to compensate for the poorly fit trend in the `'BuildingMaterials'` series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "axs = y_train.unstack(['Industries']).plot(\n",
    "    color='0.25', subplots=True, sharex=True,\n",
    ")\n",
    "axs = y_test.unstack(['Industries']).plot(\n",
    "    color='0.25', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "axs = y_pred_boosted.unstack(['Industries']).plot(\n",
    "    color='C0', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "axs = y_forecast_boosted.unstack(['Industries']).plot(\n",
    "    color='C3', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "for ax in axs: ax.legend([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-facing",
   "metadata": {},
   "source": [
    "Let's compare the performance of our hybrid model with XGBoost when fit directly to the dataset (instead of to a residual series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = pd.DataFrame(xgb.predict(X_train), index=y_train.index)\n",
    "y_forecast_xgb = pd.DataFrame(xgb.predict(X_test), index=y_test.index)\n",
    "\n",
    "axs = y_train.unstack(['Industries']).plot(\n",
    "    color='0.25', subplots=True, sharex=True,\n",
    ")\n",
    "axs = y_test.unstack(['Industries']).plot(\n",
    "    color='0.25', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "axs = y_pred_xgb.unstack(['Industries']).plot(\n",
    "    color='C0', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "axs = y_forecast_xgb.unstack(['Industries']).plot(\n",
    "    color='C3', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "for ax in axs: ax.legend([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-aviation",
   "metadata": {},
   "source": [
    "You can see that XGBoost failed to learn the trend in these series completely (adding a trend feature wouldn't help either, in fact). The superior performance of the hybrid method shows up the error metrics as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"Hybrid\")\n",
    "print(\"------\")\n",
    "print(\"Train MAE:\\t{:.3f}\".format(mean_absolute_error(y_train, y_pred_boosted)))\n",
    "print(\"Test MAE:\\t{:.3f}\".format(mean_absolute_error(y_test, y_forecast_boosted)))\n",
    "print(\"\\n\")\n",
    "print(\"XGBoost Only\")\n",
    "print(\"------------\")\n",
    "print(\"Train MAE:\\t{:.3f}\".format(mean_absolute_error(y_train, y_pred_xgb)))\n",
    "print(\"Test MAE:\\t{:.3f}\".format(mean_absolute_error(y_test, y_forecast_xgb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-turkey",
   "metadata": {},
   "source": [
    "# Your Turn #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
