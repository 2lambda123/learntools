{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "elect-poker",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction #\n",
    "\n",
    "Linear regression excels at extrapolating trends, but can't learn interactions. XGBoost excels at learning interactions, but can't extrapolate trends. In this lesson, we'll learn how to create \"hybrid\" forecasters that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other. \n",
    "\n",
    "# Components and Residuals #\n",
    "\n",
    "So that we can design effective hybrids, we need a better understanding of how time series are constructed. We've studied up to now three patterns of dependence: trend, seasons, and cycles. Many time series can be closely described by an additive model of just these three components plus some essentially unpredictable, entirely random *error*:\n",
    "\n",
    "```\n",
    "series = trend + seasons + cycles + error\n",
    "```\n",
    "\n",
    "Each of the terms in this model we would then call a **component** of the time series.\n",
    "\n",
    "The **residuals** of a model are the difference between the target the model was trained on and the predictions the model makes -- the difference between the actual curve and the fitted curve, in other words. Plot the residuals against a feature, and you get the \"left over\" part of the target, or what the model failed to learn about the target from that feature.\n",
    "\n",
    "We could imagine learning the components of a time series as an iterative process: first learn the trend and subtract it out from the series, then learn the seasonality from the detrended residuals and subtract the seasons out, and so on.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/DQtn9P6.png\" width=1000, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Add together all the components we learned and we get the complete model.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/0hOE3tI.png\" width=600, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Patterns in the residuals suggest ways a regression model can be improved. The residuals for the complete model in the figure above appear to be very close to *white noise*, meaning entirely random. In other words, after subtracting out the components all we're left with is the unpredictable `error`, and we know this model can't be improved further (at least with the data we have).\n",
    "\n",
    "# Hybrid Forecasting with Residuals #\n",
    "\n",
    "The basic observation we need to construct our hybrids is that we don't need to model every component all at once with a single algorithm. Instead, we can learn the components separately and then add up the separate predictions at the end. We could, for instance, use linear regression to learn the trend of a series and XGBoost to learn everything else -- the \"everything else\" being the residuals of the trend predictions.\n",
    "\n",
    "In detail, the process is this:\n",
    "```\n",
    "# 1. Train and predict with first model\n",
    "model_1.fit(X_train_1, y_train)\n",
    "y_pred_1 = model_1.predict(X_train)\n",
    "\n",
    "# 2. Train and predict with second model on residuals\n",
    "model_2.fit(X_train_2, y_train - y_pred_1)\n",
    "y_pred_2 = model_2.predict(X_train_2)\n",
    "\n",
    "# 3. Add to get overall predictions\n",
    "y_pred = y_pred_1 + y_pred_2\n",
    "```\n",
    "\n",
    "You could in principal use more than two models, but in practice this doesn't seem to be especially helpful. In fact, the commonest strategy for constructing hybrids is the one we've just described: a simple (usually linear) learning algorithm followed by a complex, non-linear learner like GBDTs or a deep neural net, the simple model typically designed as a \"helper\" for the powerful algorithm that follows.\n",
    "\n",
    "### Designing Hybrids (optional)\n",
    "\n",
    "*NOTE: I'm worried this section is maybe too much. I tought it might provide motivation and encourage creativity, but the material is a bit heavy and I'm worried there's not enough space to give an adequate explanation. I'm thinking about how to simplify...*\n",
    "\n",
    "Their are generally two ways a regression algorithm can make predictions: either by transforming the *features* or by transforming the *target*. Feature-transforming algorithms learn some mathematical function that takes features as an input and produces an output that matches the target values in the training set. Linear regression and neural nets are of this kind. \n",
    "\n",
    "Target-transforming algorithms use the features to group the target values in the training set and make predictions by averaging values in a group; a set of feature just indicates which group to average. Decision trees and nearest neighbors are of this kind.\n",
    "\n",
    "The upshot is this: feature transformers generally can *extrapolate* target values beyond the training set given appropriate features, but the predictions of target transformers will always be bound within the range of the training set. If the time dummy continues counting time steps, linear regression continues drawing the trend line. Given the same time dummy a decision tree will predict the value indicated by the last step of the training data forever. Decision trees cannot extrapolate trends. Random forests and GBDTs are ensembles of decision trees, so they also cannot extrapolate trends.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/ZZtfuFJ.png\" width=600, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Decision trees fail to extrapolate trends beyond the training set.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "This difference is what motivates the hybrid design in this lesson: use linear regression to extrapolate the trend, transform the *target* to remove the trend, and apply GBDTs to the detrended residuals. To hybridize a neural net (a feature transformer), you could instead include the predictions of another model as a feature, which the neural net could then include as part of its own predictions. The method of fitting to residuals is actually the same method the gradient boosting algorithm uses, so we will call these **boosted** hybrids; the method of using predictions as features is known as \"stacking\", so we will call these **stacked** hybrids.\n",
    "\n",
    "<blockquote>\n",
    "<strong>Forecasting hybrids from competitions</strong>\n",
    "<ul>\n",
    "<li>Ridge regression boosted with XGBoost (Restaurant)</li>\n",
    "<li>Exponential smoothing stacked with LSTM neural net (M4)</li>\n",
    "<li>ARIMA boosted with GBDT (Walmart)</li>\n",
    "<li>...</li>\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "# Example - US Retail Sales #\n",
    "\n",
    "The [*US Retail Sales*](https://www.census.gov/retail/index.html) dataset contains monthly sales data for six retail industries from 1992 to 2019, as collected by the US Census Bureau. Our goal will be to forecast sales in the years 2016-2019 given sales in the earlier years. We'll see how a hybrid linear regression / XGBoost forecaster will outperform XGBoost alone on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "from pathlib import Path\n",
    "from warnings import simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import (cross_val_score,\n",
    "                                     TimeSeriesSplit,\n",
    "                                     train_test_split)\n",
    "from statsmodels.tsa.deterministic import (CalendarFourier,\n",
    "                                           DeterministicProcess)\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "simplefilter(\"ignore\")\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\n",
    "    \"figure\",\n",
    "    autolayout=True,\n",
    "    figsize=(12, 6),\n",
    "    titlesize=18,\n",
    "    titleweight='bold',\n",
    ")\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    ")\n",
    "\n",
    "data_dir = Path(\"../input/ts-course-data/\")\n",
    "industries = [\"BuildingMaterials\", \"Clothing\", \"FoodAndBeverage\"]\n",
    "retail = pd.read_csv(\n",
    "    data_dir / \"us-retail-sales.csv\",\n",
    "    usecols=['Month'] + industries,\n",
    "    parse_dates=['Month'],\n",
    "    index_col='Month',\n",
    ").to_period('D').reindex(columns=industries)\n",
    "retail = pd.concat({'Sales': retail}, names=[None, 'Industries'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-setting",
   "metadata": {},
   "source": [
    "First let's use a linear regression model to learn the trend in each series. For demonstration, we'll use a quadratic (order 2) trend. The code here is just the same as in previous lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "y = retail.copy()\n",
    "\n",
    "# Create trend features\n",
    "dp = DeterministicProcess(\n",
    "    index=y.index,  # dates from the training data\n",
    "    constant=True,  # the intercept\n",
    "    order=2,        # quadratic trend\n",
    "    drop=True,      # drop terms to avoid collinearity\n",
    ")\n",
    "X = dp.in_sample()  # features for the training data\n",
    "\n",
    "# Test on the years 2016-2019. It will be easier for us later if we\n",
    "# split the date index instead of the dataframe directly.\n",
    "idx_train, idx_test = train_test_split(\n",
    "    y.index, test_size=12 * 4, shuffle=False,\n",
    ")\n",
    "X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\n",
    "y_train, y_test = y.loc[idx_train], y.loc[idx_test]\n",
    "\n",
    "# Fit trend model\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_fit = pd.DataFrame(\n",
    "    model.predict(X_train),\n",
    "    index=y_train.index,\n",
    "    columns=y_train.columns,\n",
    ")\n",
    "y_pred = pd.DataFrame(\n",
    "    model.predict(X_test),\n",
    "    index=y_test.index,\n",
    "    columns=y_test.columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-independence",
   "metadata": {},
   "source": [
    "Though the fit isn't perfect, it will be enough for our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "axs = y_train.plot(color='0.25', subplots=True, sharex=True)\n",
    "axs = y_test.plot(color='0.25', subplots=True, sharex=True, ax=axs)\n",
    "axs = y_fit.plot(color='C0', subplots=True, sharex=True, ax=axs)\n",
    "axs = y_pred.plot(color='C3', subplots=True, sharex=True, ax=axs)\n",
    "for ax in axs: ax.legend([])\n",
    "_ = plt.suptitle(\"Trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-peter",
   "metadata": {},
   "source": [
    "While the linear regression algorithm is capable of multi-output regression, the XGBoost algorithm is not. To predict multiple series at once with XGBoost, we'll instead convert these series from *wide* format, with one time series per column, to *long* format, with series indexed by categories along rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `stack` method converts column labels to row labels, pivoting\n",
    "# from wide format to long\n",
    "X = retail.stack()  # pivot dataset wide to long\n",
    "display(X.head())\n",
    "y = X.pop('Sales')  # grab target series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-afternoon",
   "metadata": {},
   "source": [
    "So that XGBoost can learn to distinguish our three time series, we'll turn the row labels for `'Industries'` into a categorical feature with a label encoding. We'll also create a feature for annual seasonality by pulling the month numbers out of the time index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn row labels into categorical feature columns\n",
    "X = X.reset_index('Industries')\n",
    "# Label encoding for 'Industries' feature\n",
    "for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# Label encoding for annual seasonality\n",
    "X[\"Month\"] = X.index.month  # 1, 2, ..., 12\n",
    "\n",
    "# Create splits\n",
    "X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\n",
    "y_train, y_test = y.loc[idx_train], y.loc[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-milan",
   "metadata": {},
   "source": [
    "Now we'll convert the trend predictions made earlier to long format and then subtract them from the original series. That will give us detrended (residual) series that XGBoost can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot wide to long (stack) and convert DataFrame to Series (squeeze)\n",
    "y_fit = y_fit.stack().squeeze()    # trend from training set\n",
    "y_pred = y_pred.stack().squeeze()  # trend from test set\n",
    "\n",
    "# Create residuals (the collection of detrended series) from the\n",
    "# training set\n",
    "y_resid = y_train - y_fit\n",
    "\n",
    "# Train XGBoost on the residuals\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_resid)\n",
    "\n",
    "# Add the predicted residuals onto the predicted trends\n",
    "y_pred_boosted = xgb.predict(X_train) + y_fit\n",
    "y_forecast_boosted = xgb.predict(X_test) + y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-climate",
   "metadata": {},
   "source": [
    "The fit appears quite good, though we can see how the trend learned by XGBoost is only as good as the trend learned by our linear regression model -- in particular, XGBoost wasn't able to compensate for the poorly fit trend in the `'BuildingMaterials'` series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "axs = y_train.unstack(['Industries']).plot(\n",
    "    color='0.25', subplots=True, sharex=True,\n",
    ")\n",
    "axs = y_test.unstack(['Industries']).plot(\n",
    "    color='0.25', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "axs = y_pred_boosted.unstack(['Industries']).plot(\n",
    "    color='C0', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "axs = y_forecast_boosted.unstack(['Industries']).plot(\n",
    "    color='C3', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "for ax in axs: ax.legend([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-honey",
   "metadata": {},
   "source": [
    "# Your Turn #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
