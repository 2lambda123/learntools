{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In this lesson, you'll learn how to address some of the unique challenges that come with forecasting.\n",
    "\n",
    "- *Electricity Demand*\n",
    "\n",
    "# Defining the Forecasting Goal #\n",
    "\n",
    "Yesterday - the fit period:\n",
    "- expanding\n",
    "- windowed\n",
    "\n",
    "Today - the forecast origin:\n",
    "- Fixed-origin\n",
    "- Rolling-origin\n",
    "\n",
    "Tomorrow:\n",
    "- One Step Ahead vs Multiple Steps Ahead\n",
    "- the Forecast Horizon (aka lead time)\n",
    "\n",
    "We need to understand the circumstances of the problem. What is the goal? What are the constraints?\n",
    "\n",
    "<blockquote>\n",
    "**Forecasting with non-deterministic features**\n",
    "\n",
    "What if you are using a time series as a feature? This problem arises when using a lag embedding, for instance.\n",
    "\n",
    "Two solutions:\n",
    "- **Recursive**\n",
    "- **Direct**\n",
    "\n",
    "You'll have a chance to explore the recursive method in the Bonus Lesson.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Drift\n",
    "\n",
    "<note>TODO: data drift image</note>\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"\" width=400, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Ways the data distribution can change. <strong>One:</strong>Trend is a change in the mean of the distribution.<strong>Two:</strong>Changes in variance can manifest in the seasonality.</center><strong>Three:</strong>Current events can cause sudden and catastrophic changes that are hard to predict.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Features of rolling statistics, like mean and standard deviation, have been very effective in forecasting competitions on Kaggle. (See solution writeups from <note>TODO and TODO</note>.) Such features could help your model track drift in the data distribution.\n",
    "\n",
    "- interpolation vs. extrapolation\n",
    "\n",
    "A model that is very good at making predictions on the training data distribution, can fail (sometimes spectacularly) outside of it:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"\" width=400, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>How models can fail to extrapolate from the training data. <strong>Top:</strong> A tenth-degree polynomial diverges rapidly. <strong>Bottom:</strong> A tree ensemble (XGBoost, namely) fails to predict new values.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Machine learning models are trained to minimize loss only on their training data; there's no penalty for performing badly on new data. Generally, .\n",
    "\n",
    "Flexible models like ... are good at **interpolation**, or \"connecting the dots\" between points within the training distribution. Linear regression, however, is often a good choice for **extrapolation**. Linear regression assumes that the data will continue to change at the same constant rate that it did on the training data.\n",
    "\n",
    "In forecasting, we are often asking a model to do something it wasn't trained to do. For this to work, we need to use models we know will act sensibly on new data. <note>TODO</note>(As we'll see in the next lesson, a way to overcome these limitations is to stack a flexible model -- like XGBoost -- on top of a model that can extrapolate distribution shifts -- like linear regression. With stacking, we can get the best of both.)\n",
    "\n",
    "<blockquote>\n",
    "**Distribution shift**\n",
    "\n",
    "When the patterns in a time series stay the same over time, the series is called *stationary*. If you have a time series that's truly stationary, there's no information in the future that isn't already available in the past, and so forecasting becomes almost like an ordinary regression problem: ordinary cross-validation can work surprisingly well.\n",
    "\n",
    "Most time series are not stationary. The world is constantly changing and so is the data the world produces.\n",
    "\n",
    "Sometimes, time series are predictably non-stationary, like in the case of a linear trend. \n",
    "\n",
    "Coping with a changing world is one of the hardest problems in machine learning. In production environments, a lot of care is taken to detect when new data has shifted too far from the data used for training. In practice, many models need to be frequently retrained. Because of these problems, a model's robustness to change is often just as important as its predictive accuracy.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation #\n",
    "\n",
    "With ordinary machine learning, you typically create splits through random sampling. This works because the observations (the rows in the dataframe) are independent -- you could shuffle the index of the dataframe and nothing essential will have changed. (Sometimes there are complications when data have been observed in groups.)\n",
    "\n",
    "In a time series, however, the observations are indexed by time. Shuffling the index of a time series would make the time component meaningless. The problem then is that random sampling won't give us independent splits, which means there is a danger that our usual way of evaluating model quality (through train / test splits or cross-validation) won't be accurate for time series.\n",
    "\n",
    "<note>example?</note>\n",
    "\n",
    "A good default strategy is to split the data chronologically: older data is used for training, while later data is used for evaluation. The assumption is that the later data will be most similar to that used for forecasting and so will give better error estimates. (Depending on the situation, this won't always be the case.)\n",
    "\n",
    "<img>chronological train / valid / test splits</img>\n",
    "\n",
    "You can also use a rolling validation, also called a \"backtest\".\n",
    "\n",
    "<img>backtest</img>\n",
    "\n",
    "There are a lot of variations on this scheme. You might choose to only use a fixed-size window of training data, or you might want to leave a gap between the training and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, it's a good idea to mimic with your validation strategy what you'll be doing when making forecasts. The closer you can come the better your validation error estimates will be.\n",
    "\n",
    "<blockquote>\n",
    "<strong>Model explainability</strong>\n",
    "Model explainability techniques are a great way to test the robustness of your model.\n",
    "\n",
    "Check out our <a href=\"https://www.kaggle.com/learn/machine-learning-explainability\">Machine Learning Explainability</a> course for more.\n",
    "</blockquote>\n",
    "\n",
    "# Metrics and Baselines #\n",
    "\n",
    "In additional to common regression metrics like RMSE and MAE, there are a number of metrics commonly used with time series.\n",
    "\n",
    "- MAPE\n",
    "\n",
    "The performance of a machine learning model is often compared to a **baseline**.\n",
    "\n",
    "There are several **baselines** that time series models are often compared to:\n",
    "- **trend**\n",
    "- **season**\n",
    "- **mean**\n",
    "\n",
    "<img>baseline forecasters</img>\n",
    "\n",
    "The **MASE** metric measures how well a forecaster performs against the <note>TODO</note> baseline. It has the advantage of being symmetric, etc.\n",
    "\n",
    "The best metric to use in a given problem, will always be the one that measures the kinds of outcomes you actually care about. The MASE metric, however, give sensible results across a range of forecasting situations and makes a reasonable default.\n",
    "\n",
    "# Example - Electricity Demand #\n",
    "\n",
    "The *Electricity Demand* dataset contains hourly demand for electricity.\n",
    "\n",
    "The hidden cell defines some utility functions from the previous lessons and loads the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "from pathlib import Path\n",
    "from warnings import simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.deterministic import (CalendarFourier,\n",
    "                                           DeterministicProcess)\n",
    "\n",
    "simplefilter(\"ignore\")\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True, figsize=(11, 5))\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# Load data\n",
    "data_dir = Path(\"../input/ts-course-data\")\n",
    "elecdemand = pd.read_csv(data_dir / \"elecdemand.csv\", parse_dates=[\"Datetime\"])\n",
    "elecdemand = elecdemand.set_index(\"Datetime\").to_period(\"H\")\n",
    "\n",
    "# Create features\n",
    "\n",
    "# Data is hourly. There are 168 hours per week, so `fourier` creates\n",
    "# about half as many features (42 * 2) as indicators would (168 - 1).\n",
    "fourier = CalendarFourier(freq=\"W\", order=42)\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "    index=elecdemand.index,\n",
    "    constant=True,               # level\n",
    "    order=2,                     # trend (order 1 means linear)\n",
    "    seasonal=True,               # daily seasonality (indicators)\n",
    "    additional_terms=[fourier],  # weekly seasonality (fourier)\n",
    "    drop=True,                   # drop terms to avoid collinearity\n",
    ")\n",
    "\n",
    "X = dp.in_sample()\n",
    "y = elecdemand.Demand.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll use holdout validation.\n",
    "\n",
    "We can use `train_test_split` from scikit-learn to create our data splits. It's important to set `shuffle=False` or else the test set will be sampled at random dates instead of taken as a continuous block at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=24 * 14,  # 14 days\n",
    "    shuffle=False,      # time series should not be shuffled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the predictions and look at the train and test error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_fit = pd.Series(\n",
    "    model.predict(X_train),\n",
    "    index=X_train.index,\n",
    ")\n",
    "y_pred = pd.Series(\n",
    "    model.predict(X_test),\n",
    "    index=y_test.index,\n",
    ")\n",
    "\n",
    "train_rmse = mean_squared_error(y_train, y_fit, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print((f\"Train RMSE: {train_rmse:.2f}\\n\" f\"Test RMSE: {test_rmse:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With timeseries validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "\n",
    "cv = TimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    test_size=24 * 14,\n",
    "    gap=0,\n",
    ")\n",
    "\n",
    "cv_rmse = (-1) * cross_val_score(\n",
    "    LinearRegression(),\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    ")\n",
    "cv_rmse = np.sqrt(cv_rmse.mean())\n",
    "\n",
    "print(\"Backtest RMSE: \", cv_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting future demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit model to entire training set\n",
    "model.fit(X, y)\n",
    "\n",
    "# create features for forecast\n",
    "X_oos = dp.out_of_sample(steps=24 * 14)\n",
    "\n",
    "y_forecast = pd.Series(\n",
    "    model.predict(X_oos),\n",
    "    index=X_oos.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ax = y.plot()\n",
    "_ = y_forecast.plot(ax=ax, color='C3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
