{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adverse-report",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction #\n",
    "\n",
    "Linear regression excels at extrapolating trends, but can't learn interactions. XGBoost excels at learning interactions, but can't extrapolate trends. In this lesson, we'll learn how to create \"hybrid\" forecasters that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other. \n",
    "\n",
    "# Components and Residuals #\n",
    "\n",
    "So that we can design effective hybrids, we need a better understanding of how time series are constructed. We've studied up to now three patterns of dependence: trend, seasons, and cycles. Many time series can be closely described by an additive model of just these three components plus some essentially unpredictable, entirely random *error*:\n",
    "\n",
    "```\n",
    "series = trend + seasons + cycles + error\n",
    "```\n",
    "\n",
    "Each of the terms in this model we would then call a **component** of the time series.\n",
    "\n",
    "The **residuals** of a model are the difference between the target the model was trained on and the predictions the model makes -- the difference between the actual curve and the fitted curve, in other words. Plot the residuals against a feature, and you get the \"left over\" part of the target, or what the model failed to learn about the target from that feature.\n",
    "\n",
    "We could imagine learning the components of a time series as an iterative process: first learn the trend and subtract it out from the series, then learn the seasonality from the detrended residuals and subtract the seasons out, and so on.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/DHoKGwO.png\" width=700, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Add together all the components we learned and we get the complete model.\n",
    "\n",
    "Patterns in the residuals suggest ways a regression model can be improved.\n",
    "- error term: underfitting and overfitting; important for next lesson\n",
    "- underfitting: still component left in the error term\n",
    "- overfitting: error is white noise (so possibly overfit)\n",
    "\n",
    "# Hybrid Forecasting with Residuals #\n",
    "\n",
    "The basic observation we need to construct our hybrids is that we don't need to model every component all at once with a single algorithm. Instead, we can learn the components separately and then add up the separate predictions at the end. We could, for instance, use linear regression to learn the trend of a series and XGBoost to learn everything else -- the \"everything else\" being the residuals of the trend predictions.\n",
    "\n",
    "In detail, the process is this:\n",
    "```\n",
    "# 1. Train and predict with first model\n",
    "model_1.fit(X_train_1, y_train)\n",
    "y_pred_1 = model_1.predict(X_train)\n",
    "\n",
    "# 2. Train and predict with second model on residuals\n",
    "model_2.fit(X_train_2, y_train - y_pred_1)\n",
    "y_pred_2 = model_2.predict(X_train_2)\n",
    "\n",
    "# 3. Add to get overall predictions\n",
    "y_pred = y_pred_1 + y_pred_2\n",
    "```\n",
    "\n",
    "You could in principal use more than two models, but in practice this doesn't seem to be especially helpful. In fact, the commonest strategy for constructing hybrids is the one we've just described: a simple (usually linear) learning algorithm followed by a complex, non-linear learner like GBDTs or a deep neural net, the simple model typically designed as a \"helper\" for the powerful algorithm that follows.\n",
    "\n",
    "### Designing Hybrids\n",
    "\n",
    "There are many ways you could combine machine learning models besides the way we've outlined in this lesson. Successfully combining models, though, requires that we dig a bit deeper into how these algorithms operate.\n",
    "\n",
    "Their are generally two ways a regression algorithm can make predictions: either by transforming the *features* or by transforming the *target*. Feature-transforming algorithms learn some mathematical function that takes features as an input and then combines and transforms them to produce an output that matches the target values in the training set. Linear regression and neural nets are of this kind.\n",
    "\n",
    "Target-transforming algorithms use the features to group the target values in the training set and make predictions by averaging values in a group; a set of feature just indicates which group to average. Decision trees and nearest neighbors are of this kind.\n",
    "\n",
    "The important thing is this: feature transformers generally can **extrapolate** target values beyond the training set given appropriate features as inputs, but the predictions of target transformers will always be bound within the range of the training set. If the time dummy continues counting time steps, linear regression continues drawing the trend line. Given the same time dummy, a decision tree will predict the trend indicated by the last step of the training data into the future forever. *Decision trees cannot extrapolate trends.* Random forests and GBDTs are ensembles of decision trees, so they also cannot extrapolate trends.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/ZZtfuFJ.png\" width=600, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>A decision tree will fail to extrapolate a trend beyond the training set.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "This difference is what motivates the hybrid design in this lesson: use linear regression to extrapolate the trend, transform the *target* to remove the trend, and apply GBDTs to the detrended residuals. To hybridize a neural net (a feature transformer), you could instead include the predictions of another model as a feature, which the neural net would then include as part of its own predictions. The method of fitting to residuals is actually the same method the gradient boosting algorithm uses, so we will call these **boosted** hybrids; the method of using predictions as features is known as \"stacking\", so we will call these **stacked** hybrids.\n",
    "\n",
    "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n",
    "<strong>Winning Hybrids from Kaggle Competitions</strong>\n",
    "<ul>\n",
    "<li>Ridge regression boosted with GBDT (Restaurant)</li>\n",
    "<li>Exponential smoothing stacked with LSTM neural net (M4)</li>\n",
    "<li>ARIMA boosted with GBDT (Walmart)</li>\n",
    "<li>TODO: Get the details/solution links on these.</li>\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "# Example - US Retail Sales #\n",
    "\n",
    "The [*US Retail Sales*](https://www.census.gov/retail/index.html) dataset contains monthly sales data for various retail industries from 1992 to 2019, as collected by the US Census Bureau. Our goal will be to forecast sales in the years 2016-2019 given sales in the earlier years. In addition to creating a linear regression + GBDT hybrid, we'll also see how to set up a time series dataset for use with XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "from pathlib import Path\n",
    "from warnings import simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.deterministic import (CalendarFourier,\n",
    "                                           DeterministicProcess)\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "simplefilter(\"ignore\")\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\n",
    "    \"figure\",\n",
    "    autolayout=True,\n",
    "    figsize=(11, 4),\n",
    "    titlesize=18,\n",
    "    titleweight='bold',\n",
    ")\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    ")\n",
    "\n",
    "data_dir = Path(\"../input/ts-course-data/\")\n",
    "industries = [\"BuildingMaterials\", \"FoodAndBeverage\"]\n",
    "retail = pd.read_csv(\n",
    "    data_dir / \"us-retail-sales.csv\",\n",
    "    usecols=['Month'] + industries,\n",
    "    parse_dates=['Month'],\n",
    "    index_col='Month',\n",
    ").to_period('D').reindex(columns=industries)\n",
    "retail = pd.concat({'Sales': retail}, names=[None, 'Industries'], axis=1)\n",
    "\n",
    "retail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-leisure",
   "metadata": {},
   "source": [
    "First let's use a linear regression model to learn the trend in each series. For demonstration, we'll use a quadratic (order 2) trend. (The code here is basically the same as that in previous lessons.) Though the fit isn't perfect, it will be enough for our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "y = retail.copy()\n",
    "\n",
    "# Create trend features\n",
    "dp = DeterministicProcess(\n",
    "    index=y.index,  # dates from the training data\n",
    "    constant=True,  # the intercept\n",
    "    order=2,        # quadratic trend\n",
    "    drop=True,      # drop terms to avoid collinearity\n",
    ")\n",
    "X = dp.in_sample()  # features for the training data\n",
    "\n",
    "# Test on the years 2016-2019. It will be easier for us later if we\n",
    "# split the date index instead of the dataframe directly.\n",
    "idx_train, idx_test = train_test_split(\n",
    "    y.index, test_size=12 * 4, shuffle=False,\n",
    ")\n",
    "X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\n",
    "y_train, y_test = y.loc[idx_train], y.loc[idx_test]\n",
    "\n",
    "# Fit trend model\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_fit = pd.DataFrame(\n",
    "    model.predict(X_train),\n",
    "    index=y_train.index,\n",
    "    columns=y_train.columns,\n",
    ")\n",
    "y_pred = pd.DataFrame(\n",
    "    model.predict(X_test),\n",
    "    index=y_test.index,\n",
    "    columns=y_test.columns,\n",
    ")\n",
    "\n",
    "# Plot\n",
    "axs = y_train.plot(color='0.25', subplots=True, sharex=True)\n",
    "axs = y_test.plot(color='0.25', subplots=True, sharex=True, ax=axs)\n",
    "axs = y_fit.plot(color='C0', subplots=True, sharex=True, ax=axs)\n",
    "axs = y_pred.plot(color='C3', subplots=True, sharex=True, ax=axs)\n",
    "for ax in axs: ax.legend([])\n",
    "_ = plt.suptitle(\"Trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-discharge",
   "metadata": {},
   "source": [
    "While the linear regression algorithm is capable of multi-output regression, the XGBoost algorithm is not. To predict multiple series at once with XGBoost, we'll instead convert these series from *wide* format, with one time series per column, to *long* format, with series indexed by categories along rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `stack` method converts column labels to row labels, pivoting from wide format to long\n",
    "X = retail.stack()  # pivot dataset wide to long\n",
    "display(X.head())\n",
    "y = X.pop('Sales')  # grab target series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-quality",
   "metadata": {},
   "source": [
    "So that XGBoost can learn to distinguish our two time series, we'll turn the row labels for `'Industries'` into a categorical feature with a label encoding. We'll also create a feature for annual seasonality by pulling the month numbers out of the time index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn row labels into categorical feature columns\n",
    "X = X.reset_index('Industries')\n",
    "# Label encoding for 'Industries' feature\n",
    "for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# Label encoding for annual seasonality\n",
    "X[\"Month\"] = X.index.month  # values are 1, 2, ..., 12\n",
    "\n",
    "# Create splits\n",
    "X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\n",
    "y_train, y_test = y.loc[idx_train], y.loc[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-hollywood",
   "metadata": {},
   "source": [
    "Now we'll convert the trend predictions made earlier to long format and then subtract them from the original series. That will give us detrended (residual) series that XGBoost can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot wide to long (stack) and convert DataFrame to Series (squeeze)\n",
    "y_fit = y_fit.stack().squeeze()    # trend from training set\n",
    "y_pred = y_pred.stack().squeeze()  # trend from test set\n",
    "\n",
    "# Create residuals (the collection of detrended series) from the training set\n",
    "y_resid = y_train - y_fit\n",
    "\n",
    "# Train XGBoost on the residuals\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_resid)\n",
    "\n",
    "# Add the predicted residuals onto the predicted trends\n",
    "y_fit_boosted = xgb.predict(X_train) + y_fit\n",
    "y_pred_boosted = xgb.predict(X_test) + y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-polymer",
   "metadata": {},
   "source": [
    "The fit appears quite good, though we can see how the trend learned by XGBoost is only as good as the trend learned by the linear regression -- in particular, XGBoost wasn't able to compensate for the poorly fit trend in the `'BuildingMaterials'` series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "axs = y_train.unstack(['Industries']).plot(\n",
    "    color='0.25', figsize=(11, 5), subplots=True, sharex=True,\n",
    "    title=['BuildingMaterials', 'FoodAndBeverage'],\n",
    ")\n",
    "axs = y_test.unstack(['Industries']).plot(\n",
    "    color='0.25', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "axs = y_fit_boosted.unstack(['Industries']).plot(\n",
    "    color='C0', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "axs = y_pred_boosted.unstack(['Industries']).plot(\n",
    "    color='C3', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "for ax in axs: ax.legend([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-remains",
   "metadata": {},
   "source": [
    "# Your Turn #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
