{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Classification\n",
    "\n",
    "In the first exercise, you will train a model that classifies product reviews into \"good\" or \"bad\" sentiments. You'll use a basic Naive Bayes model to get a baseline which you'll improve on over the next exercises. \n",
    "\n",
    "Original data is from here: https://www.kaggle.com/bittlingmayer/amazonreviews/downloads/amazonreviews.zip/2\n",
    "But I think I'm going to trim it down and make my own dataset for the course.\n",
    "\n",
    "Alternatively, dataset on sms spam https://www.kaggle.com/uciml/sms-spam-collection-dataset. I might use this for the tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set up code checking\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.nlp.ex1 import *\n",
    "print(\"\\nSetup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  rating\n",
       "0  Stuning even for the non-gamer: This sound tra...       1\n",
       "1  The best soundtrack ever to anything.: I'm rea...       1\n",
       "2  Amazing!: This soundtrack is my favorite music...       1\n",
       "3  Excellent Soundtrack: I truly like this soundt...       1\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../input/amazon/train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this code takes a really long time, so I'm actually only going to use a small sample from this dataset. Maybe I should make the whole thing available, but have students work with a small sample to optimize for time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(1_000_000, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Input Text\n",
    "\n",
    "In this step, you'll create your feature vectors from the raw text data using scikit-learn's `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the CountVectorizer\n",
    "count_vect = ____\n",
    "\n",
    "# Convert the text data to token counts\n",
    "X_train_counts = ____\n",
    "\n",
    "#q_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(data.text)\n",
    "\n",
    "#step_1.assert_check_passed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many messages contain the word \"tea\"? Should I do this? Might be too advanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6169"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts[:, count_vect.vocabulary_['tea']].data.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize with TF-IDF\n",
    "\n",
    "You have the text data encoded with occurence counts, now normalize those counts relative to the frequency of occurence across the messages. This will prove to be a better representation for models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Create the transformer for TF-IDF\n",
    "tfidf_transformer = ____\n",
    "\n",
    "# Use the transformer to convert the count data to tf-idf frequencies\n",
    "X_train_tfidf = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#step_2.assert_check_passed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000000x448236 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 54884567 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Naive Bayes model\n",
    "\n",
    "With the data processed, we're ready to use it as input to a classification model. Use the `MultinomialNB` model as the classifier. Remember that the Naive Bayes model assumes no relationships or correlations between the input words. As such, this should be the simplest model and can be used as a baseline for developing more powerful models.\n",
    "\n",
    "So that you can measure the performance of the model after training, we'll first split the data into training and validation sets. If this is unfamiliar to you, please take our Intro to Machine Learning mini-course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data.rating\n",
    "# split data into training and validation data, for both features and target\n",
    "train_X, val_X, train_y, val_y = train_test_split(X_train_tfidf, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, train the Naive Bayes model and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = ____\n",
    "# Fit the model on the training data\n",
    "____\n",
    "\n",
    "# Calculate the accuracy using the validation data\n",
    "accuracy = ____\n",
    "\n",
    "#step_3.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846124\n"
     ]
    }
   ],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(train_X, train_y)\n",
    "\n",
    "accuracy = nb_model.score(val_X, val_y)\n",
    "print(accuracy)\n",
    "#step_3.assert_check_passed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have found that the accuracy is quite high, around 96%. However, we should check the confusion matrix to see how the model is misclassifying the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      " [[108564  16664]\n",
      " [ 21805 102967]]\n",
      "\n",
      "Normalized confusion matrix\n",
      "[0.86693072 0.13306928]\n",
      "[0.17475876 0.82524124]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predicted = nb_model.predict(val_X)\n",
    "cm = confusion_matrix(val_y, predicted)\n",
    "\n",
    "print(\"Confusion matrix\\n\", cm)\n",
    "print(\"\\nNormalized confusion matrix\")\n",
    "for row in cm:\n",
    "    print(row / row.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is that there are few actual spam messages in our dataset. This is true of a normal dataset of text messages as well, the vast majority will be non-spam. Our model is able to classify these ham messages with perfect accuracy, but it misses over a quarter of the actual spam messages. In the next tutorial, we'll look at improving our model to better predict when a message is spam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
