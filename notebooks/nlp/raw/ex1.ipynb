{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Classification\n",
    "\n",
    "In the first exercise, you will train a model that classifies Yelp reviews into \"good\" or \"bad\" sentiments. You'll use ScaPy to \n",
    "\n",
    "Original data is from here: https://www.kaggle.com/yelp-dataset/yelp-dataset#yelp_academic_dataset_review.json. That file is ~5GB, I sampled roughly 50,000 reviews from it. I converted the ratings, the \"stars\" field in the JSON data, to a binary 0 & 1. Reviews with 1 or 2 stars are considered \"negative\", encoded with sentiment 0. Reviews with 4 or 5 stars are considered \"positive\" and encoded with sentiment 1. Reviews with 3 stars are \"neutral\" and excluded from the data.\n",
    "\n",
    "The resulting sample has around 75% positive sentiments. Might need to resample to balance the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set up code checking\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.nlp.ex1 import *\n",
    "print(\"\\nSetup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars  sentiment\n",
       "0  Total bill for this horrible service? Over $8G...    1.0          0\n",
       "1  I *adore* Travis at the Hard Rock's new Kelly ...    5.0          1\n",
       "2  I have to say that this office really has it t...    5.0          1\n",
       "3  Went in for a lunch. Steak sandwich was delici...    5.0          1\n",
       "4  Today was my second out of three sessions I ha...    1.0          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv('../input/yelp_ratings.csv', index_col=0)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Exercise: Create the model\n",
    "\n",
    "Here, create the text classifier model and add the labels `\"NEGATIVE\"` and `\"POSITIVE\"`. For the model, use the \"bow\" architecture. The other architectures will likely result in better performance, but train much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    # Create an empty model\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    # Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
    "    textcat = nlp.create_pipe(\n",
    "                \"textcat\",\n",
    "                config={\n",
    "                    \"exclusive_classes\": True,\n",
    "                    \"architecture\": \"bow\"})\n",
    "    nlp.add_pipe(textcat)\n",
    "\n",
    "    # Add NEGATIVE and POSITIVE labels to text classifier\n",
    "    textcat.add_label(\"NEGATIVE\")\n",
    "    textcat.add_label(\"POSITIVE\")\n",
    "    \n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "Here I've included a function to load the data and split it into training and validation slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_file, split=0.8):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = data.sample(frac=1, random_state=7)\n",
    "    \n",
    "    texts = train_data.text.values\n",
    "    labels = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)}\n",
    "              for y in train_data.sentiment.values]\n",
    "    split = int(len(train_data) * split)\n",
    "    \n",
    "    return texts[:split], labels[:split], texts[split:], labels[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels, val_texts, val_labels = load_data('../input/yelp_ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Exercise: Train Function\n",
    "\n",
    "Implement a function `train` that updates a model with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch\n",
    "import random\n",
    "\n",
    "def train(model, train_data, optimizer, batch_size=8):\n",
    "    print(\"Training model!\")\n",
    "    \n",
    "    losses = {}\n",
    "    random.shuffle(train_data)\n",
    "    batches = minibatch(train_data, size=batch_size)\n",
    "    for batch in batches:\n",
    "        # Need to get separate iterables for texts and labels\n",
    "        texts, labels = zip(*batch)\n",
    "        model.update(texts, labels, sgd=optimizer, drop=0.2, losses=losses)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "10.147658804418384\n"
     ]
    }
   ],
   "source": [
    "nlp = create_model()\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "train_data = list(zip(train_texts, [{\"cats\": labels} for labels in train_labels]))\n",
    "\n",
    "losses = train(nlp, train_data, optimizer)\n",
    "print(losses['textcat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Exercise: Making Predictions\n",
    "\n",
    "Implement a function `predict` that uses a model to predict the sentiment of text examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, texts): \n",
    "    # Use the tokenizer to tokenize each input text example\n",
    "    docs = [model.tokenizer(text) for text in texts]\n",
    "    \n",
    "    # Use textcat to get the scores for each doc\n",
    "    textcat = model.get_pipe('textcat')\n",
    "    scores, _ = textcat.predict(docs)\n",
    "    \n",
    "    # From the scores, find the class with the highest score/probability\n",
    "    predicted_class = scores.argmax(axis=1)\n",
    "    \n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0]\n",
      "[\"Some of the best sushi I've ever had. Reasonable prices. Excellent service and drinks.\"\n",
      " \"I would be remiss if I said nothing was good, because the egg rolls were good and the white rice, because it's white rice and that was pretty good. But everything else was kind of sub par. Maybe it's what I ordered. The quality of the chicken on the general tso's was really bad. You hope for crispy chicken with a spicy sauce, but this is soggy and the breading is gross. The lo mien hardly has any vegetables, and is also pretty bleh. Stay far away from the crab rangoons. \\n\\nI wanted to like this place, I had gone to the one in Vermilion and was even more disappointed. Seeing these reviews, and knowing it was under different management  I thought it would be better. The people in the restaurant love it, but it's gross.\"\n",
      " \"One of my favorite Asian restaurants. The food is not typical and seemingly more authentic. There are items on the menu I would have to be a bit more adventurous to try. Often there's a wait for s table but the staff is very accommodating and the owner very friendly\"\n",
      " \"First time there, only one waitress for the whole restaurant, her attitude wasn't nice to our table, but she smiled to all the other customers...., then when we got the bill and paid for it, she came back and told us that the bill hasn't included tips...  we had already left 10% on the tray, what was that about???  Asking for more tips???   Her attitude made us didn't want to leave any at all, but we did anyways!\"]\n"
     ]
    }
   ],
   "source": [
    "print(predict(nlp, val_texts[23:27]))\n",
    "print(val_texts[23:27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye it looks like your model is working well after going through the data just once. However you need to calculate some metric for the model's performance on the hold-out validation data.\n",
    "\n",
    "## 4) Exercise: Evaluating a Trained Model\n",
    "\n",
    "Implement a function that evaluates a `TextCategorizer` model. This function `evaluate` takes a model along with texts and labels. It returns the accuracy of the model, the number of correct predictions divided by all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, texts, labels):\n",
    "    \"\"\" Returns the accuracy of a TextCategorizer model. \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        model: ScaPy model with a TextCategorizer\n",
    "        texts: Text samples, from load_data function\n",
    "        labels: True labels, from load_data function\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get predictions from textcat model\n",
    "    predicted_class = predict(model, texts)\n",
    "    \n",
    "    # From labels, get the true class as a list of integers (POSITIVE -> 1, NEGATIVE -> 0)\n",
    "    true_class = [int(each['POSITIVE']) for each in labels]\n",
    "    \n",
    "    # A boolean or int array indicating correct predictions\n",
    "    correct_predictions = predicted_class == true_class\n",
    "    \n",
    "    # The accuracy, number of correct predictions divided by all predictions\n",
    "    accuracy = correct_predictions.mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9413\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(nlp, val_texts, val_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the functions implemented, you can put it all into a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Loss: 9.925 \t Accuracy: 0.942\n",
      "Training model!\n",
      "Loss: 6.106 \t Accuracy: 0.945\n",
      "Training model!\n",
      "Loss: 5.325 \t Accuracy: 0.949\n",
      "Training model!\n",
      "Loss: 4.870 \t Accuracy: 0.948\n",
      "Training model!\n",
      "Loss: 4.579 \t Accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "nlp = create_model()\n",
    "optimizer = nlp.begin_training()\n",
    "train_data = list(zip(train_texts, [{\"cats\": labels} for labels in train_labels]))\n",
    "for i in range(5):\n",
    "    losses = train(nlp, train_data, optimizer)\n",
    "    accuracy = evaluate(nlp, val_texts, val_labels)\n",
    "    print(f\"Loss: {losses['textcat']:.3f} \\t Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lesson, you'll learn how to use SpaCy to represent tokens as vectors, then use these vectors to train machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
