{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Language\n",
    "\n",
    "In this exercise you will use SpaCy to convert the review text into word vectors. \n",
    "\n",
    "You'll first get these vectors with SpaCy. Then, you'll use them to train a logistic regression model and a linear support vector classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set up code checking\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.nlp.ex2 import *\n",
    "print(\"\\nSetup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars  sentiment\n",
       "0  Total bill for this horrible service? Over $8G...    1.0          0\n",
       "1  I *adore* Travis at the Hard Rock's new Kelly ...    5.0          1\n",
       "2  I have to say that this office really has it t...    5.0          1\n",
       "3  Went in for a lunch. Steak sandwich was delici...    5.0          1\n",
       "4  Today was my second out of three sessions I ha...    1.0          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data = pd.read_csv('../input/nlp-course/yelp_ratings.csv', index_col=0)\n",
    "review_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Need to load the large model to get the vectors\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Get document vectors\n",
    "\n",
    "Use SpaCy to get document vectors from the review text. \n",
    "\n",
    "Returning all 44,500 document vectors takes about 20 minutes, so here you'll need to get only the first 100. For the rest of this exercise, I've provided a file with all 44,500 document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         :(\n",
       "stars         1\n",
       "sentiment     0\n",
       "Name: 7455, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.iloc[7455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just want the vectors so we can turn off other models in the pipeline\n",
    "with nlp.disable_pipes():\n",
    "    vectors = np.array([nlp(review.text).vector for idx, review in review_data[:100].iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will load in the rest of the document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all document vectors from file\n",
    "vectors = np.load('../input/nlp-course/review_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Document Similarity\n",
    "\n",
    "Find the document most similar to an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"One of the last real pubs in San Francisco. They have solid food (the bistro burger, chicken tenders and hummus plate are my favorites). They have a great selection of taps and the owner Neil, is a great guy. It's one of the only non-posh San Francisco pubs you'll find with a clean atmosphere, food and beer. I've been in the neighborhood for 2-3 years and come here weekly. I highly recommend for a relaxing night and some decent food!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector = nlp(text).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/np.sqrt(a.dot(a)*b.dot(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mat/miniconda3/envs/kaggle/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sims = np.array([cosine_similarity(text_vector, vec) for vec in vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.59594957e-02,  1.74750701e-01, -1.43281862e-01, -1.05491593e-01,\n",
       "        1.07150845e-01, -2.63998043e-02,  4.11011614e-02, -2.59563357e-01,\n",
       "        9.51598398e-03,  2.15807128e+00, -7.23177046e-02,  3.73177268e-02,\n",
       "        9.84339975e-03, -6.19993657e-02, -1.73671991e-01, -1.39454722e-01,\n",
       "       -4.87762317e-02,  1.06987953e+00, -1.33403018e-01,  1.82681810e-02,\n",
       "       -5.22443168e-02, -5.80078661e-02, -4.15326608e-03, -1.38557283e-02,\n",
       "       -4.22815606e-02,  5.30283479e-03, -1.57636940e-01, -1.19175091e-01,\n",
       "        1.12210467e-01, -8.98163170e-02, -4.70235758e-02,  4.73255143e-02,\n",
       "       -3.16928029e-02,  5.19502647e-02,  7.86208361e-02, -6.96767718e-02,\n",
       "        3.39470953e-02, -6.29663328e-03, -1.46525025e-01, -1.36079848e-01,\n",
       "        2.27249358e-02,  1.06234916e-01,  7.26362392e-02, -6.96387962e-02,\n",
       "        4.26478758e-02,  7.52198473e-02, -1.64077535e-01, -5.70037402e-02,\n",
       "       -6.06145374e-02,  4.77300072e-03, -8.43887627e-02,  6.30923882e-02,\n",
       "        2.24606059e-02, -1.98788522e-03,  6.00897707e-02, -1.06474636e-02,\n",
       "       -5.42076025e-03, -9.82589498e-02,  4.90811504e-02, -3.18924822e-02,\n",
       "       -7.46057089e-03, -7.58186504e-02,  4.89685610e-02,  1.62842125e-01,\n",
       "        4.48342785e-02, -1.20638914e-01, -1.33535946e-02,  6.83787540e-02,\n",
       "        1.90409701e-02,  1.35479063e-01,  1.39675155e-01,  6.39251322e-02,\n",
       "        1.62093148e-01, -6.10182323e-02,  4.14981656e-02,  3.32488120e-02,\n",
       "        5.03364317e-02, -1.44546360e-01, -3.54307480e-02,  2.80718714e-01,\n",
       "       -5.18715419e-02,  1.01715282e-01, -7.59591954e-03, -2.11890955e-02,\n",
       "        3.99706401e-02, -1.88580409e-01,  1.74192458e-01, -8.13029483e-02,\n",
       "        2.47899413e-01,  1.70399714e-02, -3.65071222e-02,  8.72820020e-02,\n",
       "       -6.03335984e-02,  1.55344829e-02,  1.45775884e-01, -5.42528592e-02,\n",
       "       -5.31733641e-03, -4.78982665e-02,  1.81120448e-02,  5.01581058e-02,\n",
       "       -8.01361427e-02,  8.06408972e-02, -1.22543313e-01, -5.22504412e-02,\n",
       "        9.50355530e-02, -6.46072090e-01,  9.03950781e-02,  1.65460333e-02,\n",
       "        1.96203068e-02, -5.65963984e-02,  3.53334993e-02, -1.17835619e-01,\n",
       "        4.38342653e-02, -1.19539894e-01,  6.07959135e-03, -3.83887663e-02,\n",
       "        4.26584184e-02,  6.09761588e-02, -4.23827097e-02, -6.69091344e-02,\n",
       "        6.27542883e-02, -2.38809995e-02,  1.26647223e-02, -3.74654345e-02,\n",
       "       -6.75164908e-02,  8.10927525e-02, -7.16504902e-02, -1.80397779e-01,\n",
       "        3.15645188e-02, -5.14521860e-02, -4.83951066e-03, -1.62439607e-02,\n",
       "       -4.73250672e-02,  5.09294793e-02,  1.39033258e-01,  1.27769616e-02,\n",
       "       -2.40556849e-03,  1.41053339e-02,  8.81654397e-03, -8.15046653e-02,\n",
       "       -1.48298955e+00,  1.82947889e-01,  2.33964071e-01,  9.06526484e-03,\n",
       "       -1.27307288e-02, -9.12847072e-02, -1.18695334e-01,  4.30722907e-02,\n",
       "        1.26893252e-01, -1.03555426e-01,  7.31650321e-03,  4.00256850e-02,\n",
       "        1.74058661e-01,  6.09981008e-02, -3.16159613e-02, -1.77086666e-02,\n",
       "       -3.96179445e-02, -5.50872721e-02, -1.74115915e-02, -9.22688991e-02,\n",
       "       -5.17591126e-02, -2.79539102e-03,  1.74699891e-02, -6.38561323e-02,\n",
       "       -4.59878035e-02, -1.87157467e-01,  7.59981275e-02,  8.69157165e-03,\n",
       "        1.29216224e-01, -2.28677653e-02, -8.03718269e-02, -3.79992686e-02,\n",
       "        4.46879156e-02, -9.10191983e-02, -7.89010152e-02,  3.40580568e-02,\n",
       "       -4.47184108e-02, -1.32930055e-02, -9.78086218e-02, -6.79745525e-02,\n",
       "       -7.40480283e-03, -1.71332031e-01, -1.77558318e-01, -8.80429968e-02,\n",
       "       -6.25482798e-02, -1.75019819e-02, -7.90388212e-02,  5.16799167e-02,\n",
       "        4.92130406e-02,  1.46883875e-02,  1.04732724e-04,  1.23211434e-02,\n",
       "       -1.42023817e-01,  9.69810039e-02,  2.09795386e-02,  1.19450847e-02,\n",
       "       -4.30224910e-02, -7.80191496e-02, -6.84662536e-02,  8.23771432e-02,\n",
       "       -1.96845662e-02, -2.17635129e-02, -1.33757502e-01, -5.85997514e-02,\n",
       "        1.74432576e-01,  4.36496325e-02,  2.77477428e-02,  1.68944988e-02,\n",
       "        7.64607936e-02,  2.50647943e-02, -1.20480262e-01, -7.63081759e-02,\n",
       "       -7.00027198e-02, -1.88766465e-01,  1.26211092e-01,  1.73348576e-01,\n",
       "       -7.07633793e-02, -2.48832460e-02, -1.83811858e-01,  9.89454053e-03,\n",
       "       -1.78345386e-02, -3.54948379e-02, -8.60723108e-02,  4.10044342e-02,\n",
       "        8.73854607e-02, -3.31055447e-02, -4.16106954e-02,  1.09338209e-01,\n",
       "       -5.66798681e-03,  3.66465263e-02, -6.99680820e-02,  3.02095860e-02,\n",
       "        1.34445548e-01,  9.86359119e-02, -1.78874005e-02, -9.61239487e-02,\n",
       "        5.19823879e-02, -1.21295698e-01, -1.10421114e-01,  1.26372814e-01,\n",
       "        7.42838830e-02, -4.41348366e-03,  7.10835680e-03,  1.50268197e-01,\n",
       "        1.09058633e-01, -9.53650624e-02, -1.03502758e-01, -2.04304308e-01,\n",
       "       -1.13960825e-01,  1.47297233e-01,  4.07494009e-02, -9.78460610e-02,\n",
       "       -6.08493900e-03, -1.00220572e-02, -4.22487361e-03,  1.93547547e-01,\n",
       "        1.10952534e-01, -1.18279010e-01, -6.45217374e-02, -1.31933317e-02,\n",
       "        1.11454271e-01,  1.19165398e-01,  3.65532264e-02,  3.01371105e-02,\n",
       "        1.08591132e-01, -5.17540332e-03,  4.70786681e-03,  6.44822195e-02,\n",
       "        1.48198172e-01,  3.48621048e-02,  2.30264738e-02, -6.90531507e-02,\n",
       "       -6.52346984e-02, -1.68707043e-01, -1.39644250e-01,  2.25813277e-02,\n",
       "       -2.90170535e-02,  7.82843865e-03,  3.06694731e-02,  2.23903656e-01,\n",
       "        2.20180705e-01,  3.99484634e-02,  9.26679000e-03, -6.38448149e-02,\n",
       "       -5.82860634e-02, -6.19028807e-02,  1.15550317e-01, -7.38333538e-02,\n",
       "        1.03188746e-01, -3.83210443e-02, -1.88098967e-01,  4.45551090e-02,\n",
       "        2.75397692e-02, -6.78055175e-03,  8.94323364e-02,  5.04489765e-02,\n",
       "       -4.19479683e-02, -1.00846924e-01,  3.84074338e-02,  1.21758342e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[7456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7455"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, review_data.sentiment, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Fit a logistic regression model\n",
    "\n",
    "Use the document vectors to train a scikit-learn logistic regression model and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "model = LogisticRegression(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, model.predict(X_test))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Find the best regularization parameter\n",
    "\n",
    "You can find a model with better performance by searching through hyperparameter values. For example, here you can adjust the regularization parameter `C`. The easiest way to do this is with cross-validation using `LogisticRegressionCV`. This will automatically split up your data into folds and measure the scoring metric for each fold. Using this, you can search through a range of values for `C` to maximize the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "c_vals = [0.1, 1, 10, 100, 1000, 10000]\n",
    "model = LogisticRegressionCV(Cs=c_vals, scoring='accuracy',\n",
    "                             cv=5, max_iter=10000,\n",
    "                             random_state=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test the model's performance on the hold-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model test accuracy: {model.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Try a different model\n",
    "\n",
    "It's possible to get better accuracy using a different model. Here, try scikit-learn's `LinearSVC` model to see if you can improve on the logisitic regression model. Again, use cross-validation to find the best value for the regularization parameter. This time you'll need to do the cross-validation yourself with `cross_val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, models = [], []\n",
    "for c in c_vals:\n",
    "    clf = LinearSVC(C=c, random_state=1, dual=False, max_iter=10000)\n",
    "    cv_score = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    scores.append(cv_score.mean())\n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = np.array(scores).argmax()\n",
    "model = models[max_score].fit(X_train, y_train)\n",
    "print(f'Model test accuracy: {model.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an accuracy of 94.6%, slightly better than the best logistic regression model.\n",
    "\n",
    "Congratulations on finishing this course! At this point you know how to get embeddings for each word in the documents, but you're only using the averaged document vectors with these models. Using the word vectors themselves might result in even better performing models. To do this you'll want to use a recurrent neural network (RNN for short). We won't cover RNNs in this course, but look them up if you want to learn about state-of-the-art NLP models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
