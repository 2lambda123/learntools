{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "As I noted earlier, when modeling natural language you need to represent text or speech as vectors. In the last exercise you used bag of words vectors as the model input. We can, of course, improve on bag of words which is typically considered a baseline encoding.\n",
    "\n",
    "Word embeddings (also called word vectors) are vector encodings for words that are learned by considering the context in which the words appear. Words that appear in similar contexts will have similar vectors. For example, vectors for \"leopard\", \"lion\", and \"tiger\" will be close together, while they'll be far away from \"galaxy\", \"oolong\", or \"castle\".\n",
    "\n",
    "What's even cooler is that relations between words are often maintained. Subtracting the vectors for \"man\" and \"woman\" will return another vector. If you add that to the vector for \"king\" you'll end up at the vector for \"queen\" (or at least close by).\n",
    "\n",
    "![Word vector examples](https://www.tensorflow.org/images/linear-relationships.png)\n",
    "\n",
    "These vectors can be used as features for machine learning models. Word vectors will typically improve the performance of your models above bag of words encoding. SpaCy uses embeddings learned from the Word2Vec model. To access them, you need to load one of the large language models, like `en_core_web_lg`. Then they will be available on tokens from the `.vector` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# Need to load the large model to get the vectors\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling other pipes because we don't need them and it'll speed up this part a bit\n",
    "text = \"These vectors can be used as features for machine learning models.\"\n",
    "with nlp.disable_pipes():\n",
    "    vectors = np.array([token.vector for token in  nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings from SpaCy are 300 dimension vectors. You see here that we get one vector for each word. However, we only have document-level labels and our models won't be able to use the word-level embeddings. Instead, you need a vector representation for the entire document. A document vector is calculated by averaging the word vectors for each token in the document. Then, these document vectors are used to train the model. \n",
    "\n",
    "SpaCy calculates the average document vector which you can get with `doc.vector`. Here I'll load in the spam data and convert the text to document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the spam data\n",
    "# ham is the label for non-spam messages\n",
    "spam = pd.read_csv('../input/nlp-course/spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a minute or so\n",
    "with nlp.disable_pipes():\n",
    "    doc_vectors = np.array([nlp(text).vector for text in spam.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02204493,  0.09757433,  0.00255367, ..., -0.09383627,\n",
       "        -0.00796944,  0.19099015],\n",
       "       [-0.07367852, -0.19237824, -0.1709596 , ..., -0.003535  ,\n",
       "         0.02119275,  0.17852125],\n",
       "       [ 0.01289313, -0.0072732 , -0.00627819, ..., -0.03291722,\n",
       "         0.08602007,  0.12489114],\n",
       "       ...,\n",
       "       [-0.05494839,  0.19570266, -0.13729948, ..., -0.10815047,\n",
       "        -0.02305553,  0.18632641],\n",
       "       [-0.06460267,  0.17402254, -0.21391848, ..., -0.02812874,\n",
       "         0.04536904,  0.21436742],\n",
       "       [ 0.09184885,  0.14416684, -0.2082083 , ...,  0.03214014,\n",
       "        -0.04769757,  0.19402859]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the document vectors, we can train scikit-learn models using all the normal methods like cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_vectors, spam.label, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "model = LogisticRegression(random_state=1, solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, model.predict(X_test))\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that scikit-learns' `LogisticRegression` model uses L2 regularization by default. You typically want to regularize the model, but the default parameter `C=1` is likely not the best choice. You can find the best regularization parameter with something like `LogisticRegressionCV` which will perform cross validation over a range of values for `C`.\n",
    "\n",
    "Another good model to try for text classification is a support vector machine (SVM). A convenient model to use for this is the linear support vector classifier, `LinearSVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9731182795698925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Set dual=False to speed up training, and it's not needed\n",
    "svc = LinearSVC(random_state=1, dual=False, max_iter=10000)\n",
    "svc.fit(X_train, y_train)\n",
    "print(\"Accuracy: \", svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, note that `LinearSVC` uses L2 regularization by default. So, you'll want to use cross validation to find the best regularization paramater. Scikit-learn doesn't have a convenient class like `LogisticRegressionCV` to do this, but you can use `cross_validation_score` and write the equivalent code.\n",
    "\n",
    "Next, you'll get word vectors for the Yelp reviews and try to improve on the classifier you trained with SpaCy. If you aren't familiar with `LogisticRegressionCV` and `LinearSVC`, you'll want to work through our Intermediate Machine Learning course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
