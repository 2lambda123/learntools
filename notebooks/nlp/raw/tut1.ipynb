{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Data comes in many different forms such as time stamps, sensor readings, images, categorical labels, and much much more. A large amount of data exists as language, in text and speech.  The field of using computers to understand language data is known as Natural Language Processing (NLP).\n",
    "\n",
    "This understanding can come in the form of information extraction for observing trends. For example, Google can scan billions of searches to track how specific terms change in frequency over time.\n",
    "\n",
    "![Google trends for SpaCy, NLTK, and Gensim libraries](https://i.imgur.com/QR7eIjt.png)\n",
    "\n",
    "Or consider that some term is showing up in a lot of customer support tickets. You can have a program observe these tickets for frequent terms and alert the appropriate product team. \n",
    "\n",
    "Within machine learning, and in this course, we're more interested in using language data to build predictive models. As with other domains, much of the work in NLP is finding ways to represent text or speech such that it can be used with machine learning models. That is, we need to convert documents or words or even individual characters into numbers and vectors. These vectors can then be used as input to models.\n",
    "\n",
    "As with other domains, you can break down NLP into supervised and unsupervised tasks. Within supervised learning, you have applications like spam detection, machine translation, and voice recognition. A common use case of unsupervised learning is topic modeling, or clustering documents into topics. In this course you'll focus on supervised text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note before you get started\n",
    "\n",
    "This mini-course was built assuming you already have some experience with machine learning. If you don't have experience with supervised learning and the scikit-learn library, please take the Intro to Machine Learning and Intermediate Machine Learning mini-courses before continuing on with these tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into this dataset https://www.kaggle.com/crowdflower/twitter-airline-sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with SpaCy\n",
    "\n",
    "In this course you'll be using the spaCy library to extract information from text and to convert text into vectors for classification models. SpaCy is relatively new and has quickly become the most popular Python frameworks. Personally, I find it to be intuitive to use and backed up by excellent documentation.\n",
    "\n",
    "To use spaCy, you need to load a **model**. Models are language specific and come in different sizes, typically small, medium, and large. Larger models have more capabilities but also consume more memory, run slower, and take longer to load.\n",
    "\n",
    "To use a spaCy model, you load it with `spacy.load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code loads the small English model. When loading models, you might run into an error like this:\n",
    "```\n",
    "OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
    "```\n",
    "This error means the model doesn't exist on your machine. You'll need to download the model with SpaCy by running\n",
    "\n",
    "```\n",
    "python -m spacy download model_name\n",
    "```\n",
    "\n",
    "in your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model loaded, we can use it to process some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tea is healthy, calming, and delicious, don't you think?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing\n",
    "\n",
    "This returns a document object that contains **tokens**. A token is one unit of text in the document, such as individual words and punctuation. SpaCy splits contractions like \"don't\" into two tokens, \"do\" and \"n't\". To get the tokens you iterate through the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea\n",
      "is\n",
      "healthy\n",
      ",\n",
      "calming\n",
      ",\n",
      "and\n",
      "delicious\n",
      ",\n",
      "do\n",
      "n't\n",
      "you\n",
      "think\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through a document gives you token objects. Each of these tokens comes with additional information. In most cases, the important ones are `token.lemma_` and `token.is_stop`.\n",
    "\n",
    "# Text preprocessing\n",
    "\n",
    "The \"lemma\" of a word is its base form. For example, \"to be\" is the root verb of \"is\". The lemma of \"is\" then, is \"be\". Removing prefixes and suffixes also results in lemmas, such as changing \"calming\" to \"calm\". Converting words in text to their lemma version is often called \"lemmatizing\" or \"normalization\".\n",
    "\n",
    "Stopwords are words that occur frequently in the language and don't contain much information. In English, stopwords include \"the\", \"is\", \"and\", \"but\", \"not\". With a spaCy token, `token.lemma_` returns the lemma, while `token.is_stop` returns a boolean `True` if the token is a stopword and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          Lemma          Stopword\n",
      "----------------------------------------\n",
      "Tea            tea            False\n",
      "is             be             True\n",
      "healthy        healthy        False\n",
      ",              ,              False\n",
      "calming        calm           False\n",
      ",              ,              False\n",
      "and            and            True\n",
      "delicious      delicious      False\n",
      ",              ,              False\n",
      "do             do             True\n",
      "n't            not            True\n",
      "you            -PRON-         True\n",
      "think          think          False\n",
      "?              ?              False\n"
     ]
    }
   ],
   "source": [
    "print(\"{:<15}{:<15}{}\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(\"{:<15}{:<15}{}\".format(str(token), token.lemma_, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are lemmas and identifying stopwords important? Language data tends to have a lot of noise mixed in with informative content. In the sentence above, the important words are tea, healthy, calming, and delicious. Removing the stop words might improve the quality of the data for use in predictive models. Using lemma forms helps reduce noise as well by reducing multiple forms of the same word into one base form (\"calming\", \"calms\", \"calmed\" would all change to \"calm\").\n",
    "\n",
    "However, lemmatizing and dropping stopwords might result in your models performing worse. You'll need to treat this preprocessing as part of your hyperparameter optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Matching\n",
    "\n",
    "Another common use of spaCy is matching tokens or phrases within chunks of text or documents. Pattern matching is commonly done with regular expressions, but spaCy's matching capabilities are much easier to use.\n",
    "\n",
    "Create a matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add patterns to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things I need to cover:\n",
    "\n",
    "- loading SpaCy models\n",
    "- Tokenizing\n",
    "- a brief bit on lemmas and stopwords\n",
    "- Creating a matcher (You could use regex, but this is much better)\n",
    "- Iterating through text with a matcher\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
