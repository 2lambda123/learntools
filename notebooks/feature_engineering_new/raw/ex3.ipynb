{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction #\n",
    "\n",
    "In this exercise you'll apply PCA to a dataset with a fairly large number of features. By examining the variance explained by the principal components, you'll be able to reduce the number of features your models uses by more than half, while also reducing predictive error.\n",
    "\n",
    "Run this cell to set everything up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "# Setup feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.feature_engineering_new.ex3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Gold* dataset contains lagged percent-returns from various financial securities (like stocks and bonds). The task is to predict the price of gold on a three-week horizon using these past returns.\n",
    "\n",
    "Run the next cell to set up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv(\"../input/fe-course-data/gold.csv\")\n",
    "display(df.head())\n",
    "X = df.copy()\n",
    "y = X.pop(\"Gold_T+22\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Define Transforms #\n",
    "\n",
    "The *Gold* dataset has features that are all numeric and that are all measuring the same kind of thing (returns). This makes it a good candidate for PCA. Define three transforms:\n",
    "1. a `PolynomialFeatures` instance to create interaction features\n",
    "2. a `PowerTransformer` to normalize the data\n",
    "3. a `PCA` instance that retains all components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# YOUR CODE HERE: Define the feature transforms\n",
    "polynomial_features = PolynomialFeatures(\n",
    "    degree=2, interaction_only=True, include_bias=False,\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's determine how many components to keep. Run this next cell for a plot of the explained variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_pca = scaler.fit_transform(X)\n",
    "X_pca = pca.fit_transform(X_pca)\n",
    "\n",
    "explained = pca.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "c_95 = next(i+1 for i, x in enumerate(cumulative) if x >= 0.95)\n",
    "plt.figure(dpi=100, figsize=(8, 4))\n",
    "plt.subplot(121)\n",
    "plt.plot(explained)\n",
    "plt.axvline(x=c_95, color='k')\n",
    "plt.title(\"Explained Variance per Component\")\n",
    "plt.subplot(122)\n",
    "plt.plot(cumulative)\n",
    "plt.axvline(x=c_95, color='k')\n",
    "plt.title(\"Cumulative Variance\")\n",
    "plt.show();\n",
    "\n",
    "print(\"95% explained variance at {} components.\".format(c_95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems in this case that there isn't a clear \"elbow\" in the graph so we'll have to make a guess about how many components to retain. Setting a threshold of 95% variance (the vertical line) seems to give us a good compromise between dimension reduction and information loss.\n",
    "\n",
    "# Step 2 - Define PCA with Retained Components #\n",
    "\n",
    "Redefine the PCA transform retain enough components to explain at least 95% of the variance in the dataset, as indicated in the plots above. You can either specify the number of components or the percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: define the PCA transform\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Define Pipeline\n",
    "\n",
    "Now create the complete pipeline you'll use for prediction, using XGBoost as before. Pay attention to the order the transforms occur in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# YOUR CODE HERE: Create the XGBRegressor, then define the complete pipeline \n",
    "rf = RandomForestRegressor(n_jobs=-1)\n",
    "pipeline = make_pipeline(\n",
    "    scaler, pca, rf,\n",
    ")\n",
    "\n",
    "\n",
    "# Check your answer\n",
    "q_3.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Estimate Performance #\n",
    "\n",
    "Now evaluate the complete pipeline with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# YOUR CODE HERE: Cross-validate\n",
    "score = cross_val_score(\n",
    "    pipeline, X, y, cv=5, scoring='neg_mean_absolute_error'\n",
    ")\n",
    "score = -1 * score.mean()\n",
    "print(\"Score: {:.4f}\".format(score))\n",
    "\n",
    "\n",
    "# Check your answer\n",
    "q_4.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can compare this pipeline to one without the PCA transform by replacing `pipeline` with `rf` in the cell above and rerunning.\n",
    "\n",
    "# Keep Going #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
