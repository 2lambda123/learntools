{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "We've learned how feature hashing is an efficient technique for dealing with high-cardinality categorical features. In this exercise you'll apply feature hashing to large dataset with a feature having over 3000 categories.\n",
    "\n",
    "Run this cell to set everything up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 31415\n",
    "\n",
    "# Setup feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.feature_engineering_new.ex4 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll create a model to predict the rating an audience will give to movies various genres. The *MovieLens 1 Million* dataset contains one million movie ratings together with the movie's genre and some simple demographic data like the rater's occupation and zipcode.\n",
    "\n",
    "Run the next cell to set up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv(\"../input/fe-course-data/movielens1m.csv\")\n",
    "display(df.head())\n",
    "print(\"Number of Unique Zipcodes: {}\".format(df[\"Zipcode\"].nunique()))\n",
    "\n",
    "X = df.copy()\n",
    "X.drop(['User ID', 'Movie ID'], inplace=True, axis=1)\n",
    "y = X.pop(\"Rating\").to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Define Transforms #\n",
    "\n",
    "Notice that there are quite a large number of categories in the `Zipcode` feature with the possibility of unseen categories in future data. This makes it a good candidate for feature hashing.\n",
    "\n",
    "Define a feature hasher that will hash the zipcodes into 400 features. You'll need to use `make_column_transformer` to restrict the hasher to the `Zipcode` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "\n",
    "# YOUR CODE HERE: Define a feature hasher for the Zipcode column\n",
    "hasher = FeatureHasher(input_type='string', n_features=400)\n",
    "transformer = make_column_transformer(\n",
    "    (hasher, \"Zipcode\"),\n",
    "    remainder='passthrough',\n",
    ")\n",
    "\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Create Pipeline #\n",
    "\n",
    "Define a pipeline with the transformer you just defined followed by an XGBoost regressor. Since this dataset is quite large, use XGBoost's histogram algorithm with `tree_method='hist'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# YOUR CODE HERE: Create a pipeline with a FeatureHasher followed by XGBRegressor\n",
    "xgb = XGBRegressor(tree_method='hist')\n",
    "pipeline = make_pipeline(transformer, xgb)\n",
    "\n",
    "\n",
    "# Check your answer\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Estimate Generalization Error #\n",
    "\n",
    "Estimate the generalization error using 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "score = cross_val_score(\n",
    "    pipeline, X, y, cv=3, scoring='neg_mean_absolute_error'\n",
    ")\n",
    "score = -1 * score.mean()\n",
    "print(\"XGB with Feature Hashing: {:.4f}\".format(score))\n",
    "\n",
    "\n",
    "# Check your answer\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Tune #\n",
    "\n",
    "It's sometimes worth tuning the number of features the hasher creates. Use `GridSearchCV` to compare the model with 400 features to one with 10 and one with 2000 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "num_features = [10, 2000]\n",
    "param_grid = {\"columntransformer__featurehasher__n_features\": num_features}\n",
    "\n",
    "# YOUR CODE HERE: Tune the number of features created\n",
    "tuner = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_absolute_error')\n",
    "tuner.fit(X, y)\n",
    "\n",
    "\n",
    "# Check your answer\n",
    "q_3.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell without changes to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"Best Score: {}\".format(-1 * tuner.best_score_))\n",
    "print(\"Best Params: {}\".format(tuner.best_params_))\n",
    "results = pd.DataFrame(\n",
    "    -1 * tuner.cv_results_['mean_test_score'],\n",
    "    index=num_features,\n",
    "    columns=['CV Score'],\n",
    ")\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Evaluate #\n",
    "\n",
    "Were the results suprising to you? Can you think of an explanation? How does that affect your evaluation of the usefulness of feature hashing?\n",
    "\n",
    "After you've thought about it, run the next cell for some discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Check your answer\n",
    "q_4.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Solution: Feature hashing creates collisions, but many ML models (including XGBoost) can learn to work around them. That's why 400 hashed features was as good as 2000 in this case. Hashing into only 10 features though clearly reduced performance.\n",
    "\n",
    "For high-cardinality features, the hashing trick is very useful. It can reduce computational requirements while preserving predictive performance.\n",
    "```\n",
    "\n",
    "# Keep Going #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
