{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "First encountering a new dataset can sometimes feel overwhelming. You might be presented with hundreds or thousands of features without even a description to go by. Where do you even begin?\n",
    "\n",
    "A great first step is to construct a ranking with a **feature utility metric**, a function measuring associations between a feature and the target. Then you can choose a smaller set of the most useful features to develop initially and have more confidence that your time will be well spent.\n",
    "\n",
    "Mutual information is a great general-purpose metric and especially useful at the start of feature development when you might not know what model you'd like to use yet. It is:\n",
    "- easy to use and interpret,\n",
    "- computationally efficient,\n",
    "- theoretically well-founded,\n",
    "- resistant to overfitting, and,\n",
    "- able to detect any kind of relationship (not just linear).\n",
    "\n",
    "# Mutual Information and What it Measures #\n",
    "\n",
    "The **mutual information** (MI) between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If you knew the value of a feature, how much more confident would you be about the target?\n",
    "\n",
    "Here is an example from the *California Housing* dataset; each point shows a population group of a few hundred to a few thousand.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/Umt7qHG.png\" width=400, alt=\"Points (blue) representing samples from the California Housing dataset with longitude -120 highlighted in red.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Knowing the longitude reduces uncertainty about the latitude.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Suppose we know the longitude of some point is -120Â°. How much more certain can we be about its latitude? As we can see from the figure, the latitude restricts to a narrower range of values: knowing the longitude reduces our uncertainty about the latitude. Taking the average reduction across all points of longitude gives us the mutual information.\n",
    "\n",
    "That was just the uncertainty decrease at a single point of longitude. The mutual information is: <mark>TODO</mark>. In pseudocode:\n",
    "```\n",
    "mutual_info(y, x) = uncertainty(y | x) - uncertainty(x) + uncertainty(y)\n",
    "```\n",
    "\n",
    "(Technical note: What we're calling uncertainty is measured using a quantity from information theory known as \"entropy\", meaning roughly: \"how many yes-or-no questions you would need to describe an occurance of something, on average.\")\n",
    "\n",
    "# Interpreting Mutual Information Scores #\n",
    "\n",
    "The least possible mutual information between quantities is 0.0. When MI is zero, the quantities are independent: neither can tell you anything about the other. Conversely, in theory there's no upper bound to what MI can be. In practice though values above 2.0 or so are uncommon. (Mutual information is a logarithmic quantity, so it increases very slowly.)\n",
    "\n",
    "The next figure will give you an idea of how MI values correspond to the kind and degree of association a feature has with the target.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/Dt75E1f.png\" width=800, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center><strong>Left:</strong> Mutual information increases as the dependence between feature and target becomes tighter. <strong>Right:</strong> Mutual information can capture any kind of association (not just linear, like correlation.)\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Here are some things to remember when applying mutual information:\n",
    "- MI can help you to understand the *relative potential* of a feature as a predictor of the target, considered by itself.\n",
    "- It's possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI *can't detect interactions* between features. It is a **univariate** metric.\n",
    "- The *actual* usefulness of a feature *depends on the model you use it with*. A feature is only useful to the extent that its relationship with the target is one your model can learn. Just because a feature has a high MI score doesn't mean your model will be able to do anything with that information. You may need to transform the feature first to expose the association.\n",
    "\n",
    "# Example - 1985 Automobiles #\n",
    "\n",
    "The *Automobile* dataset consists of 193 cars from the 1985 model year with a `price` target and 23 features. In this example, we'll rank its features with mutual information and investigate the results by data visualization.\n",
    "\n",
    "This hidden cell imports some libraries and loads the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "df = pd.read_csv(\"../input/fe-course-data/autos.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn algorithm for MI treats discrete features differently from continuous features. Consequently, you need to tell it which are which. As a rule of thumb, anything that *must* have a `float` dtype is *not* discrete. Categoricals (`object` dtype) can be treated as discrete by giving them a label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(\"price\")\n",
    "\n",
    "# Label encoding for categoricals\n",
    "for colname in X.select_dtypes(\"object\"):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "discrete_features = X.dtypes == int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has two mutual information metrics in its `feature_selection` module: one for real-valued targets (`mutual_info_regression`) and one for categorical targets (`mutual_info_classif`). Our target, `price`, is real-valued. The next cell computes the MI scores for our features and wraps them up in a nice dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "mi_scores[::3]  # show a few features with their MI scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a bar plot to make comparisions easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_utility_scores(scores):\n",
    "    y = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(y))\n",
    "    ticks = list(y.index)\n",
    "    plt.barh(width, y)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_utility_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualization is a great follow-up to a utility ranking. Let's take a closer look at a couple of these.\n",
    "\n",
    "As we might expect, the high-scoring `curb_weight` feature exhibits a strong relationship with `price`, the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"curb_weight\", y=\"price\", data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fuel_type` feature has a fairly low MI score, but as we can see from the figure, it clearly separates two `price` populations with different trends within the `horsepower` feature. This indicates that `fuel_type` contributes an interaction effect and might not be unimportant after all. Before deciding a feature is unimportant from its MI score, it's good to investigate any possible interaction effects -- domain knowledge can offer a lot of guidance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"horsepower\", y=\"price\", hue=\"fuel_type\", data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualization is a great addition to your feature-engineering toolbox. Along with utility metrics like mutual information, visualizations like these can help you discover important relationships in your data. Check out our [Data Visualization](https://www.kaggle.com/learn/data-visualization) course to learn more!\n",
    "\n",
    "# Keep Going #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
