{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "After you've identified a good set of features to start developing, what kinds of things could you do with them then? That's what the remainder of this course is about. In this lesson you'll learn some transformations that can be done completely in dataframes. If you're feeling rusty, we've got a great [course on Pandas](https://www.kaggle.com/learn/pandas).\n",
    "\n",
    "# Arithmetic Transforms #\n",
    "\n",
    "For numeric data, arithmetic transforms are often useful. Domain knowledge can be especially useful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do computations over the columns of a dataframe just as if they were individual numbers. It's common to combine features with ordinary arithmetic operations.\n",
    "```\n",
    "df[\"Feature_1\"] + df[\"Feature_2\"]\n",
    "df[\"Feature_1\"] - df[\"Feature_2\"]\n",
    "df[\"Feature_1\"] * df[\"Feature_2\"]\n",
    "df[\"Feature_1\"] / df[\"Feature_2\"]\n",
    "```\n",
    "Products and ratios are especially worth investigating if you had discovered any interaction effects. Ratios especially are difficult for most machine learning models to discover on their own.\n",
    "\n",
    "Power transforms and logarithms are common ways of transforming individual features.\n",
    "```\n",
    "np.log(df[\"Feature\"])  # import numpy as np\n",
    "df[\"Feature\"] ** 2\n",
    "```\n",
    "These kinds of transforms are often applied to reduce skewness; check out our [lesson on normalization](https://www.kaggle.com/alexisbcook/scaling-and-normalization) in *Data Cleaning* where you can also learn about the *Box-Cox transformation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to count up the number of features satisfying some property. Examples could be a set of binary features indicating risk factors for some disease, or sensor recordings of neuronal activity, and you want to count how many neurons pass some threshold.\n",
    "```\n",
    "df[[\"Binary_1\", \"Binary_2\", \"Binary_3\"]].sum(axis=1)\n",
    "df[[\"Numeric_1\", \"Numeric_2\", \"Numeric_3\"]].gt(0).sum(axis=1)\n",
    "```\n",
    "Tree-based models (like random forests and XGBoost) don't have a natural way of integrating information across large numbers of features at once. Counting up properties across many features could be a good idea for these kinds of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine transformations to compute any formulas you might come across. If you had a dataframe containing the features `'AirTempF'` and `'WindSpdMPH'`, you could add a feature for the corresponding [wind-chill](https://www.weather.gov/oun/safety-winter-windchill) (in US units) like:\n",
    "```\n",
    "df[\"WindChill\"] = (\n",
    "    35.74\n",
    "    + 0.6215 * df[\"AirTempF\"]\n",
    "    - 35.75 * df[\"WindSpdMPH\"] ** 0.16\n",
    "    + 0.4275 * df[\"AirTempF\"] * df[\"WindSpdMPH\"] ** 0.16\n",
    ")\n",
    "```\n",
    "\n",
    "Doing a bit of research about your problem domain during feature engineering can pay off with ideas for new features. If the experts agree some feature is useful, there's a good chance your model will find it useful too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Up and Breaking Down Features #\n",
    "\n",
    "Categorical features often show up in a dataframe as strings. You can do column-wise operations on strings using Pandas `pd.Series.str` methods. (Note that the `str` methods only work with `Series` -- that is, single columns -- and not on entire dataframes.)\n",
    "\n",
    "Often you'll have complex strings that can usefully be broken into simpler pieces. Common examples might be: \n",
    "- ID numbers: `'123-45-6789'`\n",
    "- Phone numbers: `'(999) 555-0123'`\n",
    "- Street addresses: `'8241 Kaggle Ln., Goose City, NV'`\n",
    "- Internet addresses: `'http://www.kaggle.com`\n",
    "- Product codes: `'0 36000 29145 2'`\n",
    "- Dates and times: `'Mon Sep 30 07:06:05 2013'`\n",
    "\n",
    "These kinds of things will usually be structured in some logical way and can yield a suprising amount of information in addition to what's in the string itself. As always, some research can pay off here.\n",
    "\n",
    "Say we had a dataframe with instances like:\n",
    "\n",
    "| Location                  | AccountNum |\n",
    "|---------------------------|------------|\n",
    "| \"Los Angeles, California\" | 123456     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could decompose these using Pandas' native string methods.\n",
    "\n",
    "```\n",
    "df[[\"City\", \"State\"]] = df[\"Location\"].str.split(\",\", expand=True)\n",
    "df[\"Branch\"] = df[\"AccountNum\"].str.slice(stop=3)\n",
    "\n",
    "df.head()\n",
    "```\n",
    "\n",
    "You could also join simpler features to create a composed feature. This would help your model detect interactions between the two.\n",
    "\n",
    "```\n",
    "df[\"City\"] + \", \" + df[\"State\"]\n",
    "```\n",
    "\n",
    "Similar tricks work with numeric data. Want to know if prices ending in $0.99 lead to more sales? You can split numbers into whole and fractional parts with the Numpy function `np.modf`. It returns a tuple, so we'll create the features accordingly.\n",
    "\n",
    "```\n",
    "df[\"Cents\"], df[\"Dollars\"] = np.modf(df[\"Price\"])\n",
    "```\n",
    "\n",
    "# Group Transforms #\n",
    "\n",
    "**Group transforms** aggregate information across multiple rows. If you had discovered a category interaction, a group transform over that categry could be something good to investigate.\n",
    "\n",
    "It's common to do statistical aggregations group-wise. The first line computes the average income within each state. The second computes how different each person's income is from the average of the state they live in.\n",
    "```\n",
    "df[\"AverageIncome\"] = df.groupby(\"State\")[\"Income\"].transform(\"mean\")\n",
    "df[\"DiffAvgIncome\"] = df[\"Income\"] - df[\"AverageIncome\"]\n",
    "```\n",
    "\n",
    "Somewhat similar is computing the *frequency* of a feature's values. This would give the frequency of each state in the dataset.\n",
    "```\n",
    "df[\"StateFreq\"] = df.groupby('State')['State'].transform('size') / len(df)\n",
    "```\n",
    "You could use this as an alternative to a one-hot or label encoding for a categorical feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using training and validation splits, it's best to create a grouped feature using only the training set and then join it to the validation set. Using the entire set will create a dependency between the splits, which can throw off cross-validation.\n",
    "\n",
    "This will join an `AverageIncome` feature created on a training set `df_train` to the validation set `df_valid`. (There should be, of course, only one average income per state.)\n",
    "```\n",
    "df_valid = pd.merge(\n",
    "    df_valid,\n",
    "    df_train[[\"State\", \"AverageIncome\"]].drop_duplicates(),\n",
    "    on=\"salary\",\n",
    "    how=\"left\",\n",
    ")\n",
    "```\n",
    "\n",
    "Group transforms are easiest to use when the grouping categories are known and relatively frequent. \n",
    "\n",
    "*Unknown categories*: Pandas will create missing values (`NaN`) for categories that aren't in the training set when you perform the `merge`. You could fill them in with the ungrouped transform.\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "*Rare categories*: A mean computed from only two or three values isn't likely to be very accurate. Consider \n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "# Example - 1985 Automobiles #\n",
    "\n",
    "Let's develop the features we chose in the last lesson. Here they are as a reminder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"../input/fe-course-data/autos.csv\")\n",
    "df.head()\n",
    "\n",
    "top_features = [\n",
    "    \"curb_weight\",\n",
    "    \"horsepower\",\n",
    "    \"highway_mpg\",\n",
    "    \"city_mpg\",\n",
    "    \"width\",\n",
    "    \"length\",\n",
    "    \"wheel_base\",\n",
    "    \"bore\",\n",
    "    \"fuel_system\",\n",
    "]\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a drill-down and create our first set of derived features.\n",
    "\n",
    "It can help to start with a bit of research.\n",
    "\n",
    "The \"displacement\" of an automotive engine is a measure of its power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"displacement\"] = (\n",
    "    np.pi * ((0.5 * df.bore) ** 2) * df.stroke * df.num_of_cylinders\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the \"stroke ratio\" is a measure of how efficient an engine is vs how performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stroke_ratio\"] = df.stroke / df.bore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insurance companies sometimes define the \"size class\" of a vehicle in terms of its shadow and curb_weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['shadow'] = df.length * df.width\n",
    "df['size_class'] = df.shadow * df.curb_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vehicle's \"wheel base\" can determine the smoothness of its ride. Luxury versions sometimes have an extended wheel base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"wheel_base_diff\"] = df[\"wheel_base\"] - df.groupby(\"make\")[\n",
    "    \"wheel_base\"\n",
    "].transform(\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EPA defines the \"combined fuel economy\" of a vehicle as a weighted average of highway and city fuel economies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ave_mpg'] = 0.55 * df.highway_mpg + 0.45 * df.city_mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining features with common units can be fruitful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mpg_ratio'] = df.highway_mpg / df.city_mpg\n",
    "df['volume'] = df.length * df.width * df.height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up the acronyms in the `fuel_system` feature we see they fall into categories, which we can decompose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fuel_system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a feature can be suprisingly *unimportant*. When that's the case, it's worth investigating why. For categoricals, it could be that one class predominates in the data -- it just doesn't contain much information. It could be worth dropping.\n",
    "\n",
    "It's unlikely that all of these features will end up being important. But that's usually how it is.\n",
    "\n",
    "Get familiar with your data, do some research, and you'll often be able to come up with features that improve your dataset quite a lot.\n",
    "\n",
    "# Keep Going #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
