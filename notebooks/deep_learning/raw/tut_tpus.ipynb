{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "Welcome to **Petals to the Metal**! In this tutorial, you'll learn how to build an image classifier in Keras and train it on a TPU. First, we'll walk through a typical TPU project notebook. Then, you'll create your *own* notebook, and together we'll learn how to commit the notebook with the TPU engaged and make a submission to the competition. At the end, you'll have a complete project you can build off of with ideas of your own.\n",
    "\n",
    "# Load the Helper Functions #\n",
    "\n",
    "Attached to the starter template is a utility script called [`petal_helper`](https://www.kaggle.com/ryanholbrook/petal-helper). It contains a number of helper functions related to data loading and visualization. The following cell will import them into your notebook session. We'll also import TensorFlow, which we'll use to create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from petal_helper import *\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution Strategy #\n",
    "\n",
    "A TPU has eight different *cores* and each of these cores acts as its own accelerator. (A TPU is sort of like having eight GPUs in one machine.) We tell TensorFlow how to make use of all these cores at once through a **distribution strategy**. Run the following cell to create the distribution strategy that we'll later apply to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Detect TPU, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() \n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow will distribute training among the eight TPU cores by creating eight different *replicas* of your model. Since we haven't connected the TPU yet, it tells us we only have one model replica (for CPU or GPU training). Later, you'll turn on the TPU while committing your notebook. Then you'll see all eight replicas.\n",
    "\n",
    "# Load the Competition Data #\n",
    "\n",
    "The next three functions are preloaded with the `petals_helper` script, but we'll show them here so you can see how data pipelines are created in TensorFlow. As you learn more about data pipelines, you may want to experiment with modifying these functions. (It's not so important to understand the details for now though.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset():\n",
    "    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n",
    "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(ordered=False):\n",
    "    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(ordered=False):\n",
    "    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your starter template, the following cell will create your training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = get_training_dataset()\n",
    "ds_valid = get_validation_dataset()\n",
    "\n",
    "print(\"Training:\", ds_train)\n",
    "print(\"Validation:\", ds_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are `tf.data.Dataset` objects. You can think about a dataset in TensorFlow as a *stream* of data records. We won't get into details here, but check out [this guide](https://www.tensorflow.org/guide/data) for more on working with the `tf.data` API.\n",
    "\n",
    "# Explore the Data #\n",
    "\n",
    "Imported with `petal_helper` is a list of the names of the flower species we'll classify our images into. The following cell has some ideas of things you could try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Number of classes: {}\".format(len(CLASSES)))\n",
    "\n",
    "print (\"First five classes, sorted alphabetically:\")\n",
    "for name in sorted(CLASSES)[:5]:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, model training in TensorFlow is done in *batches*, just meaning that training examples are streamed into the model in groups, instead of one at a time. The helper functions will automatically batch the dataset, with the batch size determined by default in `petals_helper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display a single batch of images from a dataset with another of our helper functions. The next cell will turn the dataset into an iterator of batches of 20 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ds_iter = iter(ds_train.unbatch().batch(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just use the Python `next` function to pop out the next batch in the stream and display it with the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch = next(ds_iter)\n",
    "display_batch_of_images(one_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining `ds_iter` and `one_batch` in separate cells, you can just rerun the cell above to see a new batch of images.\n",
    "\n",
    "# Define Model #\n",
    "\n",
    "Now you're ready to create a neural network for classifying images! The method we'll use is called *transfer learning*. If you're unfamiliar with this idea, it just means that we'll reuse part of model that's already been trained on a dataset similar to our training data (in this case, [ImageNet](http://image-net.org/)).\n",
    "\n",
    "Recall that we created a **distribution strategy** earlier. This distribution strategy contains a [context manager](https://docs.python.org/3/reference/compound_stmts.html#with), `strategy.scope()`, that tells TensorFlow how to distribute data to the TPU cores. It's important that you define your model inside this context when training on a TPU.\n",
    "\n",
    "The code cell below shows you how to create an image classifier in TensorFlow that will run on a TPU. We've chosen to use a pretrained model called **VGG16**. Start with this one for now, but later you might want to experiment with some of [the other models](https://www.tensorflow.org/api_docs/python/tf/keras/applications) included with Keras. ([Xception](https://www.tensorflow.org/api_docs/python/tf/keras/applications/Xception) wouldn't be a bad choice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    pretrained_model = tf.keras.applications.VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False ,\n",
    "        input_shape=[*IMAGE_SIZE, 3]\n",
    "    )\n",
    "    pretrained_model.trainable = False\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # To a base pretrained on ImageNet to extract features from images...\n",
    "        pretrained_model,\n",
    "        # ... attach a new head to act as a classifier.\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'],\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set up your model definition correctly, after running this cell you should see a summary of the model you created.\n",
    "\n",
    "# Training #\n",
    "\n",
    "Now we're ready to set up the model training. This just consists of defining some configuration parameters and then calling the `fit` method of our model. The `history` object contains the values at each epoch for the loss and metrics defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size. This will be 16 with TPU off and 128 (=16*8) with TPU on\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "\n",
    "# Define training epochs\n",
    "EPOCHS = 12\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell shows how the loss and metrics progressed during training. Not the best, but at least it converges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "display_training_curves(\n",
    "    history.history['loss'],\n",
    "    history.history['val_loss'],\n",
    "    'loss',\n",
    "    211,\n",
    ")\n",
    "display_training_curves(\n",
    "    history.history['sparse_categorical_accuracy'],\n",
    "    history.history['val_sparse_categorical_accuracy'],\n",
    "    'accuracy',\n",
    "    212,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation #\n",
    "\n",
    "Before making your final predictions on the test set, it's a good idea to evaluate your model's predictions on the validation set. This can help you diagnose problems in training or suggest ways your model could be improved. We'll look at two common ways of validation: the **confusion matrix** and **visual validation**.\n",
    "\n",
    "## Confusion Matrix ##\n",
    "\n",
    "A confusion matrix shows the actual class of an image tabulated against the class your model predicted. They are one of the best tools you have for evaluating the performance of a classifier.\n",
    "\n",
    "The following cell does some processing on the validation data and then creates the matrix with the `confusion_matrix` function included in SciKit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdataset = get_validation_dataset(ordered=True)\n",
    "images_ds = cmdataset.map(lambda image, label: image)\n",
    "labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n",
    "\n",
    "cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy()\n",
    "cm_probabilities = model.predict(images_ds)\n",
    "cm_predictions = np.argmax(cm_probabilities, axis=-1) # convert sparse probabilities into classes\n",
    "\n",
    "labels = range(len(CLASSES))\n",
    "cmat = confusion_matrix(\n",
    "    cm_correct_labels,\n",
    "    cm_predictions,\n",
    "    labels=labels,\n",
    ")\n",
    "cmat = (cmat.T / cmat.sum(axis=1)).T # normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be familiar with metrics like [F1-score](https://en.wikipedia.org/wiki/F1_score) or [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall). This cell will compute these metrics and display them with a plot of the confusion matrix. (These metrics are defined in the Scikit-learn module `sklearn.metrics`; we've imported them in the helper script for you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = f1_score(\n",
    "    cm_correct_labels,\n",
    "    cm_predictions,\n",
    "    labels=labels,\n",
    "    average='macro',\n",
    ")\n",
    "precision = precision_score(\n",
    "    cm_correct_labels,\n",
    "    cm_predictions,\n",
    "    labels=labels,\n",
    "    average='macro',\n",
    ")\n",
    "recall = recall_score(\n",
    "    cm_correct_labels,\n",
    "    cm_predictions,\n",
    "    labels=labels,\n",
    "    average='macro',\n",
    ")\n",
    "display_confusion_matrix(cmat, score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Validation ##\n",
    "\n",
    "It can also be helpful to look at some examples from the validation set and see what class your model predicted. Sometimes, this can reveal patterns in the kinds of images your model has trouble with.\n",
    "\n",
    "This cell will set up the validation set to display 20 images at a time -- you can change this to display more or fewer, if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_validation_dataset()\n",
    "dataset = dataset.unbatch().batch(20)\n",
    "batch = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell shows images from the validation set along with their actual and predicted class. You can run the cell again to see another set of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(batch)\n",
    "probabilities = model.predict(images)\n",
    "predictions = np.argmax(probabilities, axis=-1)\n",
    "display_batch_of_images((images, labels), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions #\n",
    "\n",
    "Once you're satisfied with everything, you're ready to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_test_dataset(ordered=True)\n",
    "\n",
    "print('Computing predictions...')\n",
    "test_images_ds = test_ds.map(lambda image, idnum: image)\n",
    "probabilities = model.predict(test_images_ds)\n",
    "predictions = np.argmax(probabilities, axis=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell generates a file `submission.csv`. This file is what you'll submit to get your score on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating submission.csv file...')\n",
    "test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n",
    "test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n",
    "np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n",
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn! #\n",
    "\n",
    "Now that you've seen what a typical TPU project looks like, you're ready to create your own! The remainder of this tutorial consists of three steps, which we'll walk through together:\n",
    "1. Create a copy of the competition notebook\n",
    "2. Commit the notebook with the TPU engaged to produce predictions\n",
    "3. Submit the predictions to the competition\n",
    "\n",
    "## Create Your Notebook ##\n",
    "\n",
    "As the Exercise that accompanies this tutorial, we've provided a starter notebook that will help you get set up. Keeping this tutorial open, navigate back to the [Deep Learning homepage](https://www.kaggle.com/learn/deep-learning) and click on the **Exercise** link for this bonus lesson.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.imgur.com/ytaknId.png\" alt=\"Click on the Exercise link.\" width=600>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This starter notebook mostly reproduces the code we've gone over together in this tutorial -- with some extra suggestions for things to try on your own! Take a quick look through it, if you like, and then move on to the next step.\n",
    "\n",
    "## Commit Your Notebook ##\n",
    "\n",
    "**Commiting** your notebook will run a fresh copy of the notebook start to finish, saving a copy of the `submission.csv` file as output. Here's how we commit the notebook so that it runs with the TPU turned on:\n",
    "\n",
    "First, click on the blue button to the upper right that says **Save Version**.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.imgur.com/ebMUMSq.png\" alt=\"The blue Save Version button.\" width=300>\n",
    "</figure>\n",
    "\n",
    "Now choose **Advanced Settings**.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.imgur.com/FJVJC3v.png\" alt=\"Advanced Settings in the Version menu.\" width=600>\n",
    "</figure>\n",
    "\n",
    "Now select **Run with TPU for this session** from the dropdown menu and click the blue **Save** button.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.imgur.com/1cB5ykf.png\" alt=\"The Accelerator dropdown menu.\" width=600>\n",
    "</figure>\n",
    "\n",
    "Now select **Save & Run All (Commit)** and click the blue **Save** button.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.imgur.com/VYN8gho.png\" alt=\"The Save Version menu.\" width=600>\n",
    "</figure>\n",
    "\n",
    "The commit may take a while to finish (about 10-15 min), but there's no harm in doing something else while it's running and coming back later.\n",
    "\n",
    "Once it's finished, this dialog should appear:\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.imgur.com/hjspvlJ.png\" alt=\"The Done dialog.\" width=300>\n",
    "</figure>\n",
    "\n",
    "Clicking on **View** will take you to the notebook viewer. Continue this tutorial from there.\n",
    "\n",
    "## Make a Submission ##\n",
    "\n",
    "Now you're ready to make a submission! Assuming you're in the Notebook Viewer, click on the **Output** heading in the menu to the right of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"https://i.imgur.com/thKwt1q.png\" alt=\"The Output heading.\" width=300>\n",
    "</figure>\n",
    "\n",
    "And finally you'll submit the predictions! Just look for the blue **Submit** button. After clicking it, you should shortly be on the leaderboard!\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.imgur.com/j00mDeI.png\" alt=\"The Save Version menu.\" width=600>\n",
    "</figure>\n",
    "\n",
    "# Conclusion #\n",
    "\n",
    "Now that you've joined **Petals to the Metal**, why not try your hand at improving the model and see if you can climb the ranks! Here are some resources to check out for ideas:\n",
    "\n",
    "- The *original* flower competition: [Flower Classification with TPUs](https://www.kaggle.com/c/flower-classification-with-tpus) has a wealth of information in its notebooks and discussion forum.\n",
    "\n",
    "- *something*\n",
    "\n",
    "- *something*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
