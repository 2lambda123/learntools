{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction #\n\nWelcome to **Petals to the Metal**! This tutorial will guide you through creating an image classifier in Keras and training it on the TPU. By the end, you should have a submission-ready notebook, which you can use to jump-start your own ideas.\n\n# Load the Helper Functions #\n\nAttached to the starter template is a utility script called [`petal_helper`](https://www.kaggle.com/ryanholbrook/petal-helper). It contains a number of helper functions related to data loading and visualization. The following cell will import them into your notebook session. We'll also import TensorFlow, which we'll use to create our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from petal_helper import *\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution Strategy #\n\nA TPU has eight different *cores* and each of these cores acts as its own accelerator. (A TPU is sort of like having eight GPUs in one machine.) We tell TensorFlow how to make use of all these cores at once through a **distribution strategy**. Run the following cell to create the distribution strategy that we'll later apply to our model.","execution_count":null},{"metadata":{"lines_to_next_cell":2,"trusted":true},"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TensorFlow will distribute training among the eight TPU cores by creating eight different *replicas* of your model. Since we haven't connected the TPU yet, it tells us we only have one model replica (for CPU or GPU training). Later, you'll turn on the TPU while committing your notebook. Then you'll see all eight replicas.\n\n# Load the Competition Data #\n\nThe next three functions are preloaded with the `petals_helper` script, but we'll show them here so you can see how data pipelines are created in TensorFlow. As you learn more about data pipelines, you may want to experiment with modifying these functions. (It's not so important to understand the details for now though.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In your starter template, the following cell will create your training and validation datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_train = get_training_dataset()\nds_valid = get_validation_dataset()\n\nprint(\"Training:\", ds_train)\nprint(\"Validation:\", ds_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are `tf.data.Dataset` objects. You can think about a dataset in TensorFlow as a *stream* of data records. We won't get into details here, but check out [this guide](https://www.tensorflow.org/guide/data) for more on working with the `tf.data` API.\n\n# Explore the Data #\n\nImported with `petal_helper` is a list of the names of the flower species we'll classify our images into. The following cell has some ideas of things you could try.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Number of classes: {}\".format(len(CLASSES)))\n\nprint (\"First five classes, sorted alphabetically:\")\nfor name in sorted(CLASSES)[:5]:\n    print(name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Usually, model training in TensorFlow is done in *batches*. Your dataset has already been put into batches, with the batch size determined by default in `petals_helper`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can display a single batch of images from a dataset with one of our helper functions. Try typing the code below into a new cell. Run it to see some examples from the training data.","execution_count":null},{"metadata":{"lines_to_next_cell":2,"trusted":true},"cell_type":"code","source":"# Peek at training data\none_batch = next(iter(ds_train.unbatch().batch(20))\ndisplay_batch_of_images(one_batch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Model #\n\nNow you're ready to create a neural network for classifying images! The method we'll use is called *transfer learning*. If you're unfamiliar with this idea, it just means that we'll reuse part of model that's already been trained on a dataset similar to our training data (in this case, [ImageNet](http://image-net.org/)).\n\nRecall that we created a **distribution strategy** earlier. This distribution strategy contains a [context manager](https://docs.python.org/3/reference/compound_stmts.html#with), `strategy.scope()`, that tells TensorFlow how to distribute data to the TPU cores. It's important that you define your model inside this context when training on a TPU.\n\nThe code cell below shows you how to create an image classifier in TensorFlow that will run on a TPU. We've chosen to use a pretrained model called **VGG16**. Start with this one for now, but later you might want to experiment with some of [the other models](https://www.tensorflow.org/api_docs/python/tf/keras/applications) included with Keras. ([Xception](https://www.tensorflow.org/api_docs/python/tf/keras/applications/Xception) wouldn't be a bad choice.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    pretrained_model = tf.keras.applications.VGG16(\n        weights='imagenet',\n        include_top=False ,\n        input_shape=[*IMAGE_SIZE, 3]\n    )\n    pretrained_model.trainable = False\n    \n    model = tf.keras.Sequential([\n        # To a base pretrained on ImageNet to extract features from images...\n        pretrained_model,\n        # ... attach a new head to act as a classifier.\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n    ])\n    model.compile(\n        optimizer='adam',\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy'],\n    )\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you set up your model definition correctly, after running this cell you should see a summary of the model you created.\n\n# Training #\n\nNow we're ready to set up the model training. This just consists of defining some define some configuration parameters and then calling the `fit` method of our model. The `history` object contains the values at each epoch for the loss and metrics defined in the previous cell.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the batch size. This will be 16 with TPU off and 128 (=16*8) with TPU on\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\n# Define training epochs\nEPOCHS = 12\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=EPOCHS,\n    steps_per_epoch=STEPS_PER_EPOCH,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This next cell shows how the loss and metrics progressed during training. Not the best, but at least it converges!","execution_count":null},{"metadata":{"lines_to_next_cell":2,"trusted":true},"cell_type":"code","source":"display_training_curves(\n    history.history['loss'],\n    history.history['val_loss'],\n    'loss',\n    211,\n)\ndisplay_training_curves(\n    history.history['sparse_categorical_accuracy'],\n    history.history['val_sparse_categorical_accuracy'],\n    'accuracy',\n    212,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation #\n\n## Confusion Matrix ##\n\nBefore making your final predictions on the test set, it's a good idea to evaluate your model's predictions on the validation set. This can help you diagnose problems in training or suggest ways your model could be improved. The following cell will create a set of class predictions for the validation set and then display a confusion matrix showing how your model performed on every class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cmdataset = get_validation_dataset(ordered=True)\nimages_ds = cmdataset.map(lambda image, label: image)\nlabels_ds = cmdataset.map(lambda image, label: label).unbatch()\n\ncm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy()\ncm_probabilities = model.predict(images_ds)\ncm_predictions = np.argmax(cm_probabilities, axis=-1)\n\nlabels = range(len(CLASSES))\ncmat = confusion_matrix(\n    cm_correct_labels,\n    cm_predictions,\n    labels=labels,\n)\ncmat = (cmat.T / cmat.sum(axis=1)).T # normalize","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You might be familiar with metrics like [F1-score](https://en.wikipedia.org/wiki/F1_score) or [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall). This cell will compute these metrics and display them along with the confusion matrix. (These metrics are defined in the Scikit-learn module `sklearn.metrics`; we've imported them in the helper script for you.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score = f1_score(\n    cm_correct_labels,\n    cm_predictions,\n    labels=labels,\n    average='macro',\n)\nprecision = precision_score(\n    cm_correct_labels,\n    cm_predictions,\n    labels=labels,\n    average='macro',\n)\nrecall = recall_score(\n    cm_correct_labels,\n    cm_predictions,\n    labels=labels,\n    average='macro',\n)\ndisplay_confusion_matrix(cmat, score, precision, recall)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visual Validation ##\n\nIt can also be helpful to look at some examples from the validation set and see what class your model predicted for them. Sometimes, this can reveal patterns in the kinds of images your model has trouble with.\n\nThis cell will set up the validation set to display 20 images at a time &ndash you can change this to display more or fewer, if you like.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = get_validation_dataset()\ndataset = dataset.unbatch().batch(20)\nbatch = iter(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This next cell shows images from the validation set along with their actual and predicted class. You can run the cell again to see another set of images.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"images, labels = next(batch)\nprobabilities = model.predict(images)\npredictions = np.argmax(probabilities, axis=-1)\ndisplay_batch_of_images((images, labels), predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions #\n\nOnce you're satisfied with everything, you're ready to make predictions on the test set. This is what you'll submit to get your score on the leaderboard.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ds = get_test_dataset(ordered=True)\n\nprint('Computing predictions...')\ntest_images_ds = test_ds.map(lambda image, idnum: image)\nprobabilities = model.predict(test_images_ds)\npredictions = np.argmax(probabilities, axis=-1)\nprint(predictions)\n\nprint('Generating submission.csv file...')\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n!head submission.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Your Turn! #\n\n## Create Your Notebook ##\n\nNow it's your turn!\n\nWe've provided a starter notebook that will help you get set up. Follow the next link and click the blue \"Copy and Edit\" button in the upper-right corner. Follow along with the remainder of this tutorial while you work in your new notebook.\n\n- [Starter Notebook](https://www.kaggle.com/ryanholbrook/petals-to-the-metal-example-submission)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Commit Your Notebook ##\n\nNow we'll commit the notebook with the TPU engaged. First, click on the blue button to the upper right that says **Save Version**.\n\n<figure>\n<img src=\"https://i.imgur.com/ebMUMSq.png\" alt=\"The blue Save Version button.\" width=300>\n</figure>\n\nNow choose **Advanced Settings**.\n\n<figure>\n<img src=\"https://i.imgur.com/FJVJC3v.png\" alt=\"Advanced Settings in the Version menu.\" width=600>\n</figure>\n\nNow select **Run with TPU for this session** from the dropdown menu and click the blue **Save** button.\n\n<figure>\n<img src=\"https://i.imgur.com/1cB5ykf.png\" alt=\"The Accelerator dropdown menu.\" width=600>\n</figure>\n\nNow select **Save & Run All (Commit)** and click the blue **Save** button.\n\n<figure>\n<img src=\"https://i.imgur.com/VYN8gho.png\" alt=\"The Save Version menu.\" width=600>\n</figure>\n\nCommiting your notebook will run a fresh copy of the notebook and save the output. We've already prepared the code for making predictions in your exercise notebook, so once the commit is finished, you'll be able to submit your model's predictions. The commit may take a while to finish, but there's no harm in doing something else while it's running and coming back later.\n\nOnce it's finished, this dialog should appear:\n\n<figure>\n<img src=\"https://i.imgur.com/hjspvlJ.png\" alt=\"The Done dialog.\" width=300>\n</figure>\n\nClicking on **View** will take you to the notebook viewer. Continue this tutorial from there.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Make a Submission ##\n\nAnd now you're ready to make a submission! Assuming you're still in the notebook viewer... **TODO**\n\n<figure>\n<img src=\"https://i.imgur.com/j00mDeI.png\" alt=\"The Save Version menu.\" width=300>\n</figure>\n\n# Conclusion #\n\n**TODO**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}