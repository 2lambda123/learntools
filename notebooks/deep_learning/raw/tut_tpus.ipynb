{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "Welcome to the [**Petals to the Metal**](https://www.kaggle.com/c/tpu-getting-started) competition! In this competition, youâ€™re challenged to build a machine learning model that identifies the type of flowers in a dataset of images.\n",
    "\n",
    "In this tutorial notebook, you'll learn how to build an image classifier in Keras and train it on a [Tensor Processing Unit (TPU)](https://www.kaggle.com/docs/tpu). Then, in the [following exercise](#$NEXT_NOTEBOOK_URL$), you'll create your *own* notebook and make a submission to the competition. At the end, you'll have a complete project you can build off of with ideas of your own.\n",
    "\n",
    "# Load the Helper Functions #\n",
    "\n",
    "Attached to the notebook is a utility script called [`petal_helper`](https://www.kaggle.com/ryanholbrook/petal-helper). It contains a number of helper functions related to data loading and visualization. The following cell will import them into your notebook session. We'll also import TensorFlow, which we'll use to create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from petal_helper import *\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution Strategy #\n",
    "\n",
    "A TPU has eight different *cores* and each of these cores acts as its own accelerator. (A TPU is sort of like having eight GPUs in one machine.) We tell TensorFlow how to make use of all these cores at once through a **distribution strategy**. Run the following cell to create the distribution strategy that we'll later apply to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Detect TPU, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() \n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow will distribute training among the eight TPU cores by creating eight different *replicas* of your model. \n",
    "\n",
    "# Loading the Competition Data #\n",
    "\n",
    "The next three functions are preloaded with the `petals_helper` script, but we'll show them here so you can see how data pipelines are created in TensorFlow. As you learn more about data pipelines, you may want to experiment with modifying these functions. (It's not so important to understand the details for now though.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset():\n",
    "    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n",
    "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(ordered=False):\n",
    "    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(ordered=False):\n",
    "    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use these functions to create all of our dataset splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = get_training_dataset()\n",
    "ds_valid = get_validation_dataset()\n",
    "ds_test = get_test_dataset()\n",
    "\n",
    "print(\"Training:\", ds_train)\n",
    "print (\"Validation:\", ds_valid)\n",
    "print(\"Test:\", ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are `tf.data.Dataset` objects. You can think about a dataset in TensorFlow as a *stream* of data records. Check out [this guide](https://www.tensorflow.org/guide/data) for more on working with the `tf.data` API.\n",
    "\n",
    "# Explore the Data #\n",
    "\n",
    "Imported with `petal_helper` are some constants containing information about the dataset: `CLASSES`, `NUM_TRAINING_IMAGES`, `NUM_VALIDATION_IMAGES`, and `NUM_TEST_IMAGES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of classes: {}\".format(len(CLASSES)))\n",
    "\n",
    "print(\"First five classes, sorted alphabetically:\")\n",
    "for name in sorted(CLASSES)[:5]:\n",
    "    print(name)\n",
    "\n",
    "print (\"Number of training images: {}\".format(NUM_TRAINING_IMAGES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation sets are streams of `(image, label)` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shapes:\")\n",
    "for image, label in ds_train.take(3):\n",
    "    print(image.numpy().shape, label.numpy().shape)\n",
    "print(\"Training data label examples:\", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set is a stream of `(image, idnum)` pairs; `idnum` here is the unique identifier given to the image that we'll use later when we make our submission as a `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test data shapes:\")\n",
    "for image, idnum in ds_test.take(3):\n",
    "    print(image.numpy().shape, idnum.numpy().shape)\n",
    "print(\"Test data IDs:\", idnum.numpy().astype('U')) # U=unicode string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, model training in TensorFlow is done in *batches*, just meaning that training examples are streamed into the model in groups, instead of one at a time. The helper functions will automatically batch the dataset, with the batch size determined by default in `petal_helper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display a single batch of images from a dataset with another of our helper functions. The next cell will turn the dataset into an iterator of batches of 20 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ds_iter = iter(ds_train.unbatch().batch(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just use the Python `next` function to pop out the next batch in the stream and display it with the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch = next(ds_iter)\n",
    "display_batch_of_images(one_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining `ds_iter` and `one_batch` in separate cells, you only need to rerun the cell above to see a new batch of images.\n",
    "\n",
    "# Define Model #\n",
    "\n",
    "Now we're ready to create a neural network for classifying images! We'll use what's known as **transfer learning**. With transfer learning, you reuse part of a pretrained model to get a head-start on a new dataset.\n",
    "\n",
    "For this tutorial, we'll to use a model called **VGG16** pretrained on [ImageNet](http://image-net.org/)). Later, you might want to experiment with [other models](https://www.tensorflow.org/api_docs/python/tf/keras/applications) included with Keras. ([Xception](https://www.tensorflow.org/api_docs/python/tf/keras/applications/Xception) wouldn't be a bad choice.)\n",
    "\n",
    "The distribution strategy we created earlier contains a [context manager](https://docs.python.org/3/reference/compound_stmts.html#with), `strategy.scope`. This context manager tells TensorFlow how to divide the work of training among the eight TPU cores. When using TensorFlow with a TPU, it's important to define your model in a `strategy.scope()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    pretrained_model = tf.keras.applications.VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False ,\n",
    "        input_shape=[*IMAGE_SIZE, 3]\n",
    "    )\n",
    "    pretrained_model.trainable = False\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # To a base pretrained on ImageNet to extract features from images...\n",
    "        pretrained_model,\n",
    "        # ... attach a new head to act as a classifier.\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'],\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training #\n",
    "\n",
    "And now we're ready to train the model! After defining a few parameters, we're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size. This will be 16 with TPU off and 128 (=16*8) with TPU on\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "\n",
    "# Define training epochs\n",
    "EPOCHS = 12\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell shows how the loss and metrics progressed during training. Thankfully, it converges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "display_training_curves(\n",
    "    history.history['loss'],\n",
    "    history.history['val_loss'],\n",
    "    'loss',\n",
    "    211,\n",
    ")\n",
    "display_training_curves(\n",
    "    history.history['sparse_categorical_accuracy'],\n",
    "    history.history['val_sparse_categorical_accuracy'],\n",
    "    'accuracy',\n",
    "    212,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation #\n",
    "\n",
    "Before making your final predictions on the test set, it's a good idea to evaluate your model's predictions on the validation set. This can help you diagnose problems in training or suggest ways your model could be improved. We'll look at two common ways of validation: plotting the **confusion matrix** and **visual validation**.\n",
    "\n",
    "## Confusion Matrix ##\n",
    "\n",
    "A [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) shows the actual class of an image tabulated against its predicted class. It is one of the best tools you have for evaluating the performance of a classifier.\n",
    "\n",
    "The following cell does some processing on the validation data and then creates the matrix with the `confusion_matrix` function included in [`scikit-learn`](https://scikit-learn.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdataset = get_validation_dataset(ordered=True)\n",
    "images_ds = cmdataset.map(lambda image, label: image)\n",
    "labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n",
    "\n",
    "cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy()\n",
    "cm_probabilities = model.predict(images_ds)\n",
    "cm_predictions = np.argmax(cm_probabilities, axis=-1)\n",
    "\n",
    "labels = range(len(CLASSES))\n",
    "cmat = confusion_matrix(\n",
    "    cm_correct_labels,\n",
    "    cm_predictions,\n",
    "    labels=labels,\n",
    ")\n",
    "cmat = (cmat.T / cmat.sum(axis=1)).T # normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be familiar with metrics like [F1-score](https://en.wikipedia.org/wiki/F1_score) or [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall). This cell will compute these metrics and display them with a plot of the confusion matrix. (These metrics are defined in the Scikit-learn module `sklearn.metrics`; we've imported them in the helper script for you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = f1_score(\n",
    "    cm_correct_labels,\n",
    "    cm_predictions,\n",
    "    labels=labels,\n",
    "    average='macro',\n",
    ")\n",
    "precision = precision_score(\n",
    "    cm_correct_labels,\n",
    "    cm_predictions,\n",
    "    labels=labels,\n",
    "    average='macro',\n",
    ")\n",
    "recall = recall_score(\n",
    "    cm_correct_labels,\n",
    "    cm_predictions,\n",
    "    labels=labels,\n",
    "    average='macro',\n",
    ")\n",
    "display_confusion_matrix(cmat, score, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Validation ##\n",
    "\n",
    "It can also be helpful to look at some examples from the validation set and see what class your model predicted. This can help reveal patterns in the kinds of images your model has trouble with.\n",
    "\n",
    "This cell will set up the validation set to display 20 images at a time -- you can change this to display more or fewer, if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_validation_dataset()\n",
    "dataset = dataset.unbatch().batch(20)\n",
    "batch = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a set of flowers with their predicted species. Run the cell again to see another set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(batch)\n",
    "probabilities = model.predict(images)\n",
    "predictions = np.argmax(probabilities, axis=-1)\n",
    "display_batch_of_images((images, labels), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions #\n",
    "\n",
    "Once you're satisfied with everything, you're ready to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_test_dataset(ordered=True)\n",
    "\n",
    "print('Computing predictions...')\n",
    "test_images_ds = test_ds.map(lambda image, idnum: image)\n",
    "probabilities = model.predict(test_images_ds)\n",
    "predictions = np.argmax(probabilities, axis=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate a file `submission.csv`. This file is what you'll submit to get your score on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating submission.csv file...')\n",
    "\n",
    "# Get image ids from test set and convert to unicode\n",
    "test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n",
    "test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n",
    "\n",
    "# Write the submission file\n",
    "np.savetxt(\n",
    "    'submission.csv',\n",
    "    np.rec.fromarrays([test_ids, predictions]),\n",
    "    fmt=['%s', '%d'],\n",
    "    delimiter=',',\n",
    "    header='id,label',\n",
    "    comments='',\n",
    ")\n",
    "\n",
    "# Look at the first few predictions\n",
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn! #\n",
    "\n",
    "Now that you've seen what a typical TPU project looks like, you're ready to **[create your own](#$NEXT_NOTEBOOK_URL$)**! "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
