{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming with TensorFlow\n",
    "\n",
    "As a first step with TensorFlow, we’ll use a pre-trained deep learning model to classify what’s in a photo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Images to Work With\n",
    "\n",
    "Pre-trained models are saved on Kaggle. You can attach them to your kernel workspace the same way you would attach a dataset. We’ve done that in this example kernel and in your workspace.\n",
    "\n",
    "I’ve also attached a dataset with dog pictures, and we will try to tell each dog’s breed from its picture. That data is in the `../input/dog-breed-identification/train/` directory. I picked a few image files to test our model with, and I put the file paths in a list.  Then I use the join function from Python’s `os.path` to append the filename to the directory.  The end result is a list of paths to image files. To keep things simple, we’ll make predictions with just those few images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "image_dir = '../input/dog-breed-identification/train/'\n",
    "img_paths = [join(image_dir, filename) for filename in \n",
    "                           ['0c8fe33bd89646b678f6b2891df8a1c6.jpg',\n",
    "                            '0c3b282ecbed1ca9eb17de4cb1b6e326.jpg',\n",
    "                            '04fb4d719e9fe2b6ffe32d9ae7be8a22.jpg',\n",
    "                            '0e79be614f12deb4f7cae18614b7391b.jpg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Read and Prep Images for Modeling\n",
    "\n",
    "We need to do a little bit of processing to go from the image file paths to something we can run through our model. We’ll put all those preprocessing steps into a single function called `read_and_prep_images`. Some of these preprocessing functions from TensorFlow will be new to you, but the workflow should start to feel more familiar once we get to how models are used.\n",
    "\n",
    "There are a few ways to store this data. Later on, we’ll learn about TensorFlow’s powerful DataSets API. For now, we’ll store the data in Numpy arrays.  It’s easy to switch between the two, so we won’t make much of the distinction right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "def read_and_prep_images(img_paths, img_height=image_size, img_width=image_size):\n",
    "    imgs = [load_img(img_path, target_size=(img_height, img_width)) for img_path in img_paths]\n",
    "    img_array = np.array([img_to_array(img) for img in imgs])\n",
    "    output = preprocess_input(img_array)\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the images using the `load_image` function. We have a few images, so we keep them in a list for now, using a list comprehension. The target_size argument specifies the size or pixel resolution to we want the images to be when we model them. The model we’ll use was trained with 224 by 224 resolution images. So, we’ll make them have the same resolution here.\n",
    "\n",
    "Then we convert each image into an array using the img_to_array function. Remember from the previous video that we can store images in 3-dimensional tensors. The `img_to_array` function creates that 3D tensor for each image.  Combining multiple images causes us to stack those in a new dimension, so we end up with a four dimensional tensor or array.\n",
    "\n",
    "Finally, we use a function called `preprocess_input`. This function does some arithmetic on the pixel values, specifically dividing the values in the input, so they are all between -1 and 1.  This was done when the model was first built, so we have to do it again here to be consistent. This had some function calls you haven’t seen before. These calls will start feeling more familiar as you do the exercises and get used to using the commands you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model with Pre-Trained Weights File. Make Predictions\n",
    "\n",
    "The next few lines are easier.  We specify the model much as we would in scikit-learn or other modeling frameworks.  We’ll use a type of model called the ResNet50 model. I give it an argument specifying the filepath to where we have stored the values in the convolutional filters.\n",
    "\n",
    "We’ll call the function we wrote to read and preprocess our data. Then we get predictions by calling the predict method of our model.  This line works the same way prediction works in libraries like scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "my_model = ResNet50(weights='../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "test_data = read_and_prep_images(img_paths)\n",
    "preds = my_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions\n",
    "\n",
    "Ok. We have predictions about what’s in each image.  We had 4 photographs, and our model gave 1000 probabilities for each photo. What’s the chance that the image has a tiger shark? What’s the chance that it has a pomeranian? What’s the chance that it has an toothbrush? And so on.  \n",
    "\n",
    "It’s convenient to focus on the probabilities for what the model thinks is in the image, rather than all the things it says aren’t in the image. We've provided a function called `decode_predictions` to extract the highest probabilities for each image. We call that function with predictions results, and tell it give us the top 3 probabilities for each photo.\n",
    "\n",
    "You’ll want to see the images too, to see if the model is making sense, so we've included code to display images. If you know much about dog breeds, you’ll recognize that these are pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learntools.deep_learning.decode_predictions import decode_predictions\n",
    "from IPython.display import Image, display\n",
    "\n",
    "most_likely_labels = decode_predictions(preds, top=3, class_list_path='../input/resnet50/imagenet_class_index.json')\n",
    "\n",
    "for i, img_path in enumerate(img_paths):\n",
    "    display(Image(img_path))\n",
    "    print(most_likely_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Ok.  You’ve seen the code to use this type of model.  We have an exercise for you to do it yourself.\n",
    "\n",
    "This may still feel like magic right now, but it will come together as you play with a couple examples and then as we dive into some of the details. So, it’s your turn to use these powerful models.  Then you’ll be ready for transfer learning, so you can quickly apply these models to new applications.\n",
    "\n",
    "Now you are ready to **[use a powerful TensorFlow model](#$EXERCISE_FORKING_URL$)** yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
