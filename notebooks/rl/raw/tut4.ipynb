{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "So far, our ConnectX agents have relied on detailed information about how to play the game.  The heuristic really provides a lot of guidance about how to select moves!\n",
    "\n",
    "In this tutorial, you'll learn how to use **reinforcement learning** to build an intelligent agent without the use of a heuristic.  Instead, we will gradually refine the agent's strategy over time, simply by playing the game and trying to maximize the winning rate.\n",
    "\n",
    "In this notebook, we won't be able to explore this complex field in detail, but you'll learn about the big picture and explore code that you can use to train your own agent.\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "It's difficult to come up with a perfect heuristic.  Improving the heuristic generally entails playing the game many times, to determine specific cases where the agent could have made better choices.  And, it can prove challenging to interpret what exactly is going wrong, and ultimately to fix old mistakes without accidentally introducing new ones.\n",
    "\n",
    "Wouldn't it be much easier if we had a more systematic way of improving the agent with gameplay experience?  \n",
    "\n",
    "In this tutorial, towards this goal, we'll replace the heuristic with a neural network.\n",
    "\n",
    "The network accepts the current board as input.  And, it outputs a probability for each possible move.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://i.imgur.com/KgAliYQ.png\" width=90%><br/>\n",
    "</center>\n",
    "\n",
    "Then, the agent selects a move by sampling from these probabilities.  For instance, for the game board in the image above, the agent selects column 4 with 50% probability.\n",
    "\n",
    "This way, to encode a good gameplay strategy, we need only amend the weights of the network so that _for every possible game board_, it assigns higher probabilities to better moves.\n",
    "\n",
    "At least in theory, that's our goal.  In practice, we won't actually check if that's the case -- since remember that Connect Four has over 4 trillion possible game boards!\n",
    "\n",
    "# Setup\n",
    "\n",
    "How can we approach the task of amending the weights of the network, in practice?  Here's the approach we'll take in this lesson:\n",
    "- After each move, we give the agent a **reward** that tells it how well it did:\n",
    "  - **_If_** the agent wins the game in that move, we give it a reward of `+1`.\n",
    "  - **_Else if_** the agent plays an invalid move (which ends the game), we give it a reward of `-10`.\n",
    "  - **_Else if_** the opponent wins the game in its next move (i.e., the agent failed to prevent its opponent from winning), we give the agent a reward of `-1`.\n",
    "  - **_Else_**, the agent gets a reward of `1/42`.\n",
    "  \n",
    "  \n",
    "- At the end of each game, the agent adds up its reward.  We refer to the sum of rewards as the agent's **cumulative reward**.  \n",
    "  - For instance, if the game lasted 8 moves (each player played four times), and the agent ultimately won, then its cumulative reward is `3*(1/42) + 1`.\n",
    "  - If the game lasted 11 moves (and the opponent went first, so the agent played five times), and the opponent won in its final move, then the agent's cumulative reward is `4*(1/42) - 1`.\n",
    "  - If the game ends in a draw, then the agent played exactly 21 moves, and it gets a reward of `21*(1/42) = 0.5`.\n",
    "  \n",
    "  \n",
    "Our goal is to find the weights of the neural network that (on average) maximize the agent's cumulative reward.  \n",
    "\n",
    "By setting up the problem in this way, we can "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "\n",
    "    \n",
    "tthis is one way we can ttry to offer a solution tto connectx with rl\n",
    "in general, reinforcement learning can be applied to many different problem areas; this same idea of giving an agent a reward after each action it selects yields broad framework that can be applied to robotics, inventory . management, smong otthers\n",
    "in general, plagued by instability, oftten doesn't perform well\n",
    "    \n",
    "Initially, the weights are set to random values.\n",
    "- As the agent plays the game, the algorithm will continually try out new values for the weights, to see how the cumulative reward is affected, on average.  Over time, after playing many games, we get a better idea of how the weights affect cumulative reward, the algorithm settles towards weights that performed better.  \n",
    "    - _Of course, we have glossed over the details here, and there's a lot of complexity involved in this process.  For now, we focus on the big picture!_\n",
    "    \n",
    "    \n",
    "- This way, we'll end up with an agent that tries to win the game (so it gets the final reward of `+1`) and tries to make the game last as long as possible (so that it collects the `1/42` bonus as many times as it can).\n",
    "    - _You might argue that it doesn't really make sense to want the game to last as long as possible -- this might result in a very inefficient agent that doesn't play obvious winning moves early in gameplay.  And, your intuition would be correct -- this will make the agent less efficient!  The reason we include the `1/42` bonus is to help the algorithms we'll use to converge better.  Further discussion is outside of the scope of this course, but you can learn more by reading about the \"temporal credit assignment problem\" and \"reward shaping\"._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "There are a lot of great implementations of reinforcement learning algorithms online.  In this course, we'll use [Stable Baselines](https://github.com/hill-a/stable-baselines).\n",
    "\n",
    "Currently, Stable Baselines is not yet compatible with TensorFlow 2.0.  So, we begin by downgrading to TensorFlow 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle-environments>=0.1.6 in /anaconda3/lib/python3.6/site-packages (0.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /anaconda3/lib/python3.6/site-packages (from kaggle-environments>=0.1.6) (3.2.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /anaconda3/lib/python3.6/site-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (1.13.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /anaconda3/lib/python3.6/site-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (0.15.7)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /anaconda3/lib/python3.6/site-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (18.2.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /anaconda3/lib/python3.6/site-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (0.6)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (44.0.0)\n"
     ]
    }
   ],
   "source": [
    "#$HIDE$\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "!pip install 'kaggle-environments>=0.1.6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==1.15.0 in /anaconda3/lib/python3.6/site-packages (1.15.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (1.17.4)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (0.7.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (3.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (0.32.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (0.1.8)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (1.15.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/alexiscook/.local/lib/python3.6/site-packages (from tensorflow==1.15.0) (1.11.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (1.17.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (1.13.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (0.2.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (3.11.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.15.0) (0.9.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda3/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (0.14.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /anaconda3/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (44.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.0.1)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "#$HIDE$\n",
    "!pip install 'tensorflow==1.15.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check version of tensorflow\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a bit of extra work that we need to do to make the environment compatible with Stable Baselines.  For this, we define the `ConnectFourGym` class below.  This class implements ConnectX as an [OpenAI Gym environment](http://gym.openai.com/docs/) and uses several methods:\n",
    "- `reset()` is called at the beginning of every game.  It returns the starting game board as a 2D numpy array with 6 rows and 7 columns.\n",
    "- `change_reward()` customizes the rewards that the agent receives.  (_The competition already has its own system for rewards that are used to rank the agents, and this method changes the values to match the rewards system we designed._) \n",
    "- `step()` is used to play the agent's choice of action (supplied as `action`), along with the opponent's response.  It returns:\n",
    "  - the resulting game board (as a numpy array), \n",
    "  - the agent's reward (from the most recent move only: one of `+1`, `-10`, `-1`, or `1/42`), and\n",
    "  - whether or not the game has ended (if the game has ended, `done=True`; otherwise, `done=False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make, evaluate\n",
    "from gym import spaces\n",
    "\n",
    "class ConnectFourGym:\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(self.rows,self.columns,1), dtype=np.int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 1\n",
    "        elif done: # The opponent won the game\n",
    "            return -1\n",
    "        else: # Reward 1/42\n",
    "            return 1/(self.rows*self.columns)\n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n",
    "\n",
    "# Create ConnectFour environment\n",
    "env = ConnectFourGym(agent2=\"negamax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Baselines requires us to work with [\"vectorized\" environments](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html).  For this, we can use the `DummyVecEnv` class.  \n",
    "\n",
    "The `Monitor` class lets us watch how the agent's performance gradually improves, as it plays more and more games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from stable_baselines.bench import Monitor \n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Create directory for logging training information\n",
    "#log_dir = \"/kaggle/working/log/\"\n",
    "log_dir = \"~/Desktop/kagglee\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Logging progress\n",
    "monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "# Create a vectorized environment\n",
    "vec_env = DummyVecEnv([lambda: monitor_env])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to specify the architecture of the neural network.  As defined in the code cell below, the 2D game board will be passed to a convolutional neural network with ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:57: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:66: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/common/policies.py:115: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/a2c/utils.py:136: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:151: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:312: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:312: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:161: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:107: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:108: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py:189: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from stable_baselines import PPO1 \n",
    "from stable_baselines.a2c.utils import conv, linear, conv_to_fc\n",
    "from stable_baselines.common.policies import CnnPolicy\n",
    "\n",
    "# Neural network for predicting action values\n",
    "def modified_cnn(scaled_images, **kwargs):\n",
    "    activ = tf.nn.relu\n",
    "    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n",
    "                         init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n",
    "                         init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = conv_to_fc(layer_2)\n",
    "    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n",
    "\n",
    "class CustomCnnPolicy(CnnPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)\n",
    "        \n",
    "# Initialize agent\n",
    "model = PPO1(CustomCnnPolicy, vec_env, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note there are a lot of parameters involved in this algorithm. default performs fine\n",
    "\n",
    "The next section uses template code (which we [took from the Stable Baselines documentation](https://stable-baselines.readthedocs.io/en/master/guide/examples.html#using-callback-monitoring-training)) to train the agent and modify its performance.  The code prints ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo1.pposgd_simple.PPO1 at 0x7fac58d25d30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "\n",
    "# How often to check for model improvement\n",
    "check_every = 2000\n",
    "\n",
    "# Initialize training information\n",
    "best_mean_reward, n_steps = -np.inf, 0\n",
    "\n",
    "# Track training progress and save best model\n",
    "def callback(_locals, _globals):\n",
    "    global n_steps, best_mean_reward\n",
    "    if (n_steps + 1) % check_every == 0:\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if len(x) > 0:\n",
    "            mean_reward = np.mean(y[-check_every:])\n",
    "            print(x[-1], 'timesteps')\n",
    "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                print(\"Saving new best model\")\n",
    "                _locals['self'].save(log_dir + 'best_model.pkl')\n",
    "    n_steps += 1\n",
    "    return True\n",
    "\n",
    "# Train agent\n",
    "model.learn(total_timesteps=50000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fac58d25438>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n",
    "    firstline = fh.readline()\n",
    "    assert firstline[0] == '#'\n",
    "    df = pd.read_csv(fh, index_col=None)['r']\n",
    "df.replace(-1, 0).rolling(window=1000).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we specify the trained agent in the format required for the competition.\n",
    "    \n",
    "note that agent not training anymore at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomCnnPolicy' object has no attribute 'pdtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c025cb7135ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'best_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Use the best model to select a column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/stable_baselines/common/base_class.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, load_path, env, custom_objects, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/stable_baselines/ppo1/pposgd_simple.py\u001b[0m in \u001b[0;36msetup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                     \u001b[0mobs_ph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_pi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_ph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                     \u001b[0maction_ph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_pi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mkloldnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_pi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_pi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomCnnPolicy' object has no attribute 'pdtype'"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "best_model = PPO1.load(log_dir + 'best_model.pkl')\n",
    "    \n",
    "def agent1(obs, config):\n",
    "    # Use the best model to select a column\n",
    "    col, _ = best_model.predict(np.array(obs['board']).reshape(6,7,1))\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell, we see the outcome of one game round against a random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the game environment\n",
    "env = make(\"connectx\")\n",
    "\n",
    "# Two random agents play one game round\n",
    "env.run([agent1, \"random\"])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we calculate how it performs on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "# To learn more about the evaluate() function, check out the documentation here: (insert link here)\n",
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,0])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([0,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0.5]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0.5, None]))\n",
    "    print(\"Number of Draws (in {} game rounds):\".format(n_rounds), outcomes.count([0.5, 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(agent1=agent1, agent2=\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn\n",
    "\n",
    "tbd ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
