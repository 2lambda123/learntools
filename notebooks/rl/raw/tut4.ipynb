{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "So far, our agents have relied on detailed information about how to play the game.  In particular, the heuristic provides a lot of guidance about how to select moves.  \n",
    "\n",
    "In this tutorial, you'll learn how to use **reinforcement learning** to train an intelligent agent without the use of a heuristic.  Instead, your agent will gradually develop its own strategy over time, simply by playing the game and trying to maximize its winning rate.\n",
    "\n",
    "# (Deep) Reinforcement Learning\n",
    "\n",
    "DQN as jumping ground\n",
    "\n",
    "maps state to value of each action\n",
    "\n",
    "since \n",
    "\n",
    "# Code\n",
    "\n",
    "There are a lot of great implementations of reinforcement learning algorithms online.  In this course, we'll use [Stable Baselines](https://github.com/hill-a/stable-baselines).\n",
    "\n",
    "There's a bit of extra work that we need to do to ensure that the environment is compatible with Stable Baselines.  For this, we define the class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle_simulations as ks\n",
    "from gym import spaces\n",
    "\n",
    "class ConnectFourGym:\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = ks.make(\"connectx\")\n",
    "        ks_env._Environment__get_space = self.__get_space\n",
    "        self.env = ks_env.gym([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=(self.rows,self.columns,1), dtype=np.int)\n",
    "        self.metadata = None\n",
    "        self.reward_range = (-1, 1)\n",
    "        self.spec = None\n",
    "    def reset(self):\n",
    "        obs, reward, done, info = self.env.reset()\n",
    "        return np.array(obs['board']).reshape(self.rows,self.columns,1)\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(int(action))\n",
    "        return np.array(obs['board']).reshape(self.rows,self.columns,1), reward, done, info\n",
    "    def __get_space(self, spec):\n",
    "        return \n",
    "\n",
    "# Create ConnectFour environment\n",
    "env = ConnectFourGym()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable baselines requires us to work with \"vectorized\" environments.  For this, we can use the `DummyVecEnv` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines.bench import Monitor \n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Create directory for logging training information\n",
    "log_dir = \"/kaggle/working/log/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# \n",
    "monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "# Create a vectorized environment\n",
    "vec_env = DummyVecEnv([lambda: monitor_env])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to specify the architecture of the neural network that will be used to predict the action values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from stable_baselines import DQN \n",
    "from stable_baselines.a2c.utils import conv, linear, conv_to_fc\n",
    "from stable_baselines.deepq.policies import CnnPolicy\n",
    "\n",
    "# Neural network for predicting action values\n",
    "def modified_cnn(scaled_images, **kwargs):\n",
    "    activ = tf.nn.relu\n",
    "    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = conv_to_fc(layer_2)\n",
    "    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n",
    "\n",
    "class CustomCnnPolicy(CnnPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)\n",
    "        \n",
    "# Initialize agent\n",
    "model = DQN(CustomCnnPolicy, vec_env, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "\n",
    "# How often to check for model improvement\n",
    "check_every = 2000\n",
    "\n",
    "# Initialize training information\n",
    "best_mean_reward, n_steps = -np.inf, 0\n",
    "\n",
    "# Track training progress and save best model\n",
    "def callback(_locals, _globals):\n",
    "    global n_steps, best_mean_reward\n",
    "    if (n_steps + 1) % check_every == 0:\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if len(x) > 0:\n",
    "            mean_reward = np.mean(y[-check_every:])\n",
    "            print(x[-1], 'timesteps')\n",
    "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                print(\"Saving new best model\")\n",
    "                _locals['self'].save(log_dir + 'best_model.pkl')\n",
    "    n_steps += 1\n",
    "    return True\n",
    "\n",
    "# Train agent\n",
    "model.learn(total_timesteps=500000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n",
    "    firstline = fh.readline()\n",
    "    assert firstline[0] == '#'\n",
    "    df = pd.read_csv(fh, index_col=None)['r']\n",
    "df.replace(-1, 0).rolling(window=1000).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent1(obs, config):\n",
    "    # Load the best model\n",
    "    \n",
    "    # Use the best model to select a column\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent plays against a random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the game environment\n",
    "env = ks.make(\"connectx\")\n",
    "\n",
    "# Two random agents play one game round\n",
    "env.run([agent1, \"random\"])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn\n",
    "\n",
    "tbd ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
