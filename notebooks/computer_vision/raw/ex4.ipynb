{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In these exercises, you'll explore the operations a couple of popular convnet architectures use for feature extraction, learn about how convnets can capture large-scale visual features through stacking layers, and finally see how convolution can be used on one-dimensional data, in this case, a time series.\n",
    "\n",
    "Run the cell below to set everything up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.computer_vision.ex4 import *\n",
    "\n",
    "from cv_prelude import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Feature Extraction #\n",
    "\n",
    "In the tutorial, we looked at the operations VGG16 uses to perform the feature extraction. Now let's take a look at the parameters used in a couple of more advanced models: [ResNet50](https://keras.io/api/applications/resnet/#resnet50-function) and [InceptionV3](https://keras.io/api/applications/inceptionv3/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) ResNet50\n",
    "\n",
    "Run this next cell to load the model and an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import visiontools\n",
    "import warnings\n",
    "\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings(\"ignore\") # to clean up output cells\n",
    "\n",
    "IMAGE_PATH = '/kaggle/input/computer-vision-resources/car_illus.jpg'\n",
    "image = visiontools.read_image(IMAGE_PATH, channels=1)\n",
    "image = tf.image.resize(image, size=[224, 224], method='nearest')\n",
    "\n",
    "resnet50 = tf.keras.models.load_model(\n",
    "    '/kaggle/input/cv-course-models/cv-course-models/resnet50',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run this cell to get a random kernel from the first convolutional layer of ResNet50. It should return a `(7,7)` kernel. This convolution it follows with a pooling layer with a `(3, 3)` window and strides of `(2, 2)`. Run it several times if you like to see different examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = visiontools.random_kernel(model=resnet50, layer='conv1_conv')\n",
    "\n",
    "visiontools.show_kernel(kernel, label=False)\n",
    "\n",
    "visiontools.show_extraction(\n",
    "    image, kernel,\n",
    "    conv_stride=2,\n",
    "    conv_padding='same',\n",
    "    activation='relu',\n",
    "    pool_size=3,\n",
    "    pool_stride=2,\n",
    "    pool_padding='same',\n",
    "subplot_shape=(1, 4),\n",
    "figsize=(16, 6));\n",
    "q_1.a.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) InceptionV3\n",
    "\n",
    "The InceptionV3 architecture takes a different approach to reducing parameters. Though many of its layers use the traditional `(3, 3)` kernels, it also introduced the idea of an asymmetric convolution. Instead of a single `(7, 7)` kernel, for instance, it sometimes will use a `(7, 1)` kernel followed by a `(1, 7)` kernel. This allows it to get the same range of connectivity, but with many fewer parameters. Let's take a look at one of these kinds of kernels.\n",
    "\n",
    "First run this cell to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "inceptionv3 = tf.keras.models.load_model(\n",
    "    '/kaggle/input/cv-course-models/cv-course-models/inceptionv3',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run this cell to see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Change the layer to 'conv2d_39' if you'd like to see a \"long\" kernel instead\n",
    "kernel = visiontools.random_kernel(model=inceptionv3, layer='conv2d_38')\n",
    "\n",
    "visiontools.show_kernel(kernel, label=False)\n",
    "\n",
    "visiontools.show_extraction(\n",
    "    image, kernel,\n",
    "    conv_stride=1,\n",
    "    conv_padding='same',\n",
    "    activation='relu',\n",
    "    pool_size=2,\n",
    "    pool_stride=2,\n",
    "    pool_padding='same',\n",
    "subplot_shape=(1, 4),\n",
    "figsize=(16, 6));\n",
    "q_1.b.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, these two examples illustrated the range of possibilities for feature extraction just in the choice of parameters values. The ResNet and Inception architectures infact introduced other innovations as well, if you're interested in how convnets are built, they're worth reading more about!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Receptive Field #\n",
    "\n",
    "The **receptive field** of a neuron is the part of the image input it is able to receive information from, that is, the patch of input pixels it is ultimately connected to. We would like our base however to recognize large-scale features, like wheels or windows for the cars and trucks. Let's see how we can \"grow\" the receptive field through stacking layers, so that the outputs can get information from large patches of the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) How the Receptive Field Grows\n",
    "\n",
    "This next picture illustrates two stacked convolutional layers both with `(3, 3)` kernels. The bottom layer represents the input. Each of the neurons in the first (middle) layer has a $3 \\times 3$ receptive field. Following the path of connections, we can see that each of the neurons in the second (top) layer has a $5 \\times 5$ receptive field.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.imgur.com/HmwQm2S.png\" alt=\"Illustration of the receptive field of two stacked convolutions.\" width=250>\n",
    "</figure>\n",
    "\n",
    "If you added a *third* convolutional layer with a `(3, 3)` kernel, each of its neurons would have a receptive field of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_2.a.hint()\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_2.a.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now say you add a `(2, 2)` maximum pooling layer with `strides=2` after the third convolution. What receptive field do the outputs have now? (This is harder. Try the hint if you need help.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_2.b.hint()\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_2.b.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Dimensional Convolution #\n",
    "\n",
    "Though we've been using convolutional networks on two-dimensional data, it turns out that they can also be useful on *one*-dimensional data, like time series or natural language texts. In fact, convolutional networks tend to be successful on any kind of data with a strong **local topological structure**, meaning that the information about a point tends to be concentrated in nearby points -- you can most successfully predict the value of a pixel by looking at nearby pixels, you can most successfully predict the weather today by looking at the weather yesterday instead of a month ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Apply a 1D Convolution\n",
    "\n",
    "In this exercise, we'll see how a convolution can be used on a **time series**. The time series we'll use is from [Google Trends](https://trends.google.com/trends/); it measures the popularity of the search term \"machine learning\" for weeks from January 25, 2015 to January 15, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the time series as a Pandas dataframe\n",
    "machinelearning = pd.read_csv(\n",
    "    '/kaggle/input/computer-vision-resources/machinelearning.csv',\n",
    "    parse_dates=['Week'],\n",
    "    index_col='Week',\n",
    ")\n",
    "\n",
    "machinelearning.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data is one-dimensional, the kernel needs to be one-dimensional as well. Define a one dimensional kernel. Though not required, you'll get better results if the entries sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Define a 1D kernel. \n",
    "kernel = tf.constant([____])\n",
    "q_3.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "kernel = tf.constant([0.1, 0.2, 0.3, 0.4])\n",
    "q_3.assert_check_passed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the next cell to apply the kernel with a convolution and see what effect it had on the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat for TensorFlow\n",
    "ts_data = machinelearning.to_numpy()\n",
    "ts_data = tf.expand_dims(ts_data, axis=0)\n",
    "ts_data = tf.cast(ts_data, dtype=tf.float32)\n",
    "kern = tf.reshape(kernel, shape=(*kernel.shape, 1, 1))\n",
    "\n",
    "ts_filter = tf.nn.conv1d(\n",
    "    input=ts_data,\n",
    "    filters=kern,\n",
    "    stride=1,\n",
    "    padding='VALID',\n",
    ")\n",
    "\n",
    "# Format as Pandas Series\n",
    "machinelearning_filtered = pd.Series(tf.squeeze(ts_filter).numpy())\n",
    "\n",
    "machinelearning_filtered.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "This lesson ends our discussion of feature extraction. Hopefully, having completed these lessons, you've gained some intuition about how the process works and why the usual choices for its implementation are often the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep Going #\n",
    "\n",
    "In the next lesson, [**Lesson 5**](#$NEXT_NOTEBOOK_URL$), you'll learn how to compose the `Conv2D` and `MaxPool2D` layers to build your own convolutional networks from scratch."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
