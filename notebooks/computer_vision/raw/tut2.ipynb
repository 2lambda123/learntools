{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In the last lesson, you used transfer learning with a pre-trained model. But what exactly did our model learn? Let's open up the \"black box\" and find out!\n",
    "\n",
    "<!-- header illustration -->\n",
    "\n",
    "In this lesson you'll,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convnets #\n",
    "\n",
    "A **convolutional neural network** (also, **CNN** or **convnet**) is a special neural network designed to learn **spatial information** such as images, text, and video. Its characteristic operation, the convolution, makes a convnet very efficient at processing this kind of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Convnets Work ##\n",
    "\n",
    "A convnet used for classifying images will consist of two parts: a **convolutional base** and a **dense head**.\n",
    "\n",
    "<!-- TODO: diagram -->\n",
    "\n",
    "The convolutional base acts as a visual **feature extractor**, while the dense head acts as a classifier. A convnet classifies an image like this: \n",
    "1. the base extracts the visual features\n",
    "2. the head decides the class using those features\n",
    "\n",
    "<!-- TODO: diagram -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and Activations / Filters and Features #\n",
    "\n",
    "You might remember **weights** and **activations** from *Introduction to Deep Learning*. An input to a neural network layer produces activations, and the weights in the layer determine what those activations are.\n",
    "\n",
    "Convnets have a special layers called **convolutional layers** that they use for feature extraction. The weights in a convolutional layer define **filters**, and every filter will extract a particular kind of visual feature.\n",
    "<!-- blue box -->\n",
    "- an input produces activations determined by the weights\n",
    "- an image produces features determined by the filter\n",
    "<!-- end blue box -->\n",
    "\n",
    "Here are the features produced by a filter for extracting vertical lines:\n",
    "<!-- TODO: image -->\n",
    "\n",
    "When training a network, the goal is to learn weights which produce activations with minimal loss. The goal when training a convnet is to learn filters which produce features with minimal classification error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple to Complex / General to Specific #\n",
    "\n",
    "A convnet repeats the feature extraction process many times as information flows through its layers. The first set of features extracted feeds into the next convolutional layer which extracts another, more complex, set of featuers.\n",
    "\n",
    "<!-- TODO: image -->\n",
    "\n",
    "The result is that the filters the network learns that are near the input are the simplest and most general, while the filters near the output are the most complex and specific.\n",
    "\n",
    "You can see this in the following illustration. The filters in the beginning layers extract things like straight lines or curves, while the filters at the end extract complex things like wheels and windshields.\n",
    "\n",
    "<!-- TODO: illustration -->\n",
    "\n",
    "This is what makes transfer learning work! If you were to train a convnet on images of dogs, in the ending layers would be filters specific to dogs, likely things like eyes and snouts and ears. But at the beginning would be filters for things like lines and curves, which are useful for images of all kinds. The more similar the new dataset is to the original, the more layers are likely to be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Visualizations #\n",
    "\n",
    "Let's train a simple convnet on the cars dataset. Then we'll see how we can look at the filters it learned and at the activations they produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(kernel_size=3, filters=16, padding='same', activation='relu', input_shape=[*IMAGE_SIZE, 3]),\n",
    "    tf.keras.layers.Conv2D(kernel_size=3, filters=30, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "    tf.keras.layers.Conv2D(kernel_size=3, filters=60, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "    tf.keras.layers.Conv2D(kernel_size=3, filters=90, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "    tf.keras.layers.Conv2D(kernel_size=3, filters=110, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "    tf.keras.layers.Conv2D(kernel_size=3, filters=130, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv2D(kernel_size=1, filters=40, padding='same', activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss= 'categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been trained, we can get its filters like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we can see the features it extracts from an image like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some filter/feature pairs as go we deeper and deeper down the network.\n",
    "\n",
    "<!-- TODO: filter/feature pairs -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Why a New Layer? # -->\n",
    "\n",
    "<!-- In this lesson, you'll learn about convolutional neural networks, sometimes called **CNNs** **convnets**. These neural networks use **convolutional layers**, which are specially designed to learn images. -->\n",
    "\n",
    "<!-- The kind of network layers you've seen so far have mostly been *dense layers*. Because dense layers are fully-connected, they are very flexible. <\\!-- TODO: advantages of dense layers -\\-> -->\n",
    "\n",
    "<!-- But this flexibility comes at a price. Because dense layers are fully-connected they will have very many parameters that must be trained. The VGG16 model we saw in the last tutorial has <\\!-- X -\\-> convolutional layers and <\\!-- X -\\-> parameters to train. An equivalent number of dense layers would give <\\!-- very many -\\-> parameters! <\\!-- TODO: disadvantages of dense layers -\\-> -->\n",
    "\n",
    "<!-- <\\!-- TODO: diagram of dense vs conv layer (maybe) -\\-> -->\n",
    "\n",
    "<!-- A model whose design reflects the structure of the problem will make the most efficient use of your time and data. What is best is if your model can vary in just the way your data can vary, but no more. -->\n",
    "\n",
    "<!-- The design of a deep learning model is primarily reflected in its layers and in the way those layers are connected. -->\n",
    "\n",
    "<!-- Convolutional layers learn images. -->\n",
    "\n",
    "<!-- Convolutional networks use two operations in alternation: **convolution** and **pooling**. A convolution is a kind of weighted average that in effect divides spatial information into its components. The pooling operation condenses the result of a convolution, keeping the important part. -->\n",
    "\n",
    "<!-- As information flows through a convolutional network,  -->\n",
    "\n",
    "<!-- In fact, convolutional layers are well adapted to learning *any* kind of spatial data. This includes data that changes through time! Convnets that learn images use 2D convolutions. But with a 1D layer, a convnet can also learn text and audio. And a 3D layer can learn video! -->\n",
    "\n",
    "<!-- In this tutorial, we'll explore convnets by looking at the **feature maps** its layers produce. We'll see the kinds of features a layer has learned and also what parts of an image the network thinks are most important to identifying its class. -->\n",
    "\n",
    "<!-- # Convnet Architecture # -->\n",
    "\n",
    "<!-- Here is a diagram of a small convolutional network. -->\n",
    "\n",
    "<!-- <\\!-- TODO: diagram of convnet -\\-> -->\n",
    "<!-- ```python -->\n",
    "<!-- tf.keras.utils.plot_model(model, show_layer_names=False, show_shapes=True) -->\n",
    "<!-- ``` -->\n",
    "\n",
    "<!-- You'll learn more about how a convnet constructs these feature maps in the next lesson. -->\n",
    "\n",
    "<!-- # Looking at Feature Maps # -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Conclusion # -->\n",
    "\n",
    "<!-- There are many ways of visualizing convnets. -->\n",
    "\n",
    "<!-- The ability to \"open up\" a convnet is a great advantage in interpretability. -->\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
