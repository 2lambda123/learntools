{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--TITLE: Convolution and ReLU-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In the last lesson, we saw that a convolutional classifier has two parts: a convolutional **base** and a **head** of dense layers. We learned that the job of the base is to extract visual features from an image, which the head would then use to classify the image.\n",
    "\n",
    "Over the next few lessons, we're going to learn about the two most important types of layers that you'll usually find in a convolutional image classifier. These are: the **convolutional layer** with **ReLU activation**, and the **maximum pooling layer**. In Lesson 5, you'll learn how these layers are usually composed into **blocks** that perform the feature extraction.\n",
    "\n",
    "This lesson is about the convolutional layer with its ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction #\n",
    "\n",
    "The **feature extraction** performed by the base consists of **three basic operations** common to convolutional image classifiers.\n",
    "1. **Filter** an image for a particular feature (convolution)\n",
    "2. **Detect** that feature within the filtered image (ReLU)\n",
    "3. **Condense** to isolate the feature (maximum pooling)\n",
    "\n",
    "Here is what this process looks like. You can see how these three operations are able to isolate some particular characteristic of the original image.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/2-show-extraction.png\" width=\"1000\" alt=\"An example of the feature extraction process.\"> -->\n",
    "<img src=\"https://i.imgur.com/GDKuk6m.png\" width=\"1000\" alt=\"An example of the feature extraction process.\">\n",
    "</figure>\n",
    "\n",
    "Typically, the network will perform several extractions in parallel on a single image. By the time the data reaches the classifier, a network might be producing over 1000 features! It is images like these that the head of dense layers uses to predict a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter with Convolution #\n",
    "\n",
    "A convolutional layer carries out the filtering step. Typically, you would define a convolutional layer within a model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(filters=64, kernel_size=3) # activation is None\n",
    "    # More layers follow\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **weights** a convnet learns during training are primarily contained in its convolutional layers. These weights we call **kernels**. We can represent them as small arrays:\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-kernel.png\" width=\"150\" alt=\"A 3x3 kernel.\"> -->\n",
    "<img src=\"https://i.imgur.com/uJfD9r9.png\" width=\"150\" alt=\"A 3x3 kernel.\">\n",
    "</figure>\n",
    "\n",
    "A convolutional layer will usually contain many kernels -- often hundreds or thousands. They are what determine the kinds of features produced. You can think about a kernel as a kind of polarized lens, letting through only a certain pattern of information. More specifically, a kernel produces a \"weighted sum\" of a pixel and its neighbors. \n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-kernel-lens.png\" width=\"400\" alt=\"A kernel acts as a kind of lens.\"> -->\n",
    "<img src=\"https://i.imgur.com/j3lk26U.png\" width=\"250\" alt=\"A kernel acts as a kind of lens.\">\n",
    "</figure>\n",
    "\n",
    "The **activations** in the network we call **feature maps**. They are what result when we apply a filter to an image; they are the visual features the network extracts. Here are a few kernels pictured with the feature maps they produced when applied to an image.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-kernels-and-maps.png\" width=\"600\" alt=\"The channels of a color image.\"> -->\n",
    "<img src=\"https://i.imgur.com/JxBwchH.png\" width=\"800\" alt=\"An embossing kernel and the feature map it produces.\">\n",
    "</figure>\n",
    "\n",
    "From the pattern of numbers in the kernel, you can tell what kind of feature maps it will produce. Generally, what a convolution accentuates in its inputs will match the shape of the *positive* numbers in the kernel. The left and middle kernels above will both filter for horizontal shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect with ReLU #\n",
    "\n",
    "After filtering, the feature maps pass through the activation function. The **ReLU function** has a graph like this:\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-channels-stack.png\" width=\"300\" alt=\"Channels form the depth dimension.\"> -->\n",
    "<img src=\"https://i.imgur.com/3Ud5xhK.png\" width=\"300\" alt=\"Graph of the ReLU activation function.\">\n",
    "</figure>\n",
    "\n",
    "(*ReLU* stands for *Rectified Linear Unit*.)\n",
    "\n",
    "Typically, you'll include it as the activation function of the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu')\n",
    "    # More layers follow\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could think about the activation function as normalizing the pixel values according to some measure of importance. The ReLU function says that negative values are not important and so sets them to 0. (\"Everything unimportant is equally unimportant.\")\n",
    "\n",
    "Like other activation functions used in neural networks, the ReLU function is *nonlinear*. Essentially this means that the total effect of all the layers in the network is different than what we would get by just adding the effects together -- which would be no different than what you would get with a single layer.\n",
    "\n",
    "The ReLU function ensures that only pixels with positive activation remain in the feature map. This is desireable because we don't want any negative activations destroying the features we detect deeper in the network, which is what would happen if we simply added them together.\n",
    "\n",
    "Here is ReLU applied the feature maps above. Notice how it succeeds at isolating the feature of interest.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-relu-and-maps.png\" width=\"800\" alt=\"ReLU applied to feature maps.\"> -->\n",
    "<img src=\"https://i.imgur.com/dKtwzPY.png\" width=\"800\" alt=\"ReLU applied to feature maps.\">\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Apply Convolution and ReLU #\n",
    "\n",
    "Let's apply these operations to an image to get a feel for what they do.\n",
    "\n",
    "Here is the image we'll use for this example:\n",
    "<!-- #endregion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "\n",
    "image_path = '/kaggle/input/computer-vision-resources/car_feature.jpg'\n",
    "image = tf.io.read_file(image_path)\n",
    "image = tf.io.decode_jpeg(image)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the filtering step, we'll define a kernel and then apply it with the convolution. The kernel in this case is an \"edge detection\" kernel. We'll define it as an array just like in Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import visiontools\n",
    "\n",
    "kernel = tf.constant([\n",
    "    [-1, -1, -1],\n",
    "    [-1,  8, -1],\n",
    "    [-1, -1, -1],\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "visiontools.show_kernel(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow includes many common operations performed by neural networks in its `tf.nn` [module](https://www.tensorflow.org/api_docs/python/tf/nn). This is what we'll use to apply convolution and ReLU for this example, but remember that when you're building models, you'll use layers in Keras as usual.\n",
    "\n",
    "This next hidden cell does some reformatting to make things compatible with TensorFlow. The details aren't important for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE$\n",
    "# Reformat for batch compatibility.\n",
    "image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "image = tf.expand_dims(image, axis=0)\n",
    "kernel = tf.reshape(kernel, [*kernel.shape, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply our kernel and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filter = tf.nn.conv2d(\n",
    "    input=image,\n",
    "    filters=kernel,\n",
    "    # we'll talk about these two in the next lesson!\n",
    "    strides=1,\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image_filter))\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the detection step with the ReLU function. This function is much simpler than the convolution, as it doesn't have any parameters to set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_detect = tf.nn.relu(image_filter)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image_detect))\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "**TODO**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
