{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "Run the cell below to set everything up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.computer_vision.ex6 import *\n",
    "\n",
    "# Imports\n",
    "import visiontools\n",
    "from visiontools import StanfordCars\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load training and validation sets\n",
    "DATA_DIR = '/kaggle/input/stanford-cars-for-learn/'\n",
    "(ds_train_, ds_valid_), ds_info = tfds.load(\n",
    "    'stanford_cars/simple_2',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    "    data_dir=DATA_DIR,\n",
    "    download=False,\n",
    ")\n",
    "\n",
    "# Create data pipeline\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "SIZE = [192, 192]\n",
    "\n",
    "preprocess = visiontools.make_preprocessor(SIZE)\n",
    "\n",
    "ds_train = (ds_train_\n",
    "            .map(preprocess, AUTO)\n",
    "            .cache()\n",
    "#            .shuffle(1000)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTO))\n",
    "\n",
    "ds_valid = (ds_valid_\n",
    "            .map(preprocess, AUTO)\n",
    "            .cache()\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you'd like to see a few examples of the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.show_examples(ds_train_, ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune a Model\n",
    "\n",
    "This time you'll use the custom convnet you made in Lesson 2 as the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Define Model\n",
    "\n",
    "Here is a diagram for the model you'll define:\n",
    "\n",
    "<!--TODO: ex6.1 model-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "NUM_CLASSES = ds_info.features['label'].num_classes\n",
    "\n",
    "pretrained_base = tf.keras.models.load_model('mini_vgg_headless.h5')\n",
    "pretrained_base.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    # YOUR CODE HERE\n",
    "    ----\n",
    "])\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the model you defined by running the following cell, if you'd like to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "NUM_CLASSES = ds_info.features['label'].num_classes\n",
    "\n",
    "pretrained_base = tf.keras.models.load_model('mini_vgg_headless.h5')\n",
    "pretrained_base.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    pretrained_base,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Train Head\n",
    "\n",
    "Compile the model with the following parameters:\n",
    "- 10 epochs\n",
    "- `'binary_crossentropy'` loss\n",
    "- `'AUC'` metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.002,\n",
    "    decay_steps=41,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True,\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "# Number of epochs per round of training\n",
    "# YOUR CODE HERE\n",
    "EPOCHS = ____\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    # YOUR CODE HERE\n",
    "    ____\n",
    ")\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.002,\n",
    "    decay_steps=41,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True,\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "# Number of epochs per round of training\n",
    "EPOCHS = 5\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['AUC'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got the right answer, run the cell below to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(ds_train,\n",
    "                    validation_data=ds_valid,\n",
    "                    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Fine Tune Base\n",
    "\n",
    "Let's fine tune the convolutional layers in the last block of the model. Run the cell below to get a list of their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, layer in enumerate(pretrained_base.layers):\n",
    "    print(idx, layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the indices you'll select for retraining?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "idx_tuned=[____]\n",
    "\n",
    "q_3.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got the right answer, run the cell below to start the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, idx in enumerate(idx_tuned):\n",
    "    print(\"Unfreezing layer at index {}.\".format(idx))\n",
    "    pretrained_base.layers[idx].trainable = True\n",
    "\n",
    "    # Recompile model after unfreezing a layer.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['AUC'],\n",
    "    )\n",
    "    # Define epochs\n",
    "    INIT, = history.epoch[-1] + 1, # start after last iteration's end\n",
    "    TOTAL = history.epoch[-1] + 1 + EPOCHS\n",
    "    # Retrain with layer at idx unfrozen.\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_valid,\n",
    "        initial_epoch=INIT,\n",
    "        epochs=TOTAL,\n",
    "    )\n",
    "    # Concatenate this round's history to previous history\n",
    "    history_frame = pd.concat([history_frame, pd.DataFrame(history.history)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Evaluate\n",
    "\n",
    "Run the cell below to see the training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['AUC', 'val_AUC']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "q_4.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "The technique we saw in this lesson is just one way to do transfer learning. There are several others which might be better in other situations.\n",
    "\n",
    "- save \"bottlenecks\"\n",
    "- frozen base (like in Lesson 1)\n",
    "- fine tuning entire base with warmup\n",
    "\n",
    "We encourage you to explore these on your own!\n",
    "\n",
    "The technique we'll look at in the next lesson -- **data augmentation** -- is a great complement to fine tuning."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
