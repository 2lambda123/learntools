{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In these exercises, you'll explore what effect various random transformations have on an image, consider what kind of augmentation might be appropriate on a given dataset, and then use data augmentation with the *Car or Truck* dataset to train a custom network.\n",
    "\n",
    "Run the cell below to set everything up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learntools.computer_vision.cv_prelude import *\n",
    "# Setup feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.computer_vision.ex6 import *\n",
    "\n",
    "from learntools.computer_vision.cv_prelude import *\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.layers.experimental.preprocessing as preprocessing\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Explore Augmentation\n",
    "\n",
    "Uncomment a transformation and run the cell to see what it does. You can experiment with the parameter values too, if you like. (The `factor` parameters should be greater than 0 and, generally, less than 1.) Run the cell again if you'd like to get a new random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# all of the \"factor\" parameters indicate a percent-change\n",
    "augment = keras.Sequential([\n",
    "    # preprocessing.RandomContrast(factor=0.5),\n",
    "    preprocessing.RandomFlip(mode='horizontal'), # meaning, left-to-right\n",
    "    # preprocessing.RandomFlip(mode='vertical'), # meaning, top-to-bottom\n",
    "    # preprocessing.RandomWidth(factor=0.15), # horizontal stretch\n",
    "    # preprocessing.RandomRotation(factor=0.20),\n",
    "    # preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "])\n",
    "\n",
    "\n",
    "ex = next(iter(ds_train.unbatch().map(lambda x, y: x).batch(1)))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(16):\n",
    "    image = augment(ex, training=True)\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(tf.squeeze(image))\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the transformations you chose seem reasonable for the *Car or Truck* dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing an Augmentation\n",
    "\n",
    "In this exercise, we'll look at a few datasets and think about what kind of augmentation might be appropriate. Your reasoning might be different that what we discuss in the solution. That's okay. The point of these problems is just to think about how a transformation might interact with a classification problem -- for better or worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) EuroSAT\n",
    "\n",
    "The [EuroSAT](https://www.kaggle.com/ryanholbrook/eurosat) dataset consists of satellite images of the Earth classified by geographic feature. Run the next cell to see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, ds_info = tfds.load('eurosat/rgb:2.0.0',\n",
    "                        with_info=True,\n",
    "                        split='train',\n",
    "                        data_dir='../input/eurosat',\n",
    "                        download=False)\n",
    "ds = ds.shuffle(1024)\n",
    "\n",
    "tfds.show_examples(ds, ds_info);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kinds of transformations might be appropriate for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_2.a.hint()\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_2.a.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) TensorFlow Flowers\n",
    "\n",
    "The [TensorFlow Flowers](https://www.kaggle.com/ryanholbrook/tensorflow-flowers) dataset consists of photographs of flowers of several species. Run the next cell to see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, ds_info = tfds.load('tf_flowers:3.0.1',\n",
    "                        with_info=True,\n",
    "                        split='train',\n",
    "                        data_dir='../input/tensorflow-flowers',\n",
    "                        download=False)\n",
    "ds = ds.shuffle(1024)\n",
    "\n",
    "tfds.show_examples(ds, ds_info);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kinds of transformations might be appropriate for the TensorFlow Flowers dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_2.b.hint()\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_2.b.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Random Transforms to a Model\n",
    "\n",
    "Now you'll use data augmentation with a custom convnet similar to the one you built in Exercise 5. Since data augmentation effectively increases the size of the dataset, we can increase the capacity of the model in turn without as much risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a) Add Preprocessing Layers\n",
    "\n",
    "Add these preprocessing layers to the given model.\n",
    "\n",
    "```\n",
    "preprocessing.RandomContrast(factor=0.10),\n",
    "preprocessing.RandomFlip(mode='horizontal'),\n",
    "preprocessing.RandomRotation(factor=0.10),\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=[128, 128, 3]),\n",
    "    \n",
    "    # Data Augmentation\n",
    "    # ____,\n",
    "\n",
    "    # Block One\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Two\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Three\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Head\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "# Check your answer\n",
    "q_3.a.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Check number of layers\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=[128, 128, 3]),\n",
    "    \n",
    "    # Data Augmentation\n",
    "    preprocessing.RandomFlip(mode='horizontal'),\n",
    "    preprocessing.RandomRotation(factor=0.10),\n",
    "    \n",
    "    # Block One\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Two\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Three\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Head\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "q_3.a.assert_check_failed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Check layer types\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=[128, 128, 3]),\n",
    "    \n",
    "    # Data Augmentation\n",
    "    preprocessing.RandomRotation(factor=0.10),\n",
    "    preprocessing.RandomContrast(factor=0.10),\n",
    "    preprocessing.RandomFlip(mode='horizontal'),\n",
    "    \n",
    "    # Block One\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Two\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Three\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Head\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "q_3.a.assert_check_failed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=[128, 128, 3]),\n",
    "    \n",
    "    # Data Augmentation\n",
    "    preprocessing.RandomContrast(factor=0.15),\n",
    "    preprocessing.RandomFlip(mode='vertical'),\n",
    "    preprocessing.RandomRotation(factor=0.15),\n",
    "    \n",
    "    # Block One\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Two\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Three\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Head\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "q_3.a.assert_check_failed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=[128, 128, 3]),\n",
    "    \n",
    "    # Data Augmentation\n",
    "    preprocessing.RandomContrast(factor=0.10),\n",
    "    preprocessing.RandomFlip(mode='horizontal'),\n",
    "    preprocessing.RandomRotation(factor=0.10),\n",
    "    \n",
    "    # Block One\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Two\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Three\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Head\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "q_3.a.assert_check_passed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution code\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_3.a.hint()\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_3.a.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Train Model\n",
    "\n",
    "Now we'll train the model. Run the next cell to compile it with a loss and accuracy metric and fit it to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(epsilon=0.01)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=50,\n",
    ")\n",
    "\n",
    "# Plot learning curves\n",
    "import pandas as pd\n",
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the training curves. What there any sign of overfitting? How does the performance of this model compare to other models you've trained in this course?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "q_3.b.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "Data augmentation is a powerful and commonly-used tool to improve model training, not only for convolutional networks, but for many other kinds of neural network models as well. Whatever your problem, the principle remains the same: you can make up for an inadequacy in your data by adding in \"fake\" data to cover it over. Experimenting with augmentations is a great way to find out just how far your data can go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End #\n",
    "\n",
    "That's all for **Computer Vision** on Kaggle Learn! We hope you've enjoyed your time with us. The ideas you've learned in this course have prepared you for many other topics in deep learning; convolutional networks are used not only with images, but also with text, audio, and video. What are you interested in?\n",
    "\n",
    "- Instead of classifying images, maybe you'd like to create some new ones? **generative adversarial networks** (GANs) can learn how to do that. Check out the notebooks and discussion in the [Generative Dog Images](https://www.kaggle.com/c/generative-dog-images/) competition to learn more.\n",
    "- If you want to teach a network to learn *where* in an image something is, you might want to learn more about **image segmentation**. The [SIIM-ACR](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/) competition challenged Kagglers to locate a lung condition called *pneumothorax* in medical scans. The convolutional U-Net model was used successfully there.\n",
    "- The [Flickr Image](https://www.kaggle.com/hsankesara/flickr-image-dataset) dataset has a number of notebooks that will show you how to make a network that can automatically generate descriptions of an image.\n",
    "- [Celeb Faces Attributes (Celeb A)](https://www.kaggle.com/jessicali9530/celeba-dataset/kernels) is a large dataset of celebrity faces with annotated descriptions. There are lots of interesting projects you could try with it.\n",
    "\n",
    "Have fun learning!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
