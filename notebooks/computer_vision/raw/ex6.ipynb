{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "Run the cell below to set everything up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.computer_vision.ex6 import *\n",
    "\n",
    "# Imports\n",
    "import visiontools\n",
    "from visiontools import StanfordCars\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load training and validation sets\n",
    "DATA_DIR = '/kaggle/input/stanford-cars-for-learn/'\n",
    "(ds_train_, ds_valid_), ds_info = tfds.load(\n",
    "    'stanford_cars/simple_2',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    "    data_dir=DATA_DIR,\n",
    "    download=False,\n",
    ")\n",
    "\n",
    "# Create data pipeline\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "SIZE = [192, 192]\n",
    "\n",
    "preprocess = visiontools.make_preprocessor(SIZE)\n",
    "\n",
    "ds_train = (ds_train_\n",
    "            .map(preprocess, AUTO)\n",
    "            .cache()\n",
    "#            .shuffle(1000)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTO))\n",
    "\n",
    "ds_valid = (ds_valid_\n",
    "            .map(preprocess, AUTO)\n",
    "            .cache()\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you'd like to see a few examples of the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.show_examples(ds_train_, ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune a Model\n",
    "\n",
    "This time you'll use the custom convnet you made in Lesson 2 as the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Define Model\n",
    "\n",
    "Here is a diagram for the model you'll define:\n",
    "\n",
    "<!--TODO: ex6.1 model-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "NUM_CLASSES = ds_info.features['label'].num_classes\n",
    "\n",
    "pretrained_base = tf.keras.models.load_model('mini_vgg_headless.h5')\n",
    "pretrained_base.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    # YOUR CODE HERE\n",
    "    ----\n",
    "])\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the model you defined by running the following cell, if you'd like to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_4 (Model)              (None, 12, 12, 512)       6860672   \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 73728)             0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 73728)             0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 16)                1179664   \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 8,040,625\n",
      "Trainable params: 1,179,953\n",
      "Non-trainable params: 6,860,672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_4 (Model)              (None, 12, 12, 512)       6860672   \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 73728)             0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 4)                 294916    \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 7,155,613\n",
      "Trainable params: 294,941\n",
      "Non-trainable params: 6,860,672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "NUM_CLASSES = ds_info.features['label'].num_classes\n",
    "\n",
    "pretrained_base = tf.keras.models.load_model('mini_vgg_headless.h5')\n",
    "pretrained_base.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    pretrained_base,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Train Head\n",
    "\n",
    "Compile the model with the following parameters:\n",
    "- 10 epochs\n",
    "- `'binary_crossentropy'` loss\n",
    "- `'AUC'` metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 6, 6, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 4)                 73732     \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 14,788,425\n",
      "Trainable params: 73,737\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.002,\n",
    "    decay_steps=41,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True,\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "# Number of epochs per round of training\n",
    "# YOUR CODE HERE\n",
    "EPOCHS = ____\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    # YOUR CODE HERE\n",
    "    ____\n",
    ")\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.002,\n",
    "    decay_steps=41,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True,\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "# Number of epochs per round of training\n",
    "EPOCHS = 10\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['AUC'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got the right answer, run the cell below to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41/41 [==============================] - 12s 289ms/step - loss: 0.7495 - binary_accuracy: 0.7548 - AUC: 0.4809 - val_loss: 0.6753 - val_binary_accuracy: 0.8035 - val_AUC: 0.5000\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 10s 241ms/step - loss: 0.6650 - binary_accuracy: 0.8037 - AUC: 0.5171 - val_loss: 0.6557 - val_binary_accuracy: 0.8035 - val_AUC: 0.5000\n",
      "Epoch 3/10\n",
      "35/41 [========================>.....] - ETA: 0s - loss: 0.6479 - binary_accuracy: 0.8080 - AUC: 0.5359"
     ]
    }
   ],
   "source": [
    "history = model.fit(ds_train,\n",
    "                    validation_data=ds_valid,\n",
    "                    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Fine Tune Base\n",
    "\n",
    "Let's fine tune the convolutional layers in the last block of the model. Run the cell below to get a list of their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_7\n",
      "1 block1_conv1\n",
      "2 block1_pool\n",
      "3 block2_conv1\n",
      "4 block2_pool\n",
      "5 block3_conv1\n",
      "6 block3_conv2\n",
      "7 block3_pool\n",
      "8 block4_conv1\n",
      "9 block4_conv2\n",
      "10 block4_conv3\n",
      "11 block4_pool\n"
     ]
    }
   ],
   "source": [
    "for idx, layer in enumerate(pretrained_base.layers):\n",
    "    print(idx, layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the indices you'll select for retraining?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing layer at index 10.\n",
      "Epoch 61/70\n",
      "41/41 [==============================] - 6s 155ms/step - loss: 0.5652 - binary_accuracy: 0.8037 - AUC: 0.6415 - val_loss: 0.5926 - val_binary_accuracy: 0.8035 - val_AUC: 0.6900\n",
      "Epoch 62/70\n",
      "41/41 [==============================] - 5s 131ms/step - loss: 0.5713 - binary_accuracy: 0.8037 - AUC: 0.6880 - val_loss: 0.5859 - val_binary_accuracy: 0.8035 - val_AUC: 0.6909\n",
      "Epoch 63/70\n",
      "41/41 [==============================] - 5s 131ms/step - loss: 0.5415 - binary_accuracy: 0.8037 - AUC: 0.6796 - val_loss: 0.5467 - val_binary_accuracy: 0.8035 - val_AUC: 0.6838\n",
      "Epoch 64/70\n",
      "41/41 [==============================] - 5s 130ms/step - loss: 0.5185 - binary_accuracy: 0.8037 - AUC: 0.6772 - val_loss: 0.5235 - val_binary_accuracy: 0.8035 - val_AUC: 0.6901\n",
      "Epoch 65/70\n",
      "41/41 [==============================] - 5s 132ms/step - loss: 0.5159 - binary_accuracy: 0.8037 - AUC: 0.6681 - val_loss: 0.5208 - val_binary_accuracy: 0.8035 - val_AUC: 0.7005\n",
      "Epoch 66/70\n",
      "41/41 [==============================] - 5s 132ms/step - loss: 0.5174 - binary_accuracy: 0.8037 - AUC: 0.6610 - val_loss: 0.5091 - val_binary_accuracy: 0.8035 - val_AUC: 0.6975\n",
      "Epoch 67/70\n",
      "41/41 [==============================] - 5s 132ms/step - loss: 0.5230 - binary_accuracy: 0.8037 - AUC: 0.6621 - val_loss: 0.5617 - val_binary_accuracy: 0.8035 - val_AUC: 0.6941\n",
      "Epoch 68/70\n",
      "41/41 [==============================] - 5s 131ms/step - loss: 0.5237 - binary_accuracy: 0.8037 - AUC: 0.6600 - val_loss: 0.5064 - val_binary_accuracy: 0.8035 - val_AUC: 0.6977\n",
      "Epoch 69/70\n",
      "41/41 [==============================] - 5s 131ms/step - loss: 0.5171 - binary_accuracy: 0.8037 - AUC: 0.6430 - val_loss: 0.5049 - val_binary_accuracy: 0.8035 - val_AUC: 0.6985\n",
      "Epoch 70/70\n",
      "41/41 [==============================] - 5s 131ms/step - loss: 0.5167 - binary_accuracy: 0.8037 - AUC: 0.6335 - val_loss: 0.5030 - val_binary_accuracy: 0.8035 - val_AUC: 0.6972\n",
      "Unfreezing layer at index 9.\n",
      "Epoch 71/80\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 0.5473 - binary_accuracy: 0.8037 - AUC: 0.6529 - val_loss: 0.5626 - val_binary_accuracy: 0.8035 - val_AUC: 0.6980\n",
      "Epoch 72/80\n",
      "41/41 [==============================] - 5s 130ms/step - loss: 0.5421 - binary_accuracy: 0.8037 - AUC: 0.6694 - val_loss: 0.5555 - val_binary_accuracy: 0.8035 - val_AUC: 0.6986\n",
      "Epoch 73/80\n",
      "41/41 [==============================] - 5s 130ms/step - loss: 0.5361 - binary_accuracy: 0.8037 - AUC: 0.6742 - val_loss: 0.5432 - val_binary_accuracy: 0.8035 - val_AUC: 0.6833\n",
      "Epoch 74/80\n",
      "41/41 [==============================] - 5s 133ms/step - loss: 0.5259 - binary_accuracy: 0.8037 - AUC: 0.6615 - val_loss: 0.5252 - val_binary_accuracy: 0.8035 - val_AUC: 0.6934\n",
      "Epoch 75/80\n",
      "41/41 [==============================] - 5s 132ms/step - loss: 0.5178 - binary_accuracy: 0.8037 - AUC: 0.6623 - val_loss: 0.5196 - val_binary_accuracy: 0.8035 - val_AUC: 0.6938\n",
      "Epoch 76/80\n",
      "41/41 [==============================] - 5s 132ms/step - loss: 0.5185 - binary_accuracy: 0.8037 - AUC: 0.6574 - val_loss: 0.5355 - val_binary_accuracy: 0.8035 - val_AUC: 0.6951\n",
      "Epoch 77/80\n",
      "12/41 [=======>......................] - ETA: 2s - loss: 0.5706 - binary_accuracy: 0.7614 - AUC: 0.7042"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-1cb277b55e05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTOTAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Concatenate this round's history to previous history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "idx_tuned=[____]\n",
    "\n",
    "q_3.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got the right answer, run the cell below to start the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, idx in enumerate(idx_tuned):\n",
    "    print(\"Unfreezing layer at index {}.\".format(idx))\n",
    "    pretrained_base.layers[idx].trainable = True\n",
    "\n",
    "    # Recompile model after unfreezing a layer.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['AUC'],\n",
    "    )\n",
    "    # Define epochs\n",
    "    INIT, = history.epoch[-1] + 1, # start after last iteration's end\n",
    "    TOTAL = history.epoch[-1] + 1 + EPOCHS\n",
    "    # Retrain with layer at idx unfrozen.\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_valid,\n",
    "        initial_epoch=INIT,\n",
    "        epochs=TOTAL,\n",
    "    )\n",
    "    # Concatenate this round's history to previous history\n",
    "    history_frame = pd.concat([history_frame, pd.DataFrame(history.history)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Evaluate\n",
    "\n",
    "Run the cell below to see the training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['AUC', 'val_AUC']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "q_4.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "The technique we saw in this lesson is just one way to do transfer learning. There are several others which might be better in other situations.\n",
    "\n",
    "- save \"bottlenecks\"\n",
    "- frozen base (like in Lesson 1)\n",
    "- fine tuning entire base with warmup\n",
    "\n",
    "We encourage you to explore these on your own!\n",
    "\n",
    "The technique we'll look at in the next lesson -- **data augmentation** -- is a great complement to fine tuning."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
