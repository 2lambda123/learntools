{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Computer Vision! #\n",
    "\n",
    "<!-- TODO: lede -->\n",
    "\n",
    "<!-- TODO: HEADER ILLUSTRATION -->\n",
    "\n",
    "In this micro-course, you'll:\n",
    "- Use modern deep-learning networks to build an **image classifier** with Keras!\n",
    "- Design your own **custom convnet** with reusable blocks!\n",
    "- Master the art of **transfer learning** to boost your models!\n",
    "- Utilize **data augmentation** to extend a dataset--for free!\n",
    "- Learn the fundamentals of **convolution** and **pooling** so you can go even further!\n",
    "\n",
    "If you've taken the /Introduction to Deep Learning/ micro-course, you'll know everything you need to be successful.\n",
    "\n",
    "Now let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "This course will introduce you to the fundamental ideas of computer vision. Our goal is to learn how a neural network can \"understand\" a natural image well-enough to solve the same kinds of problems the human visual system can solve.\n",
    "\n",
    "The neural networks that are best at this task are called **convolutional neural networks** (Sometimes we say **convnet** or **CNN** instead.) Convolution is the mathematical operation these networks use in their layers that give them a structure different from the dense layers you learned about in the introductory course. In future lessons, you'll learn why this structure is so effective at solving computer vision problems.\n",
    "\n",
    "The ideas in this course are important to any kind of computer vision problem. We will apply them to the problem of **image classification**. At the end, however, you'll be prepared for other topics in computer vision like image segmentation and GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Convolutional Classifier #\n",
    "\n",
    "A convnet used for image classification consists of two parts: a **convolutional base** and a **dense head**.\n",
    "\n",
    "<!-- TODO: parts of a convnet -->\n",
    "\n",
    "The base is used to **extract the features** from an image. It is formed primarily of layers performing the convolution operation, but often includes other kinds of layers as well. (You'll learn about these in the next lesson.)\n",
    "\n",
    "The head is used to **determine the class** of the image. It is formed primarily of dense layers, but might include other layers like dropout. \n",
    "\n",
    "What do we mean by visual feature? A feature could be a line, a color, a texture, a shape, a pattern -- or some complicated combination.\n",
    "\n",
    "The whole process goes something like this:\n",
    "\n",
    "<!-- TODO: extract -> classify -->\n",
    "\n",
    "The features actually extracted aren't quite like this, but it gives the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Classifier #\n",
    "\n",
    "The goal of the network during training is to learn two things:\n",
    "1. which features to extract from an image (base),\n",
    "2. which class goes with what features (head).\n",
    "\n",
    "These days, convnets are rarely trained from scratch. More often, we **reuse the base of a pretrained model**, that is, a model already trained on some similar dataset.\n",
    "\n",
    "To this pretrained base we then **attach an untrained head**. Because the base has already learned to extract useful features, we then only need to train the head to classify the images in the new dataset.\n",
    "\n",
    "<!-- TODO: attach head to base -->\n",
    "\n",
    "Because the head usually consists of only a few dense layers, very accurate classifiers can be created from relatively little data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example #\n",
    "\n",
    "Let's walk through an example. Our goal is to create a classifier for the `Stanford Cars` dataset. It consists of about 16,000 images in 196 classes. The steps are basically the same as you learned about in the introductory course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Load Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8144 images belonging to 196 classes.\n",
      "Found 8041 images belonging to 196 classes.\n"
     ]
    }
   ],
   "source": [
    "#$HIDE$\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# DATA_DIR = '/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data'\n",
    "DATA_DIR = '/home/jovyan/work/kaggle/datasets/stanford-cars-keras/car_data/car_data'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VALID_DIR = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "ds_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "SIZE = (150, 150)\n",
    "\n",
    "ds_train = ds_gen.flow_from_directory(directory=TRAIN_DIR,\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      shuffle=True,\n",
    "                                      target_size=SIZE,\n",
    "                                      class_mode='sparse')\n",
    "\n",
    "ds_valid = ds_gen.flow_from_directory(directory=VALID_DIR,\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      shuffle=True,\n",
    "                                      target_size=SIZE,\n",
    "                                      class_mode='sparse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to prepare your dataset. We'll skip the details of loading for now, but let's look at a few examples of images and their classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Define Pretrained Base ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 14s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "pretrained_base = VGG16(include_top=False,\n",
    "                        weights='imagenet',\n",
    "                        input_shape=[*SIZE, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Attach Head ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    pretrained_base,\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(196, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Train ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 509 steps, validate for 503 steps\n",
      "Epoch 1/15\n",
      "509/509 [==============================] - 240s 472ms/step - loss: 5.3011 - accuracy: 0.0064 - val_loss: 7.9028 - val_accuracy: 0.0053\n",
      "Epoch 2/15\n",
      "509/509 [==============================] - 235s 462ms/step - loss: 5.2469 - accuracy: 0.0093 - val_loss: 5.2999 - val_accuracy: 0.0097\n",
      "Epoch 3/15\n",
      "509/509 [==============================] - 237s 465ms/step - loss: 5.1723 - accuracy: 0.0128 - val_loss: 5.1932 - val_accuracy: 0.0143\n",
      "Epoch 4/15\n",
      "509/509 [==============================] - 237s 465ms/step - loss: 5.1440 - accuracy: 0.0151 - val_loss: 5.1406 - val_accuracy: 0.0142\n",
      "Epoch 5/15\n",
      "509/509 [==============================] - 236s 464ms/step - loss: 5.1114 - accuracy: 0.0135 - val_loss: 5.0985 - val_accuracy: 0.0155\n",
      "Epoch 6/15\n",
      "509/509 [==============================] - 237s 466ms/step - loss: 5.0665 - accuracy: 0.0130 - val_loss: 5.0428 - val_accuracy: 0.0174\n",
      "Epoch 7/15\n",
      "509/509 [==============================] - 236s 463ms/step - loss: 4.9045 - accuracy: 0.0150 - val_loss: 5.3292 - val_accuracy: 0.0154\n",
      "Epoch 8/15\n",
      "509/509 [==============================] - 237s 465ms/step - loss: 4.6924 - accuracy: 0.0225 - val_loss: 4.7728 - val_accuracy: 0.0276\n",
      "Epoch 9/15\n",
      "509/509 [==============================] - 236s 464ms/step - loss: 4.4719 - accuracy: 0.0322 - val_loss: 4.8511 - val_accuracy: 0.0234\n",
      "Epoch 10/15\n",
      "509/509 [==============================] - 236s 464ms/step - loss: 4.2607 - accuracy: 0.0454 - val_loss: 4.6319 - val_accuracy: 0.0362\n",
      "Epoch 11/15\n",
      "509/509 [==============================] - 237s 465ms/step - loss: 4.0373 - accuracy: 0.0613 - val_loss: 12.6534 - val_accuracy: 0.0062\n",
      "Epoch 12/15\n",
      "509/509 [==============================] - 236s 464ms/step - loss: 3.8331 - accuracy: 0.0796 - val_loss: 4.6333 - val_accuracy: 0.0455\n",
      "Epoch 13/15\n",
      "509/509 [==============================] - 236s 464ms/step - loss: 4.3360 - accuracy: 0.0495 - val_loss: 4.4953 - val_accuracy: 0.0414\n",
      "Epoch 14/15\n",
      "509/509 [==============================] - 237s 465ms/step - loss: 4.0178 - accuracy: 0.0718 - val_loss: 4.5128 - val_accuracy: 0.0444\n",
      "Epoch 15/15\n",
      "509/509 [==============================] - 236s 465ms/step - loss: 3.6994 - accuracy: 0.1011 - val_loss: 4.7086 - val_accuracy: 0.0531\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history = model.fit(ds_train,\n",
    "                    validation_data=ds_valid,\n",
    "                    epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Evaluate ##\n",
    "\n",
    "When training a neural network, it's always a good idea to examine the loss and metric plots. The `history` object contains this information in a dictionary `history.history`. We can use Pandas to convert this dictionary to a dataframe and plot it with a built-in method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- discuss convergence, over/underfitting -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "In this lesson, we learned about the structure of a convnet classifier: a **head** to act as a classifier atop of a **base** which performs the feature extraction.\n",
    "\n",
    "The head, essentially, is an ordinary classifier like you learned about in the introductory course. For features, it uses those features extracted by the base. This is the basic idea behind CNN image classifiers: that we can attach a unit that performs feature engineering to the classifier itself.\n",
    "\n",
    "This is one of the big advantages deep neural networks have over traditional machine learning models: given the right network structure, the deep neural net can learn how to engineer the features it needs to solve its problem.\n",
    "\n",
    "In the remainder of this micro-course, we're going to explore this convolutional base."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
