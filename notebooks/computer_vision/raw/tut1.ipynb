{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Computer Vision! #\n",
    "\n",
    "Have you ever wanted to teach a computer to see? In this micro-course, that's exactly what you'll do!\n",
    "\n",
    "<center>\n",
    "<!-- <img src=\"./images/1-header.png\" width=\"1600\" alt=\"Header illustration: a line of cars.\"> -->\n",
    "<img src=\"\" width=\"1600\" alt=\"Header illustration\">\n",
    "</center>\n",
    "\n",
    "In this micro-course, you'll:\n",
    "- Use modern deep-learning networks to build an **image classifier** with Keras!\n",
    "- Design your own **custom convnet** with reusable blocks!\n",
    "- Learn the fundamental ideas behind visual **feature extraction**!\n",
    "- Master the art of **transfer learning** to boost your models!\n",
    "- Utilize **data augmentation** to extend your dataset!\n",
    "\n",
    "If you've taken the *Introduction to Deep Learning* micro-course, you'll know everything you need to be successful.\n",
    "\n",
    "Now let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "This course will introduce you to the fundamental ideas of computer vision. Our goal is to learn how a neural network can \"understand\" a natural image well-enough to solve the same kinds of problems the human visual system can solve.\n",
    "\n",
    "The neural networks that are best at this task are called **convolutional neural networks** (Sometimes we say **convnet** or **CNN** instead.) Convolution is the mathematical operation that gives the layers of a convnet their unique structure. In future lessons, you'll learn why this structure is so effective at solving computer vision problems.\n",
    "\n",
    "We will apply these ideas to the problem of **image classification**: given a picture, can we train a computer to tell us what it's a picture *of*? You may have seen [apps](https://identify.plantnet.org/) that can identify a species of plant from a photograph. That's an image classifier! In this micro-course, you'll learn how to build image classifiers just as powerful as those used in professional applications.\n",
    "\n",
    "While our focus will be on image classification, what you'll learn in this micro-course is relevant to every kind of computer vision problem. At the end, you'll be ready to move on to more advanced applications like [generative adversarial networks](https://www.kaggle.com/tags/gan) and [image segmentation](https://www.kaggle.com/tags/object-segmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Convolutional Classifier #\n",
    "\n",
    "A convnet used for image classification consists of two parts: a **convolutional base** and a **dense head**.\n",
    "\n",
    "<center>\n",
    "<!-- <img src=\"./images/1-parts-of-a-convnet.png\" width=\"600\" alt=\"The parts of a convnet: image, base, head, class; input, extract, classify, output.\">-->\n",
    "<img src=\"https://i.imgur.com/U0n5xjU.png\" width=\"600\" alt=\"The parts of a convnet: image, base, head, class; input, extract, classify, output.\">\n",
    "</center>\n",
    "\n",
    "The base is used to **extract the features** from an image. It is formed primarily of layers performing the convolution operation, but often includes other kinds of layers as well. (You'll learn about these in the next lesson.)\n",
    "\n",
    "The head is used to **determine the class** of the image. It is formed primarily of dense layers, but might include other layers like dropout. \n",
    "\n",
    "What do we mean by visual feature? A feature could be a line, a color, a texture, a shape, a pattern -- or some complicated combination.\n",
    "\n",
    "The whole process goes something like this:\n",
    "\n",
    "<center>\n",
    "<!-- <img src=\"./images/1-extract-classify.png\" width=\"600\" alt=\"The idea of feature extraction.\"> -->\n",
    "<img src=\"https://i.imgur.com/UUAafkn.png\" width=\"600\" alt=\"The idea of feature extraction.\">\n",
    "</center>\n",
    "\n",
    "The features actually extracted look a bit different, but it gives the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Classifier #\n",
    "\n",
    "The goal of the network during training is to learn two things:\n",
    "1. which features to extract from an image (base),\n",
    "2. which class goes with what features (head).\n",
    "\n",
    "These days, convnets are rarely trained from scratch. More often, we **reuse the base of a pretrained model**.\n",
    "\n",
    "To the pretrained base we then **attach an untrained head**. Because the base has already learned to extract useful features, we then only need to train the head to classify the images in the new dataset.\n",
    "\n",
    "<center>\n",
    "<!-- <img src=\"./images/1-attach-head-to-base.png\" width=\"400\" alt=\"Attaching a new head to a trained base.\"> -->\n",
    "<img src=\"https://imgur.com/E49fsmV.png\" width=\"400\" alt=\"Attaching a new head to a trained base.\">\n",
    "</center>\n",
    "\n",
    "Because the head usually consists of only a few dense layers, very accurate classifiers can be created from relatively little data. \n",
    "\n",
    "Reusing a pretrained model is a technique known as **transfer learning**. It is so effective, that almost every image classifier these days will make use of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Train a Convnet Classifier #\n",
    "\n",
    "Let's walk through an example. The dataset we'll use is derived from the **Stanford Cars** dataset. Our dataset contains about 10,000 images of various models of automobile. Our task will be to determine whether the image is of a **Car** or of a **Truck**.\n",
    "\n",
    "The steps are basically the same as you learned about in the introductory course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Load Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "# Imports\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import visiontools\n",
    "from visiontools import StanfordCars\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Reproducibility\n",
    "def seed_everything(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed = 31415\n",
    "seed_everything(seed)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load training and validation sets\n",
    "DATA_DIR = '/kaggle/input/stanford-cars-for-learn/'\n",
    "(ds_train_, ds_valid_), ds_info = tfds.load(\n",
    "    'stanford_cars/simple',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    "    data_dir=DATA_DIR,\n",
    ")\n",
    "print((\"Loaded {} training examples \" +\n",
    "       \"and {} validation examples \" +\n",
    "       \"with classes {}.\").format(\n",
    "           ds_info.splits['train'].num_examples,\n",
    "           ds_info.splits['test'].num_examples,\n",
    "           ds_info.features['label'].names))\n",
    "\n",
    "# Create data pipeline\n",
    "BATCH_SIZE = 16\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "SIZE = [192, 192]\n",
    "preprocess = visiontools.make_preprocessor(size=SIZE)\n",
    "\n",
    "ds_train = (ds_train_\n",
    "            .map(preprocess)\n",
    "            .cache()\n",
    "            .shuffle(ds_info.splits['train'].num_examples)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTO))\n",
    "\n",
    "ds_valid = (ds_valid_\n",
    "            .map(preprocess)\n",
    "            .cache()\n",
    "            .shuffle(ds_info.splits['test'].num_examples)             \n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to prepare your dataset. We've already preloaded a training split `ds_train` and a validation split `ds_valid`, so let's skip the details of loading data for now and look at a few examples in the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "import matplotlib.pyplot as plt\n",
    "tfds.show_examples(ds_train_, ds_info);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Define Pretrained Base ##\n",
    "\n",
    "The most commonly used dataset for pretraining is [*ImageNet*](http://image-net.org/about-overview), a large dataset of many kind of natural images. Keras includes a variety models pretrained on ImageNet in its [`applications` module](https://www.tensorflow.org/api_docs/python/tf/keras/applications).\n",
    "\n",
    "The pretrained model we'll use is called **VGG16**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_base = tf.keras.models.load_model(\n",
    "    '/kaggle/input/cv-course-models/cv-course-models/vgg16-pretrained-base',\n",
    ")\n",
    "pretrained_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Attach Head ##\n",
    "\n",
    "Next, we attach the classifier head. For this example, we'll use a layer of hidden units (the first `Dense` layer) followed by a layer to transform the outputs to a probability score for class 1, `Truck`. The `Flatten` layer transforms the multidimensional outputs of the base into the one dimensional inputs needed by the head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    pretrained_base,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(6, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Train ##\n",
    "\n",
    "Finally, we'll train the model. First, compile the model with the training parameters: an `optimizer`, a `loss` function, and any desired `metrics`. (Review Lesson 5 of *Introduction to Deep Learning* if this is unfamiliar.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Evaluate ##\n",
    "\n",
    "When training a neural network, it's always a good idea to examine the loss and metric plots. The `history` object contains this information in a dictionary `history.history`. We can use Pandas to convert this dictionary to a dataframe and plot it with a built-in method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['accuracy', 'val_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "In this lesson, we learned about the structure of a convnet classifier: a **head** to act as a classifier atop of a **base** which performs the feature extraction.\n",
    "\n",
    "The head, essentially, is an ordinary classifier like you learned about in the introductory course. For features, it uses those features extracted by the base. This is the basic idea behind CNN image classifiers: that we can attach a unit that performs feature engineering to the classifier itself.\n",
    "\n",
    "This is one of the big advantages deep neural networks have over traditional machine learning models: given the right network structure, the deep neural net can learn how to engineer the features it needs to solve its problem.\n",
    "\n",
    "Over this course, we're going to explore this convolutional base and how it performs this feature engineering. Then you'll learn how to apply these ideas to design classifiers even better than the one from this lesson."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
