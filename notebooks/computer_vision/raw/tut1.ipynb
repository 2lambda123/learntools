{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Computer Vision! #\n",
    "\n",
    "Congratulations! You've just been hired as KaggleKars first data scientist! Are you ready?\n",
    "\n",
    "<!-- TODO: HEADER ILLUSTRATION -->\n",
    "\n",
    "In this micro-course, you'll:\n",
    "- Design a state-of-the-art **image classifier** with Keras!\n",
    "- Master the art of **transfer learning** to boost your models!\n",
    "- See inside a **convolutional layer** <!-- visualize what the model learns -->\n",
    "- Use the powerful **TPU accelerator** to speed-up your training!\n",
    "- Utilize **data augmentation** to get more data--for free!\n",
    "\n",
    "If you've taken the /Introduction to Deep Learning/ micro-course, you'll know everything you need to be successful.\n",
    "\n",
    "Now let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Cars #\n",
    "\n",
    "KaggleKars, a new company, has an idea for an app. If someone sees a car they like, they can take a picture, and the app will suggest cars for sale nearby of the same kind. Your job is to design the **image classifier**. If a user uploads a picture like this: \n",
    "\n",
    "<!-- TODO: picture of car -->\n",
    "\n",
    "your classifier should output the make, model, and year like this: \n",
    "\n",
    "<!-- TODO: class label -->\n",
    "\n",
    "The company has decided for now to focus on a list of 196 different vehicles. They have collected a dataset consisting of 16,185 images of these vehicles, divided into a **training set** of 8,144 images and a **validation set** of 8,041 images. The company says that to be successful, they need the classifier to achieve **95% accuracy** on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Processing Units #\n",
    "\n",
    "In this course, you'll be working with large models and large datasets. Training these models on an ordinary CPU would be very time consuming, and because of memory limitations, the accuracy of your model could suffer as well.\n",
    "\n",
    "Instead, you'll train your models on a **Tensor Processing Unit** or **TPU**. A TPU is an accelerator much like the GPU you used in the introductory course. The TPUs that Kaggle provides, however, are able to handle much larger workloads than the GPUs.\n",
    "\n",
    "<!-- TODO: picture of tpu -->\n",
    "\n",
    "TPUs are powerful and they require a bit of special handling. They are so powerful in fact that the hardest part of using them is keeping them busy!\n",
    "\n",
    "The result is worth the effort though. Models that could take weeks to train on a home computer can be trained in minutes on a cloud TPU. (And Kaggle gives you 30 hours each week for free!)\n",
    "\n",
    "What's more, the methods that you'll learn in this course will scale to workloads many times as large. Essentially the same methods could be used to train the deep learning models like BERT and BigGan, which have billions of parameters. After completing this micro-course, you'll have tools to solve demanding real-world problems.\n",
    "\n",
    "This code will initialize the TPU in preparation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "print('Running on TPU ', tpu.master())\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not important to understand the details. Just cut-and-paste whenever you want to use TPU acceleration.\n",
    "\n",
    "# Loading Data  #\n",
    "\n",
    "The first step in building a machine learning model is preparing the data. To run efficiently, TPUs require data in a special format called a TFRecord. We'll use the [TensorFlow Datasets](https://www.tensorflow.org/datasets) (TFDS) library to help us load these files, instead of the Keras data loader you used in *Intro to Deep Learning*.\n",
    "<!-- TODO: ask Alexis what she's using -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads a dataset prepared for supervised training together with its metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_datasets import KaggleDatasets\n",
    "from visiontools import StanfordCars\n",
    "\n",
    "DATA_DIR = KaggleDatasets.get_gcs_path()\n",
    "\n",
    "(ds_train, ds_valid), ds_info = tfds.load('stanford_cars/simple',\n",
    "                                          data_dir=DATA_DIR,\n",
    "                                          split=['train', 'test'],\n",
    "                                          with_info=True,\n",
    "                                          as_supervised=True,\n",
    "                                          shuffle_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that we specified the data directory as a Google Cloud bucket. Locating the dataset there helps the TPU to retrieve the data more quickly. You'll also notice that we've loaded the ~stanford_cars/simple~ version of the dataset. It has only three classes: `Convertible`, `SUV`, and `Wagon`. In the exercises, you'll use the complete dataset with all 196 classes.\n",
    "\n",
    "Now let's take a look at a few examples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.visualization.show_examples(ds_info, ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO: needs tfds v3.0.0 (nightly) for supervised dataset -->\n",
    "\n",
    "The `ds_train` and `ds_valid` objects are both generators that yield pairs `(image, label)`. \n",
    "The following code optimizes these generators for use with the TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "ds_train = (\n",
    "    ds_train.batch(BATCH_SIZE) # train images in batches, instead of one at a time\n",
    "    .cache() # save the batches in memory, instead of reloading each step\n",
    "    .prefetch(AUTO) # use the CPU to fetch a batch while the TPU is busy\n",
    ")\n",
    "\n",
    "ds_valid = (\n",
    "    ds_valid.batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO: make sure the batch size is compatible with the size of the dataset -->\n",
    "\n",
    "In Lesson 5, you'll see how to add **data augmentation** to this pipeline to give a boost to your model.\n",
    "\n",
    "Now that our data is loaded and optimized, we're ready to build the network!\n",
    "\n",
    "# Transfer Learning #\n",
    "\n",
    "To be accurate, an image classifier requires a large amount of data. This is especially true when there are a many labels to learn. (Such problems are sometimes called *fine-grained*.) To achieve your goal of 95% accuracy over 196 classes, you would need much more data than you've been provided with.\n",
    "\n",
    "The best solution would be to get more data. High-quality data, however, is expensive in both time and money -- neither of which (your company informs you) you happen to have. Fortunately, there is another solution that is highly effective and also virtually free. It is called **transfer learning**.\n",
    "\n",
    "This is the insight behind transfer learning: a neural network trained on one set of images will have learned a lot about any similar set of images, too. So why not make use of that learning? In fact, all we need to do is replace the layers closest to the outputs and then retrain the model.\n",
    "\n",
    "<!-- TODO: picture of replacing head of network for transfer learning -->\n",
    "\n",
    "Here is the Keras code that implements transfer learning with a pretrained model named **VGG16**. By using the `strategy.scope` context, we make our model aware of the TPU we initialized earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [128, 128] # [width, height]\n",
    "NUM_LABELS = 3 # Convertible, SUV, Wagon\n",
    "\n",
    "with strategy.scope():\n",
    "    # Load the pretrained VGG16 model\n",
    "    pretrained_model = tf.keras.applications.VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False, # replace layers closest to outputs\n",
    "        input_shape=[*IMAGE_SIZE, 3],\n",
    "    )\n",
    "    # pretrained_model.trainable = False # TODO: which?\n",
    "    # Construct the model\n",
    "    model = tf.keras.Sequential([\n",
    "        pretrained_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(), # TODO: replace this\n",
    "        tf.keras.layers.Dense(NUM_LABELS,\n",
    "                              activation='softmax',\n",
    "                              dtype=tf.float32)\n",
    "    ])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 is smaller than many modern convnets, but large enough to be effective on real problems. It is appropriate for our smaller example dataset.\n",
    "\n",
    "It has been pretrained on a set of images called **ImageNet**, a collection of about 14 million images of over 20,000 classes of everyday things. It is a good dataset to use for transfer learning whenever you are training on natural images. <!-- TODO: check ImageNet numbers -->\n",
    "\n",
    "# Train the Model #\n",
    "\n",
    "The hard part is over. Now all we need to do is set it going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(ds_train,\n",
    "                    validation_data=ds_valid,\n",
    "                    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO: put callbacks in visiontools? how successful without? -->\n",
    "\n",
    "When training a neural network, it's always a good idea to examine the loss and metric plots. The `history` object contains this information in a dictionary `history.history`. We can use Pandas to convert this dictionary to a dataframe and plot it with a built-in method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- discuss convergence, over/underfitting -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate #\n",
    "\n",
    "Let's get a closer look at how our classifier performed.\n",
    "\n",
    "<!-- TODO: classification report -->\n",
    "```python .noeval\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "label_names = \n",
    "true_labels =\n",
    "predicted_labels =\n",
    "classification_report(true_labels, predicted_labels, target_names=label_names)\n",
    "```\n",
    "\n",
    "<!-- TODO: plot_confusion_matrix in visiontools -->\n",
    "```python .noeval\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cmat = confusion_matrix(true_labels, predicted_labels, label_names)\n",
    "sns.heatmap(cmat, cmap='Blues');\n",
    "```\n",
    "\n",
    "# Conclusion #\n",
    "\n",
    "In this tutorial, you learned how you can leverage the power of the TPU to greatly accelerate your training. TPUs can handle the large datasets and large models demanded by today's image applications. <!-- TODO: awkward -->\n",
    "\n",
    "You also learned how to reuse a pretrained model with **transfer learning**. Transfer learning is one of the best and easiest ways to give your neural network a boost. And best of all -- it's free!\n",
    "\n",
    "# Your Turn #\n",
    "\n",
    "Now you have what you need to build a powerful image classifier of your own. Move on to the first exercise and try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The insight of transfer learning is that if we remove those topmost layers, we can reuse the bottom part, the part that's most likely to generalize. <\\!-- last two paragraphs are awkward -\\-> -->\n",
    "\n",
    "<!-- The kinds of deep neural networks you'll be using in this course are called **convolutional neural networks**. They have two parts: a **dense head** and a **convolutional base**. -->\n",
    "\n",
    "<!-- <\\!-- DIAGRAM: base/head -\\-> -->\n",
    "\n",
    "<!-- The base learns the features, and the head learns the classes. So to reuse a model trained on a different different classes, we want to keep the base, but train a new head. Additionally, we can \"unfreeze\" the topmost layers of the base so that they too will adapt to the new dataset. -->\n",
    "\n",
    "<!-- <\\!-- DIAGRAM: show swapping:imagenet to cars -\\-> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- move most of this discussion to next lesson\n",
    "- training an image classifier is hard, especially when you have a lot of labels\n",
    "  - for instance, /this problem/ needed /this many/ examples to get to /X%/ accuracy\n",
    "  - but you only have around /this many/ images per car! it you start from scratch, it won't be nearly enough\n",
    "- fortunately, there's a way around this problem. It's called **transfer learning**.\n",
    "- as complicated as the visual world is... visual data has a lot in common...\n",
    "  - for instance, all pictures will have lines and curves, transitions from one color to another\n",
    "  - (artists have known this for a long time... some artists say there are only x basic shapes! /link/)\n",
    "  - so an image classifier trained on one dataset will already know most of what it needs to be successful on any other dataset, so why not reuse that information? that's the idea behind transfer learning!\n",
    "- ImageNet contains over 14 million images in over 22,000 categories. Keras contains a number of powerful models that have been trained on ImageNet. And you can download them yourself!  -->\n",
    "\n",
    "<!-- To classify a large number of cars, classification -->\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
