{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In the last lesson, we learned how to use the transfer learning technique called fine tuning to improve our classifier. The basis of this technique was making use of the hierarchy of features within the base.\n",
    "\n",
    "In this lesson, we'll make use another property: **invariance**. We'll see how we can extend the translation invariance we learned about in Lesson 3 to other kinds of invariance which will improve our classifier even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Invariance #\n",
    "\n",
    "An idea we touched on when looking at pooling was **invariance**. Broadly, to say that a machine learning model is invariant to some property means that it will treat two examples differing only by that property as being exactly the same. We said that pooling made a network invariant to small shifts in the position of a feature. The network simply ignores that kind of difference.\n",
    "\n",
    "To solve a classification problem means to find a model that is invariant with respect to the class labels: it should assign the same label to all images in the same class.\n",
    "\n",
    "Giving our model translation invariance is helpful because things in the same class tend to have the same features, even if those features are shifted around. If our classifier detects a beak and feathers, our picture is almost certainly of a bird and not a fish or a horse, regardless of where those beak and features happen to be.\n",
    "\n",
    "<!--TODO: beak and feathers-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Invariance #\n",
    "\n",
    "So one way to improve a model is to build an invariance into the model itself, like we did with pooling. The other way is to *teach* the model the invariance. This is what you are doing when you show the model many different images with the same class label. It learns to treat those images the same.\n",
    "\n",
    "<!--TODO: images of the same class-->\n",
    "\n",
    "A typical dataset, however, won't have enough examples to fully teach the model all the ways in which images from a class might be different. If you need to improve the accuracy of your model, the best thing is to collect more data. Another (much less expensive) way is to *modify* the data you already have.\n",
    "\n",
    "The idea behind **data augmentation** is that you can teach your model an invariance by transforming your existing data in ways that images of a class in unseen data might vary. For instance, if you are classifying cars, your classifier should know that whether a car is facing left-to-right or right-to-left doesn't affect what class the car is in. If a car is a Honda in one direction, it's also a Honda in any other direction.\n",
    "\n",
    "<!--TODO: cars in different directions -->\n",
    "\n",
    "So, if you augment your dataset by flipping all of your images, you help your classifier to learn that this is a distinction it should ignore.\n",
    "\n",
    "<!--TODO: data for free-->\n",
    "\n",
    "Many kinds of transformations can be used to augment image data. Keras provides several through its `ImageDataLoader`:\n",
    "\n",
    "- example\n",
    "- example\n",
    "- example\n",
    "\n",
    "The `albumentations` is another augmentation package; it has been popular with Kagglers in competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Training with Data Augmentation #\n",
    "\n",
    "Let's see how we can use the data augmentations provided by Keras to improve the performance of the classifier from Lesson 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create Data Loader with Augmentations ##\n",
    "\n",
    "As mentioned, Keras applies augmentations through its data loader. Let's use `vertical_flip`, `zoom`, `brightness`, and `rotation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# DATA_DIR = '/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data'\n",
    "DATA_DIR = '/home/jovyan/work/kaggle/datasets/stanford-cars-keras/car_data/car_data'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VALID_DIR = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "ds_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "SIZE = (150, 150)\n",
    "\n",
    "ds_train = ds_gen.flow_from_directory(directory=TRAIN_DIR,\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      shuffle=True,\n",
    "                                      target_size=SIZE,\n",
    "                                      class_mode='sparse')\n",
    "\n",
    "ds_valid = ds_gen.flow_from_directory(directory=VALID_DIR,\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      shuffle=True,\n",
    "                                      target_size=SIZE,\n",
    "                                      class_mode='sparse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the effect these augmentations had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Define Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "pretrained_base = VGG16(include_top=False,\n",
    "                        weights='imagenet',\n",
    "                        input_shape=[*SIZE, 3])\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    pretrained_base,\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(196, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Train and Evaluate ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history = model.fit(ds_train,\n",
    "                    validation_data=ds_valid,\n",
    "                    epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
