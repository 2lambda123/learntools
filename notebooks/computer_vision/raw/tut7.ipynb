{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In the last lesson, we learned how to use the transfer learning technique called fine tuning to improve our classifier. The basis of this technique was making use of the hierarchy of features within the base.\n",
    "\n",
    "In this lesson, we'll make use another property: **invariance**. We'll see how we can extend the translation invariance we learned about in Lesson 3 to other kinds of invariance which will improve our classifier even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Invariance #\n",
    "\n",
    "An idea we touched on when looking at pooling was **invariance**. Broadly, to say that a machine learning model is invariant to some property means that it will treat two examples differing only by that property as being exactly the same. We said that pooling made a network invariant to small shifts in the position of a feature. The network simply ignores that kind of difference.\n",
    "\n",
    "To solve a classification problem means to find a model that is invariant with respect to the class labels: it should assign the same label to all images in the same class.\n",
    "\n",
    "Giving our model translation invariance is helpful because things in the same class tend to have the same features, even if those features are shifted around. If our classifier detects a beak and feathers, our picture is almost certainly of a bird and not a fish or a horse, regardless of where those beak and features happen to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Invariance #\n",
    "\n",
    "So one way to improve a model is to build an invariance into the model itself, like we did with pooling. The other way is to *teach* the model the invariance. This is what you are doing when you show the model many different images with the same class label. It learns to treat those images the same.\n",
    "\n",
    "<!--TODO: images of the same class-->\n",
    "<figure>\n",
    "<img src=\"\" width=400 alt=\"Images of the same class.\">\n",
    "</figure>\n",
    "\n",
    "A typical dataset, however, won't have enough examples to fully teach the model all the ways in which images from a class might be different. If you need to improve the accuracy of your model, the best thing is to collect more data. Another (much less expensive) way is to *modify* the data you already have.\n",
    "\n",
    "The idea behind **data augmentation** is that you can teach your model an invariance by transforming your existing data in ways that images of a class in unseen data might vary. For instance, if you are classifying cars, your classifier should know that whether a car is facing left-to-right or right-to-left doesn't affect what class the car is in. If a car is a Honda in one direction, it's also a Honda in any other direction.\n",
    "\n",
    "<!--TODO: cars in different directions -->\n",
    "<figure>\n",
    "<img src=\"\" width=400 alt=\"Same class of car in different directions.\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if you augment your dataset by flipping all of your images, you help your classifier to learn that this is a distinction it should ignore.\n",
    "\n",
    "<!--TODO: data for free-->\n",
    "<figure>\n",
    "<img src=\"\" width=400 alt=\"Augmented cars.\">\n",
    "</figure>\n",
    "\n",
    "TensorFlow includes a number of data augmentations in its `images` module, and there are a number of others in the **TensorFlow Addons** library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Training with Data Augmentation #\n",
    "\n",
    "Let's see how we can use the data augmentations provided by Keras to improve the performance of the classifier from Lesson 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create Data Loader with Augmentations ##\n",
    "\n",
    "Define augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import visiontools\n",
    "\n",
    "augment = visiontools.make_augmentor(\n",
    "    horizontal_flip=True,\n",
    "    brightness_delta=0.2,\n",
    "    hue_delta=0.5,\n",
    "    saturation_range=[0.0, 1.25],\n",
    "    contrast_range=[0.8, 1.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "# Imports\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import visiontools\n",
    "from visiontools import StanfordCars\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Reproducibility\n",
    "def seed_everything(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed = 31415\n",
    "seed_everything(seed)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load training and validation sets\n",
    "DATA_DIR = '/kaggle/input/stanford-cars-for-learn/'\n",
    "(ds_train_, ds_valid_), ds_info = tfds.load(\n",
    "    'stanford_cars/simple',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    "    data_dir=DATA_DIR,\n",
    ")\n",
    "print((\"Loaded {} training examples \" +\n",
    "       \"and {} validation examples \" +\n",
    "       \"with classes {}.\").format(\n",
    "           ds_info.splits['train'].num_examples,\n",
    "           ds_info.splits['test'].num_examples,\n",
    "           ds_info.features['label'].names))\n",
    "\n",
    "# Create data pipeline\n",
    "BATCH_SIZE = 16\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "SIZE = [192, 192]\n",
    "preprocess = visiontools.make_preprocessor(size=SIZE)\n",
    "\n",
    "ds_train = (ds_train_\n",
    "            .map(preprocess)\n",
    "            .cache()\n",
    "            .shuffle(ds_info.splits['train'].num_examples)\n",
    "            .map(augment, AUTO)            \n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTO))\n",
    "\n",
    "ds_valid = (ds_valid_\n",
    "            .map(preprocess)\n",
    "            .cache()\n",
    "            .shuffle(ds_info.splits['test'].num_examples)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what effect these augmentations can have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "for i, (image, label) in enumerate(ds_train.unbatch().take(18)):\n",
    "    plt.subplot(3, 6, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Define Model ##\n",
    "\n",
    "We'll continue with the VGG16 model we've used throughout this microcourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "pretrained_base = tf.keras.models.load_model(\n",
    "    '/kaggle/input/cv-course-models/cv-course-models/vgg16-pretrained-base',\n",
    ")\n",
    "pretrained_base.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    pretrained_base,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Train and Evaluate ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_frame = pd.DataFrame(history.history)\n",
    "\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['accuracy', 'val_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "The `albumentations` library is another source for augmentations; it has been popular with Kagglers in competitions."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
