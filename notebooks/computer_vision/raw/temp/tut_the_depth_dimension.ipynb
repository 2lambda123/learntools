{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TITLE: Filters and Feature Maps -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features: The Depth Dimension #\n",
    "\n",
    "The first feature maps are the color channels of the image. A grayscale image has one channel, the gray value. A color image will have three channels: red, green, and blue.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-channels-rgb.png\" width=\"600\" alt=\"The channels of a color image.\"> -->\n",
    "<img src=\"https://i.imgur.com/JUhUdgz.png\" width=\"600\" alt=\"The channels of a color image.\">\n",
    "</figure>\n",
    "\n",
    "<!--------->\n",
    "\n",
    "When an image first enters a network, it exists as a set of channels. We usually think about the channels as being the *depth* dimension, with the width and height as the two spatial dimensions. In the language of TensorFlow, an image is a tensor with shape `[height, width, channels]`.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-channels-stack.png\" width=\"300\" alt=\"Channels form the depth dimension.\"> -->\n",
    "<img src=\"https://i.imgur.com/JiUjn7o.png\" width=\"300\" alt=\"Channels form the depth dimension.\">\n",
    "</figure>\n",
    "\n",
    "A convolutional layer will apply a kernel to each channel in the input, and the collection of these kernels is what we call a **filter**. One filter produces one feature map.\n",
    "\n",
    "<!--TODO: filter to feature map-->\n",
    "\n",
    "A convolutional layer may produce many feature maps. So, if a convolutional layer producing 16 feature maps is applied to an image with 3 channels, it will contain 16*3=48 kernels.\n",
    "\n",
    "As mentioned, the channels are the first set of feature maps. More generally then, then depth dimension of the activation tensors contains the feature maps: `[height, width, features]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--------------->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As more extraction operations are applied, the feature maps become increasingly refined.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-simple-to-complex.png\" width=\"800\" alt=\"Feature maps, simple to complex.\"> -->\n",
    "<img src=\"https://i.imgur.com/VqmC1rm.png\" width=\"600\" alt=\"Feature maps, simple to complex.\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Visualize Activations #\n",
    "\n",
    "It can be instructive to look at the activations an image produces in a network throughout its layers. We've included a function the `visiontools` module that will plot them for you. Let's look at some activations in the network from Lesson 2 (using the same image as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('custom_convnet_512.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the image we'll run through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.io.read_file('images/car_0.jpg')\n",
    "image = tf.io.decode_image(image, channels=3)\n",
    "\n",
    "SIZE = [512, 512]\n",
    "image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "image = tf.image.resize(image, size=SIZE, method='nearest')\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following displays the first few feature maps within each convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from visiontools import show_feature_maps\n",
    "\n",
    "layer_names = [layer.name for layer in model.layers\n",
    "               if layer.__class__.__name__ is 'Conv2D']\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    show_feature_maps(model, layer_name, rows=2, cols=4, width=16)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the features extracted become more and more refined as the activations flow deeper into the network. You'll explore some other models in the exercises!\n",
    "\n",
    "\n",
    "# Example - Visualize Features #\n",
    "\n",
    "We can also visualize the kinds of features the network has learned to recognize.\n",
    "\n",
    "```python\n",
    "from visiontools import show_filters\n",
    "\n",
    "layer_names = [layer.name for layer in model.layers\n",
    "               if layer.__class__.__name__ is 'Conv2D']\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    show_filters(model, layer_name, rows=2, cols=4, width=16)\n",
    "    plt.show();\n",
    "```\n",
    "<!-- #region -->\n",
    "You can see how the features extracted become more and more refined as the activations flow deeper into the network. You'll explore some other models in the exercises!\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
