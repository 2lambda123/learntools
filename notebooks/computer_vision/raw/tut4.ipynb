{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--TITLE: Moving Windows-->\n",
    "# Introduction #\n",
    "\n",
    "In the previous two lessons, we learned about the three operations that carry out feature extraction from an image:\n",
    "1. *filter* with a **convolution** layer\n",
    "2. *detect* with **ReLU** activation\n",
    "3. *condense* with a **maximum pooling** layer\n",
    "\n",
    "The convolution and pooling operations share a common feature: they are both performed over a **moving window**. With convolution, this \"window\" is given by the dimensions of the kernel, the parameter `kernel_size`. With pooling, it is the pooling window, given by `pool_size`.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.imgur.com/LueNK6b.gif\" width=400 alt=\"A 2D moving window.\">\n",
    "</figure>\n",
    "\n",
    "There are two additional parameters affecting both convolution and pooling layers -- these are the `strides` of the window and whether to use `padding` at the image edges. The `strides` parameter says how far the window should move at each step, and the `padding` parameter describes how we handle the pixels at the edges of the input.\n",
    "\n",
    "With these two parameters, defining the two layers becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(filters=64,\n",
    "                  kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  activation='relu'),\n",
    "    layers.MaxPool2D(pool_size=2,\n",
    "                     strides=1,\n",
    "                     padding='same')\n",
    "    # More layers follow\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride #\n",
    "\n",
    "The distance the window moves at each step is called the **stride**. We need to specify the stride in both dimensions of the image: one for moving left to right and one for moving top to bottom. This animation shows `strides=(2, 2)`, a movement of 2 pixels each step.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.imgur.com/Tlptsvt.gif\" width=400 alt=\"Moving window with a stride of (2, 2).\">\n",
    "</figure>\n",
    "\n",
    "What effect does the stride have? Whenever the stride in either direction is greater than 1, the moving window will skip over some of the pixels in the input at each step.\n",
    "\n",
    "Because we want high-quality features to use for classification, convolutional layers will most often have `strides=(1, 1)`. Increasing the stride means that we miss out on potentially valuble information in our summary. Maximum pooling layers, however, will almost always have stride values greater than 1, like `(2, 2)` or `(3, 3)`, but not larger than the window itself.\n",
    "\n",
    "Finally, note that when the value of the `strides` is the same number in both directions, you only need to set that number; for instance, instead of `strides=(2, 2)`, you could use `strides=2` for the parameter setting.\n",
    "\n",
    "# Padding #\n",
    "\n",
    "When performing the moving window computation, there is a question as to what to do at the boundaries of the input. Staying entirely inside the input image means the window will never sit squarely over these boundary pixels like it does for every other pixel in the input. Since we aren't treating all the pixels exactly the same, could there be a problem?\n",
    "\n",
    "What the convolution does with these boundary values is determined by its `padding` parameter. In TensorFlow, you have two choices: either `padding='same'` or `padding='valid'`. There are trade-offs with each.\n",
    "\n",
    "When we set `padding='valid'`, the convolution window will stay entirely inside the input. The drawback is that the output shrinks (loses pixels), and shrinks more for larger kernels. This will limit the number of layers the network can contain, especially when inputs are small in size.\n",
    "\n",
    "The alternative is to use `padding='same'`. The trick here is to **pad** the input with 0's around its borders, using just enough 0's to make the size of the output the *same* as the size of the input. This can have the effect however of diluting the influence of pixels at the borders. The animation below shows a moving window with `'same'` padding.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.imgur.com/RvGM2xb.gif\" width=400 alt=\"Illustration of zero (same) padding.\">\n",
    "</figure>\n",
    "\n",
    "The VGG model we've been looking at uses `same` padding for all of its convolutional layers. Most modern convnets will use some combination of the two. (Another parameter to tune!)\n",
    "\n",
    "# Example - Exploring Moving Windows #\n",
    "\n",
    "Modern convnet architectures have used a variety...\n",
    "\n",
    "We'll look at AlexNet and VGG16 here, and you'll have a chance to explore ResNet50 and LeNet in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we'll just use a single kernel -- a horizontal line detector. This next hidden cell sets everything up and loads the image we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import visiontools\n",
    "import warnings\n",
    "\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings(\"ignore\") # to clean up output cells\n",
    "\n",
    "# Define kernel\n",
    "kernel = visiontools.bottom_sobel\n",
    "\n",
    "# Define input images\n",
    "SIZE = [64, 64]\n",
    "circle = visiontools.circle(SIZE, val=1.0, r_shrink=3)\n",
    "circle = tf.reshape(circle, [*circle.shape, 1])\n",
    "\n",
    "IMAGE_PATH = '/kaggle/input/computer-vision-resources/car_feature.jpg'\n",
    "SIZE = [300, 300]\n",
    "car = visiontools.read_image(IMAGE_PATH)\n",
    "car = tf.image.resize(car, size=SIZE, method='nearest')\n",
    "car = tf.image.convert_image_dtype(car, dtype=tf.float32)\n",
    "\n",
    "# Show kernel, circle, car\n",
    "plt.figure(figsize=(16, 8))\n",
    "gs = gridspec.GridSpec(1, 3, width_ratios=[0.75, 1, 1], wspace=0.2) \n",
    "plt.subplot(gs[0])\n",
    "visiontools.show_kernel(kernel)\n",
    "plt.subplot(gs[1])\n",
    "plt.imshow(tf.squeeze(circle))\n",
    "plt.axis('off')\n",
    "plt.subplot(gs[2])\n",
    "plt.imshow(tf.squeeze(car), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Pooling Size ##\n",
    "\n",
    "We've been using a pooling window with dimensions `(2, 2)`. Let's see what happens when we enlarge it to `(4, 4)`. The stride is the same as before, so now the pooling windows overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visiontools.show_extraction(\n",
    "    car, subplot_shape=(1, 2), figsize=(8, 3),\n",
    "    kernel=kernel,\n",
    "    pool_size=(4, 4),\n",
    "    pool_stride=(2, 2),\n",
    "    ops=['detect', 'condense'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Pooling Stride ##\n",
    "\n",
    "Now let's increase the stride to `(4, 4)` but keep the window to `(2, 2)`. This means the pooling window will skip over 2 pixels from step-to-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visiontools.show_extraction(\n",
    "    circle, subplot_shape=(1, 2), figsize=(8, 3),\n",
    "    kernel=kernel,\n",
    "    pool_size=(2, 2),\n",
    "    pool_stride=(4, 4),\n",
    "    ops=['detect', 'condense'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "In this lesson, we looked at a characteristic computation common to both convolution and pooling: the **moving window** and the parameters affecting its behavior in these layers. This style of windowed computation contributes much of what is characteristic of convolutional networks and is an essential part of their functioning. Move on to the exercise where you'll explore the effect of `strides` and `padding`, and also learn about how *stacking* convolutional layers can increase the effective window size, and also about how convolution can be used with *one-dimensional* data, like time series."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
