{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Convolution and Pooling -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In the last lesson, we learned how the activations in the network represent features and how the weights defined filters. We saw how these features developed as the array of activations flowed through a convolutional block.\n",
    "\n",
    "Convolution and pooling -- implemented in the layers of the network -- are what give a convnet its characteristic structure. Our goal in this lesson is to understand *why* this structure produces features favorable to solving the image classification problem. We want to understand why convolution and pooling are so good at visual feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Windows #\n",
    " \n",
    "Convolution and pooling both make use of a device called a **moving window**. You can think of a moving window as a box that scans over an image -- left to right, top to bottom -- summarizing pixel values along the way.\n",
    "\n",
    "For one-dimensional data (like a sequence or time series), it looks like this:\n",
    "\n",
    "<!--TODO: 1D moving window-->\n",
    "<figure>\n",
    "<img src=\"\" width=400 alt=\"A 1D moving window.\">\n",
    "</figure>\n",
    "\n",
    "For two-dimensional data (like images), it looks like this:\n",
    "\n",
    "<!--TODO: 2D moving window-->\n",
    "<figure>\n",
    "<img src=\"\" width=400 alt=\"A 2D moving window.\">\n",
    "</figure>\n",
    "\n",
    "A moving window is the right sort of device to use for feature extraction because it makes use of the *position* of the pixels. A computation like the mean of all the pixel values would be less informative because it ignores location.\n",
    "\n",
    "(In fact, their use of moving windows is one thing that makes convnets useful on other kinds of ordered data, like time series or natural language texts.)\n",
    "\n",
    "For convolution, the \"window size\" is given by the dimensions of the kernel: a kernel with 3 rows and 3 columns gives a $3 \\times 3$ window. For pooling, this is the dimension of the region it will summarize into a single pixel value: a maximum pooling layer with a pool size of `(2, 2)` will replace the 4 pixels in that window with the pixel with maximum value.\n",
    "\n",
    "The moving window computation gives convolutional layers an advantage over dense layers, which also use positional information. In natural images, information tends to be highly *local*: groups of pixels close together will tend to contain more information than pixels far apart. This is what makes convolutional layers much more efficient than dense layers at feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride ##\n",
    "\n",
    "The distance the window moves at each step is called the **stride**. Operations on two dimensional data will specify a stride in both dimensions: one for moving left to right and one for moving top to bottom.\n",
    "\n",
    "<!--TODO: stride-->\n",
    "<figure>\n",
    "<img src=\"\" width=400 alt=\"Stride.\">\n",
    "</figure>\n",
    "\n",
    "A convolutional layer will usually use a stride of 1 so that the kernel is applied equally across the entire image. This tends to maximize the amount of information retained in the feature and generally gives the best results.\n",
    "\n",
    "<!--TODO: convolution stride-->\n",
    "<figure>\n",
    "<img src=\"\" width=400 alt=\"Convolution and stride.\">\n",
    "</figure>\n",
    "\n",
    "A pooling layer will almost always have a stride greater than 1. This is what enables it to reduce the dimensions of its inputs. When the stride is the same as the window size, the input will be divided into distinct blocks. This maximum pooling with a `(2, 2)` window size and a `(2, 2)` stride.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/4-pooling-blocks.png\" width=\"800\" alt=\"Pooling reduces the dimensions of feature maps.\"> -->\n",
    "<img src=\"https://i.imgur.com/Nd3SlL3.png\" width=\"400\" alt=\"Pooling reduces the dimensions of feature maps.\">\n",
    "</figure>\n",
    "\n",
    "Whenever the stride is greater than 1, the each dimension of the output will be reduced by that factor. For instance, a stride of 2 will reduce each dimension by a factor of 2, for a total reduction in pixels by a factor of 4. You can see in the above figure how pooling reduced a \\(12 \\times 12 \\) image to a \\(6 \\times 6\\) image. This is how the pooling layer is able to condense features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Convolution #\n",
    "\n",
    "Let's see what effect the convolution's kernel size has on the feature extracted. We'll start by defining some kernels of various dimesions. (You'll explore the stride in the exercises.) All of these kernels will be *vertical edge detectors*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visiontools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def vertical_edge(kernel_size):\n",
    "    kernel = np.zeros(kernel_size, dtype=int)\n",
    "    kernel[:, 0] = 1\n",
    "    kernel[:, -1] = -1\n",
    "    return kernel\n",
    "\n",
    "k33 = vertical_edge([3, 3])\n",
    "k55 = vertical_edge([5, 5])\n",
    "k77 = vertical_edge([7, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the kernels and images we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "incorrectly_encoded_metadata": "_kg_hide-input=true _kg_hide-output=false"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from skimage import draw, transform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "\n",
    "def read_image(path, channels=0):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.io.decode_image(image, channels=channels)\n",
    "    return image\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "gs = gridspec.GridSpec(1, 3, width_ratios=[3, 5, 7], wspace=0.2) \n",
    "for i, kernel in enumerate([k33, k55, k77]):\n",
    "    plt.subplot(gs[i])\n",
    "    visiontools.show_kernel(kernel)\n",
    "plt.show();\n",
    "\n",
    "SIZE = [64, 64]\n",
    "circle = tf.reshape(visiontools.circle(SIZE, val=1.0), shape=[*SIZE, 1])\n",
    "\n",
    "car = read_image('/kaggle/input/computer-vision-resources/car_illus.jpg', channels=1)\n",
    "SIZE = [300, 300]\n",
    "car = tf.image.resize(car, size=SIZE, preserve_aspect_ratio=False)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(tf.squeeze(circle))\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(tf.squeeze(car), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll loop over the list of kernels and use the `show_extraction` function (which you might remember from last lesson) to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel in [k33, k55, k77]:\n",
    "    print(\"Kernel shape: {}\".format(kernel.shape))\n",
    "    visiontools.show_extraction(circle, kernel=kernel, subplot_shape=(1, 4), figsize=(12, 4))\n",
    "    visiontools.show_extraction(car, kernel=kernel, subplot_shape=(1, 4), figsize=(12, 4))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see? As the convolutional window grows larger, finer details are lost while larger features are emphasized. It's not uncommon for convolutional networks to start with larger kernels for the first convolutional layer and drop to smaller kernels in additional layers (from $5 \\times 5$ to $3 \\times 3$, say)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Pooling #\n",
    "\n",
    "Now let's take a look at the effect of stride with pooling. (You'll look at window size in the exercises.) We'll use the $3 \\times 3$ kernel from the last example and a standard $2 \\times 2$ pooling window as well. We'll iterate over strides with 1, 2, and 4 steps at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stride in [(1, 1), (2, 2), (4, 4)]:\n",
    "    print(\"Pooling Stride: {}\".format(stride))\n",
    "    visiontools.show_extraction(circle, kernel=k33, \n",
    "                                pool_stride=stride, pool_size=(2, 2),\n",
    "                                subplot_shape=(1, 4), figsize=(12, 4))\n",
    "    visiontools.show_extraction(car, kernel=k33,\n",
    "                                pool_stride=stride, pool_size=(2, 2),\n",
    "                                subplot_shape=(1, 4), figsize=(12, 4))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the difference in how the feature changes from the **Detect** stage to the **Condense** stage. With a $1 \\times 1$ stride, the feature is emphasized, but there is no size reduction -- which is a waste of parameters! With the $4 \\times 4$ kernel, there are \"gaps\" between the pooling windows and too much information seems to be lost. The standard method of setting the stride to be the same as the pooling window seems like the best bet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Invariance #\n",
    "\n",
    "The pooling operation gives a convnet a property called **translation invariance**. This just means that it tends not to distinguish features by their *location* in the image. (\"Translation\" is the mathematical word for changing the position of something without rotating it or changing its shape or size.)\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/.png\" width=\"800\" alt=\".\"> -->\n",
    "<img src=\"\" alt=\"Translated features.\">\n",
    "</figure>\n",
    "\n",
    "A convnet with translation invariance will treat these features the same. Why is that? Watch what happens when we repeatedly apply maximum pooling to the following feature maps.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/4-two-dots.png\" width=\"800\" alt=\"Pooling tends to destroy positional information.\"> -->\n",
    "<img src=\"https://i.imgur.com/97j8WA1.png\" width=\"800\" alt=\"Pooling tends to destroy positional information.\">\n",
    "</figure>\n",
    "\n",
    "The two dots in the original image became indistinguishable after repeated pooling. In other words, pooling destroyed some of their positional information. Since the network can no longer distinguish between them in the feature maps, it can't distinguish them in the original image either: it has become *invariant* to that difference in position.\n",
    "\n",
    "In fact, pooling only creates translation invariance in a network *over small distances*, as with the two dots in the image. Features that begin far apart will remain distinct after pooling; only *some* of the positional information was lost, but not all of it.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/4-two-dots-2.png\" width=\"800\" alt=\"Pooling tends to destroy positional information.\"> -->\n",
    "<img src=\"https://i.imgur.com/kUMWdcP.png\" width=\"800\" alt=\"But only over small distances. Two dots far apart stay separated\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "This invariance to small differences in the positions of features is a nice property for an image classifier to have. Just because of differences in perspective or framing, the same kind of feature might be positioned in various parts of the original image, but we would still like for the classifier to recognize that they are the same. Because this invariance is *built into* the network, we can get away with using much less data for training: we no longer have to teach it to ignore that difference. This gives convolutional networks a big effeciency advantage over a network with only dense layers. (You'll see another way to get invariance for free in **Lesson 8** with **Data Augmentation**!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "As we've seen over the last couple of lessons, when used together convolution and pooling are effective at extracting certain kinds of features from an image. These features, however, are too simple by themselves to be effective for use in the complex kinds of classification problems in demand today -- a network with only a single convolutional block would certainly not do well on our cars dataset! In the next lesson, you'll see how modern neural networks solve this problem by stacking deep layers of convolutional blocks. It is through many repetitions of this feature extraction process that a network can build the most informative features."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
