{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "We saw in the first lesson how to build an image classifier by attaching an untrained head of dense layers to a pretrained convolutional base. Since the pretrained base had already learned how to extract visual features, we could train an accurate image classifier with relatively little data.\n",
    "\n",
    "In this lesson, we'll use the understanding we've gained of the layers in the convolutional base to improve this method even more. The key is in the hierarchy of visual features we learned about in the previous lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Hierarchy of Features #\n",
    "\n",
    "Let's review what we know about the feature extraction in the convolutional base.\n",
    "\n",
    "The base is a sequence of layers that each perform a step in the extraction process. By the time the data has reached the classifier, it has undergone the extraction process many times. Simple features in the shallow layers combine and recombine to form the complex features in the deepest layers.\n",
    "\n",
    "<!--TODO: feature extraction-->\n",
    "<figure>\n",
    "<img src=\"\" width=400 alt=\"Feature extraction.\">\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the network learns which features to extract to solve its classification problem, the features produced by the deepest layers will often closely resemble the training data.\n",
    "\n",
    "The features produced by shallow layers, however, tend to be more generic and will generalize much better to a range of datasets.\n",
    "\n",
    "For instance, suppose we train a convnet on images of dogs. The shallow layers will tend to produce features like lines and textures and contrasts, features useful on almost every problem. The deepest layers will tend to produce features resembling fur and eyes and snouts. It is unlikely these layers would be useful when classifying cars or flowers.\n",
    "\n",
    "<!--TODO: optimal features-->\n",
    "<figure>\n",
    "<img src=\"\" width=400 alt=\"Optimal features.\">\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning #\n",
    "\n",
    "What this means is that when we are using a pretrained base as a feature extractor, it may be best to retrain some of its layers, starting with those closest to the output.\n",
    "\n",
    "This technique is called **fine tuning**. It ensures that the layers most important for the classification have a chance to adapt to the new dataset.\n",
    "\n",
    "In Lesson 1, we created a classifier by using a pretrained base with an untrained head of dense layers. During training, *only* the weights of the dense layers were updated. The the layers in the base were not trained; their weights we kept **frozen**.\n",
    "\n",
    "To fine tune the model, we continue the training by **unfreezing** the layers in the base one-by-one until the model begins to **overfit**.\n",
    "\n",
    "<!--TODO: overfitting-->\n",
    "<figure>\n",
    "<img src=\"\" width=400 alt=\"Overfitting.\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At that point, we know that unfreezing additional layers will be unlikely to improve the accuracy of the model on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Training with Fine Tuning #\n",
    "\n",
    "Let's work through an example to see how it goes. As before, let's assume we've already loaded a dataset with a training split `ds_train` and a validation split `ds_valid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.InteractiveSession(config=config)\n",
    "\n",
    "import visiontools as vt\n",
    "from visiontools import StanfordCars\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = '/home/jovyan/work/kaggle/datasets'\n",
    "\n",
    "(ds_train, ds_valid), ds_info = tfds.load('stanford_cars/simple',\n",
    "                                          split=['train', 'test'],\n",
    "                                          shuffle_files=True,\n",
    "                                          with_info=True,\n",
    "                                          data_dir=DATA_DIR)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "SIZE = [192, 192]\n",
    "preprocess = vt.make_preprocessor(size=SIZE)\n",
    "\n",
    "ds_train_ = (ds_train.map(preprocess)\n",
    "            .cache()\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTO))\n",
    "\n",
    "ds_valid_ = (ds_valid.map(preprocess)\n",
    "            .cache()\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Define Model ##\n",
    "\n",
    "Now create a model just like you did in the first lesson, with a pretrained base and several dense layers as a head to act as the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "pretrained_base = VGG16(include_top=False,\n",
    "                        weights='imagenet',\n",
    "                        input_shape=[192, 192, 3])\n",
    "pretrained_base.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    pretrained_base,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(6, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to set up the training loop a little bit differently than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=160,\n",
    "    decay_rate=0.80,\n",
    "    staircase=True,\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "# Number of epochs per round of training\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Train Classifier Head ##\n",
    "\n",
    "Now, keeping all the layers in the base frozen, train the model until the validation loss no longer improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history = model.fit(ds_train_,\n",
    "                    validation_data=ds_valid_,\n",
    "                    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['accuracy', 'val_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Fine Tune the Base ##\n",
    "\n",
    "Let's look at the layers in the base. We'll print out names and indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, layer in enumerate(pretrained_base.layers):\n",
    "    print(idx, layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tune the convolutional layers in block 5. This gives us indices, from the end backwards, `[17, 16, 15]`.\n",
    "\n",
    "Iterate over top layers, tuning one by one. Since we've set the number of epochs per training round to `EPOCHS=5`, this will give us 5+3*5 = 20 epochs total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, idx in enumerate([17, 16, 15]):\n",
    "    print(\"Unfreezing layer at index {}.\".format(idx))\n",
    "    pretrained_base.layers[idx].trainable = True\n",
    "\n",
    "    # Recompile model after unfreezing a layer.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    # Define epochs\n",
    "    INIT, = history.epoch[-1] + 1, # start after last iteration's end\n",
    "    TOTAL = history.epoch[-1] + 1 + EPOCHS\n",
    "    # Retrain with layer at idx unfrozen.\n",
    "    history = model.fit(\n",
    "        ds_train_,\n",
    "        validation_data=ds_valid_,\n",
    "        initial_epoch=INIT,\n",
    "        epochs=TOTAL,\n",
    "    )\n",
    "    # Concatenate this round's history to previous history\n",
    "    history_frame = pd.concat([history_frame, pd.DataFrame(history.history)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['accuracy', 'val_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "<!--TODO: tut6 conclusion-->"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
