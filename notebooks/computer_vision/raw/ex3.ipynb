{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Status #\n",
    "\n",
    "- [x] Outline\n",
    "- [x] Introduction\n",
    "- [ ] Exercise 1\n",
    "  - [ ] Code\n",
    "  - [x] Discussion\n",
    "  - [ ] Checking\n",
    "- [ ] Exercise 2\n",
    "  - [x] Code\n",
    "  - [x] Discussion\n",
    "  - [ ] Checking\n",
    "- [x] Exercise 3\n",
    "  - [x] Code\n",
    "  - [x] Discussion\n",
    "  - [x] Checking\n",
    "- [ ] Exercise 4\n",
    "  - [ ] Code\n",
    "  - [x] Discussion\n",
    "  - [ ] Checking\n",
    "- [x] Conclusion\n",
    "\n",
    "# Introduction #\n",
    "\n",
    "In these exercises, you'll conclude the feature extraction begun in Exercise 2, explore how invariance is created by maximum pooling, and then look at a different kind of pooling: *average* pooling.\n",
    "\n",
    "Run the cell below to set everything up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.computer_vision.ex3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Apply Pooling to Condense\n",
    "\n",
    "Run this cell to get back to where you left off in the previous lesson. There is a kernel predefined for you, but feel free to change it to the one you were using before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the last step in the sequence. Apply maximum pooling with `tf.nn.pool`. Use the same parameter values that we used in the example from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "image_condense = ____\n",
    "\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "image_condense = tf.nn.pool(\n",
    "    input=image_detect, # image in the Detect step above\n",
    "    window_shape=(2, 2),\n",
    "    pooling_type='MAX',\n",
    "    # we'll see what these do in the next lesson!\n",
    "    strides=(2, 2),\n",
    "    padding='SAME',\n",
    ")\n",
    "q_1.assert_check_passed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution code\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_1.hint()\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_1.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to see what maximum pooling did to the feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    # Reformat for plotting\n",
    "    tf.squeeze(image_detect)\n",
    ")\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Explore Invariance\n",
    "\n",
    "This next code cell will randomly apply a small shift to the circle and then condense the image several times with maximum pooling. Run this cell a few times and make note of the feature produced at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEATS = 4\n",
    "SIZE = [64, 64]\n",
    "\n",
    "# Create a randomly shifted circle\n",
    "image = visiontools.circle(SIZE, r_shrink=4, val=1)\n",
    "image = tf.expand_dims(image, axis=-1)\n",
    "image = visiontools.random_transform(image, jitter=3, fill_method='replicate')\n",
    "image = tf.squeeze(image)\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1, REPEATS+1, 1)\n",
    "plt.imshow(image, vmin=0, vmax=1)\n",
    "plt.title(\"Original\\nShape: {}x{}\".format(image.shape[0], image.shape[1]))\n",
    "plt.axis('off')\n",
    "\n",
    "# Now condense with maximum pooling several times\n",
    "for i in range(REPEATS):\n",
    "    ax = plt.subplot(1, REPEATS+1, i+2)\n",
    "    image = tf.reshape(image, [1, *image.shape, 1])\n",
    "    image = tf.nn.pool(image, window_shape=(2,2), strides=(2, 2), padding='SAME', pooling_type='MAX')\n",
    "    image = tf.squeeze(image)\n",
    "    plt.imshow(image, vmin=0, vmax=1)\n",
    "    plt.title(\"MaxPool {}\\nShape: {}x{}\".format(i+1, image.shape[0], image.shape[1]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were wanting to teach a convolutional classifier to recognize circles, would it be helpful to include all of these \"shifted\" circles in your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution code\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_2.hint()\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_2.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lesson 6, we'll explore this idea of invariance more when we learn about **Data Augmentation**.\n",
    "\n",
    "### 3) What about Average Pooling?\n",
    "\n",
    "In this exercise, we'll look at an alternative to maximum pooling that was once very popular: **average pooling**. An `AvgPool2D` layer operates the same way as `MaxPool2D`, but replaces a patch of pixels by their *average* instead of their maximum value.\n",
    "\n",
    "Run the following cell to see the effect of average pooling to a feature over several iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEATS = 4\n",
    "\n",
    "SIZE = [32, 32]\n",
    "image = visiontools.circle(SIZE)\n",
    "image = tf.reshape(image, [1, *image.shape, 1])\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1, REPEATS+1, 1)\n",
    "plt.imshow(tf.squeeze(image), vmin=0, vmax=1)\n",
    "plt.title(\"Original\\nShape: {}x{}\".format(image.shape[0], image.shape[1]))\n",
    "plt.axis('off')\n",
    "for i in range(REPEATS):\n",
    "    ax = plt.subplot(1, REPEATS+1, i+2)\n",
    "    image = tf.nn.avg_pool(image, ksize=2, strides=2, padding='SAME'),\n",
    "    plt.imshow(tf.squeeze(image), vmin=0, vmax=1)\n",
    "    plt.title(\"MaxPool {}\\nShape: {}x{}\".format(i+1, image.shape[1], image.shape[2]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? All else equal, what do you think would happen if you replaced maximum pooling with average pooling in a convolutional classifier? Would you expect its performance to improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution code\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_3.hint()\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_3.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 4) What about Global Average Pooling?\n",
    "\n",
    "We mentioned in the previous exercise that average pooling has largely been superceeded by maximum pooling within the convolutional base. There is, however, a kind of average pooling that is still widely used in the *head* of a convnet. This is **global average pooling**. A `GlobalAvgPool2D` layer is often used as an alternative to some or all of the hidden `Dense` layers in the head of the network, like so:\n",
    "\n",
    "<!--md-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    pretrained_base,\n",
    "    layers.GlobalAvgPool2D(),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--end_md-->\n",
    "\n",
    "What is this layer doing? Notice that we no longer have the `Flatten` layer that usually comes after the base to transform the 2D feature data to 1D data needed by the classifier. Now the `GlobalAvgPool2D` layer is serving this function. But, instead of \"unstacking\" the feature (like `Flatten`), it simply replaces the entire feature map with its average value. Though very destructive, it can work quite well, and has the advantage of reducing the number of parameters in the model.\n",
    "\n",
    "This next cell will show you a typical set of feature maps produced by the base of VGG16, and what happens when gloval average pooling is applied to them. Run the following cell to see `GlobalAvgPool2D` in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "q_4.a.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, rewrite the model to use a `GlobalAvgPool2D` layer instead of the `Flatten` and `Dense` hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "model = ____\n",
    "\n",
    "q_4.b.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "pretrained_base = \n",
    "\n",
    "model = keras.Sequential([\n",
    "    pretrained_base,\n",
    "    layers.GlobalAvgPool2D(),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "q_4.b.assert_check_passed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution code\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_4.b.hint()\n",
    "#_COMMENT_IF(PROD)_\n",
    "q_4.b.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's train the model. Run the following cell for credit on this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=15,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
    "q_4.c.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global average pooling is used in a number of modern convnets -- <!-- TODO -->, and is a standard option with the predefined models in `tf.keras.applications`. If you're creating a convolutional classifier, it's worth trying out!\n",
    "\n",
    "# Conclusion #\n",
    "\n",
    "In this lesson we explored the final operation in the feature extraction process: **condensing** with **maximum pooling**. Pooling is one of the essential features of convolutional networks and helps provide them with some of their characteristic advantages: efficiency with visual data, reduced parameter size compared to dense networks, translation invariance. We've seen that it's used not only in the base during feature extraction, but also can be used in the head during classification. Understanding it is essential to a full understanding of convnets.\n",
    "\n",
    "In the next lesson, we'll conclude our discussion of the feature extraction operations with **moving windows**, the typical way of describing how the convolution and pooling operations scan over an image. We'll describe here the final two parameters in the `Conv2D` and `MaxPool2D` layers: `strides` and `padding`."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
