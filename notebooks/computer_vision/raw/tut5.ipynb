{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In the last lesson, we saw how the structure of convolution and pooling layers was adapted to solving classification problems with spatial features.\n",
    "\n",
    "In this lesson, we'll look at another consequence of this structure: the **hierarchy of visual features**. We'll explore this hierarchy by looking at the features the network is most sensitive to as we travel deeper into its layers.\n",
    "\n",
    "<!--TODO: feature header-->\n",
    "\n",
    "# The Receptive Field #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Simple to Complex #\n",
    "\n",
    "The consequence of this expanding receptive field is that filters in the shallow layers will be most sensitive to features at the smallest scale, the simplest features.\n",
    "\n",
    "<!--TODO: simple features-->\n",
    "\n",
    "While the filters in the deepest layers will be most sensitive to features at a large scale, the most complex features.\n",
    "\n",
    "<!--TODO: complex features-->\n",
    "\n",
    "If we look at these features from layer to layer, we can see how the deep structure of the network produces features that are more and more complex and refined. By the time an image has reached the classifier, it has undergone the extraction process many times, its simple features being combined and recombined in complex ways.\n",
    "\n",
    "Let's look at these filters as we travel through the layers of the Inception network.\n",
    "\n",
    "<!--TODO: inception features-->\n",
    "\n",
    "# Example - Visualizing Filters #\n",
    "\n",
    "We've included a utility in the `visiontools` module that will compute the features prefered by a given location in the network. You can choose to plot features for a neuron, a filter, an entire layer, or even a list of these together.\n",
    "\n",
    "Let's look at the features recognized by the model we made in Lesson 2. You can look at the layers in a network with the `summary` method. This also tells you how many filters are in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visiontools import visualize_features\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize a layer, just give the name of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize a filter, name the layer its in and give the filter's index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize a particular neuron, give the layer name, the filter index, and the x-y index of the neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
