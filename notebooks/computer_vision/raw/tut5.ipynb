{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--TITLE:Custom Convnets-->\n",
    "# Introduction #\n",
    "\n",
    "Now that you've seen the layers a convnet uses to extract features, it's time to put them together and build a network of your own!\n",
    "\n",
    "# Simple to Complex #\n",
    "\n",
    "More extraction makes better features.\n",
    "\n",
    "# Convolutional Blocks #\n",
    "\n",
    "Arrange them into blocks.\n",
    "\n",
    "# Example - Design a Convnet #\n",
    "\n",
    "## Step 1 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "# Imports\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import visiontools\n",
    "from visiontools import StanfordCars\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Reproducibility\n",
    "def seed_everything(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed = 31415\n",
    "seed_everything(seed)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load training and validation sets\n",
    "DATA_DIR = '/kaggle/input/stanford-cars-for-learn/'\n",
    "(ds_train_, ds_valid_), ds_info = tfds.load(\n",
    "    'stanford_cars/simple',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    "    data_dir=DATA_DIR,\n",
    ")\n",
    "print((\"Loaded {} training examples \" +\n",
    "       \"and {} validation examples \" +\n",
    "       \"with classes {}.\").format(\n",
    "           ds_info.splits['train'].num_examples,\n",
    "           ds_info.splits['test'].num_examples,\n",
    "           ds_info.features['label'].names))\n",
    "\n",
    "# Create data pipeline\n",
    "BATCH_SIZE = 16\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "SIZE = [192, 192]\n",
    "preprocess = visiontools.make_preprocessor(size=SIZE)\n",
    "\n",
    "ds_train = (ds_train_\n",
    "            .map(preprocess)\n",
    "            .cache()\n",
    "            .shuffle(ds_info.splits['train'].num_examples)\n",
    "            .map(augment, AUTO)            \n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTO))\n",
    "\n",
    "ds_valid = (ds_valid_\n",
    "            .map(preprocess)\n",
    "            .cache()\n",
    "            .shuffle(ds_info.splits['test'].num_examples)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "\n",
    "    # First Convolutional Block\n",
    "    layers.Conv2D(filters=64,\n",
    "                  kernel_size=5,\n",
    "                  activation=\"relu\",\n",
    "                  padding='same')\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Second Convolutional Block    \n",
    "    layers.Conv2D(filters=128,\n",
    "                  kernel_size=3,\n",
    "                  activation=\"relu\",\n",
    "                  padding='same')\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Third Convolutional Block    \n",
    "    layers.Conv2D(filters=256,\n",
    "                  kernel_size=3,\n",
    "                  activation=\"relu\",\n",
    "                  padding='same')\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Classifier Head #\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=8,\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(units=1,\n",
    "                 activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(ds_train_,\n",
    "                    validation_data=ds_valid_,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
