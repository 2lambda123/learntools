{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Filters and Features-->\n",
    "# Introduction #\n",
    "\n",
    "In the last lesson, we saw how the structure of convolution and pooling layers was adapted to solving classification problems with spatial features.\n",
    "\n",
    "In this lesson, we'll look at another consequence of this structure: the **hierarchy of visual features**. We'll explore this hierarchy by looking at the features the network is most sensitive to as we travel deeper into its layers.\n",
    "\n",
    "# The Receptive Field #\n",
    "\n",
    "Each neuron in a convnet is connected to the neurons in a previous layer (or to the inputs) according to the \"windows\" we saw in the previous lesson -- either a kernel or a pooling block. Since our windows are usually very small ($3 \\times 3$ or $2 \\times 2$), we might worry that a convnet wouldn't be able to detect \"large\" features that could be important in a classification problem.\n",
    "\n",
    "Fortunately, because these windows \"stack\", the number of pixels a neuron receives inputs from grows the deeper the neuron in the network. The deep stacking of layers creates a widening of the receptive field. The neurons in the last convolutional layers might ultimately be receiving inputs from large parts of the original image.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"5-receptive-field.png\" alt=\"The convolution induces a widening pattern of connections.\" width=400> -->\n",
    "<img src=\"https://i.imgur.com/HmwQm2S.png\" alt=\"The convolution induces a widening pattern of connections.\" width=200>\n",
    "</figure>\n",
    "\n",
    "The consequence of this expanding receptive field is that filters in the shallow layers will be most sensitive to features at the smallest scale, the simplest features. While the filters in the deepest layers will be most sensitive to features at a large scale, the most complex features.\n",
    "\n",
    "If we look at features from layer to layer, we can see how the deep structure of the network produces features that are more and more complex and refined. By the time an image has reached the classifier, it has undergone the extraction process many times, its simple features being combined and recombined in complex ways.\n",
    "\n",
    "An image is worth a thousand words, though -- so let's open up a network and take a look!\n",
    "\n",
    "# Example - Looking Inside Convnets #\n",
    "\n",
    "We saw in Lesson 3 how a network performs the feature extraction.\n",
    "\n",
    "We're going to create a new model essentially by \"chopping off\" some layers. We do this by just rerouting the layer outputs into new model outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2,
    "region_name": "md"
   },
   "source": [
    "```python\n",
    "# New inputs are the same as the original\n",
    "inputs = model.inputs\n",
    "\n",
    "# New outputs are now a layer's outputs\n",
    "layer = model.get_layer('conv1')\n",
    "outputs = layer.outputs\n",
    "\n",
    "# And we now we can create a new model!\n",
    "viz_model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following hidden cell will prepare the image we'll use for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "from visiontools import read_image, show_image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "img = read_image('/kaggle/input/computer-vision-resources/ys.jpg')\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "show_image(img)\n",
    "plt.show()\n",
    "\n",
    "SIZE = [1500, 1500]\n",
    "img = tf.image.resize(img, size=SIZE, method='nearest')\n",
    "img = tf.keras.applications.vgg16.preprocess_input(img, data_format='channels_last')\n",
    "img = tf.image.convert_image_dtype(img, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue looking at the VGG16 model together in this tutorial. In the exercises, you'll look inside two much more powerful models: *ResNet50V2* and *Xception*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = tf.keras.models.load_model(\n",
    "    '/kaggle/input/cv-course-models/cv-course-models/vgg16-pretrained-base',\n",
    ")\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters and Features ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from visiontools import show_feature_maps\n",
    "\n",
    "LAYER_NAME = 'block1_conv1'\n",
    "\n",
    "show_feature_maps(\n",
    "    img,\n",
    "    model=vgg16,\n",
    "    layer_name=LAYER_NAME,\n",
    "    rows=1, cols=6,\n",
    ")\n",
    "show_filters(\n",
    "    model=vgg16,\n",
    "    layer_name=LAYER_NAME,\n",
    "    rows=1, cols=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_NAME = 'block3_conv2'\n",
    "\n",
    "show_feature_maps(\n",
    "    img,\n",
    "    model=vgg16,\n",
    "    layer_name=LAYER_NAME,\n",
    "    rows=1, cols=6,\n",
    ")\n",
    "show_filters(\n",
    "    model=vgg16,\n",
    "    layer_name=LAYER_NAME,\n",
    "    rows=1, cols=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
