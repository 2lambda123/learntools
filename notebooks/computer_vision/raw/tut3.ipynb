{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TITLE: Filter, Detect, Condense -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In the last lesson we saw that a convnet performs feature extraction through a sequence of three operations -- filter, detect, condense. In this lesson, we'll describe these operations in terms of **activations** and **weights** as you learned about in *Introduction to Deep Learning*. This will also be an introduction to **convolution**, **ReLU**, and **pooling**, which we will continue in Lesson 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter #\n",
    "\n",
    "Recall that a **convolutional layer** carries out the filtering step. The **weights** a convnet learns during training are primarily contained in its convolutional layers. These weights we call **kernels**. We can represent them as small arrays:\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-kernel.png\" width=\"600\" alt=\"A 2x2 kernel.\"> -->\n",
    "<img src=\"https://i.imgur.com/Hxboo61.png\" width=\"150\" alt=\"A 2x2 kernel.\">\n",
    "</figure>\n",
    "\n",
    "A convolutional layer will usually contain many kernels -- often hundreds or thousands. They are what determine the kind of filtering that occurs. They do this through the pattern of numbers they contain. You can think about a kernel as a kind of polarized lens, letting through only a certain pattern of information. \n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-kernel-lens.png\" width=\"400\" alt=\"A kernel acts as a kind of lens.\"> -->\n",
    "<img src=\"https://i.imgur.com/j3lk26U.png\" width=\"250\" alt=\"A kernel acts as a kind of lens.\">\n",
    "</figure>\n",
    "\n",
    "The **activations** in the network we call **feature maps**. They are what result when we apply a filter to an image; they are the visual features the network extracts. Here are a few kernels pictured with the feature maps they produced when applied to an image.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-kernels-and-maps.png\" width=\"600\" alt=\"The channels of a color image.\"> -->\n",
    "<img src=\"https://i.imgur.com/JxBwchH.png\" width=\"800\" alt=\"An embossing kernel and the feature map it produces.\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect #\n",
    "\n",
    "After filtering, the feature maps pass through the activation function. The **ReLU activation** has a graph like this:\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-channels-stack.png\" width=\"300\" alt=\"Channels form the depth dimension.\"> -->\n",
    "<img src=\"https://i.imgur.com/3Ud5xhK.png\" width=\"300\" alt=\"Graph of the ReLU activation function.\">\n",
    "</figure>\n",
    "\n",
    "(*ReLU* stands for *Rectified Linear Unit*.)\n",
    "\n",
    "You could think about the activation function as normalizing the pixel values according to some measure of importance. The ReLU function says that negative values are not important and so sets them to 0. (\"Everything unimportant is equally unimportant.\")\n",
    "\n",
    "Like other activation functions used in neural networks, the ReLU function is *nonlinear*. Essentially this means that the total effect of all the layers in the network is different than what we would get by just adding the effects together -- which would be no different than what you would get with a single layer.\n",
    "\n",
    "The ReLU function ensures that only pixels with positive activation remain in the feature map. This is desireable because we don't want any negative activations destroying the features we detect deeper in the network, which is what would happen if we simply added them together.\n",
    "\n",
    "Here is ReLU applied the feature maps above. Notice how it succeeds at isolating the feature of interest.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-relu-and-maps.png\" width=\"800\" alt=\"ReLU applied to feature maps.\"> -->\n",
    "<img src=\"https://i.imgur.com/dKtwzPY.png\" width=\"800\" alt=\"ReLU applied to feature maps.\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condense #\n",
    "\n",
    "Notice that after applying the ReLU function the feature map ends up with a lot of \"dead space,\" that is, large areas containing only 0's (the black areas in the image). Carrying these 0 activations through the entire network would unnecessarily increase the number of parameters. Instead, we would like to **condense** the feature map to retain only the information of interest -- the feature itself.\n",
    "\n",
    "This is what the pooling operation known as **maximum pooling** does. Max pooling will take a block of activations in the original feature map and replace them with the maximum activation in that block.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-max-pooling.png\" width=\"600\" alt=\"Maximum pooling replaces a block with the maximum value in that block.\"> -->\n",
    "<img src=\"https://i.imgur.com/5V5z7lP.png\" width=\"400\" alt=\"Maximum pooling replaces a block with the maximum value in that block.\">\n",
    "</figure>\n",
    "\n",
    "You can see that pooling reduces the dimensions of the image. In this sense, pooling is a kind of downsampling applied to the feature map. When applied after the ReLU activation, it has the effect of \"intensifying\" the feature.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-pool-and-maps.png\" width=\"800\" alt=\"Maximum pooling replaces a block with the maximum value in that block.\"> -->\n",
    "<img src=\"https://i.imgur.com/rl0Lejy.png\" width=\"800\" alt=\"Maximum pooling applied to feature maps.\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Apply Operations #\n",
    "\n",
    "Let's apply these operations to an image to get a feel for what they do.\n",
    "\n",
    "Here is the image we'll use for this example:\n",
    "<!-- #endregion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "\n",
    "img_path = '/kaggle/input/computer-vision-resources/car_feature.jpg'\n",
    "img = tf.io.read_file(img_path)\n",
    "img = tf.io.decode_jpeg(img)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(img), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by creating a simple model that performs the feature extraction. Then we'll pull out the layers so that we can apply them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "# Create Layers\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(filters=1,\n",
    "                  kernel_size=3,\n",
    "                  padding='same',\n",
    "                  use_bias=False,\n",
    "                  input_shape=img.shape),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2,\n",
    "                     padding='same'),\n",
    "])\n",
    "\n",
    "conv2d, relu, maxpool2d = model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is essentially the convolutional block you learned about in Lesson 2.\n",
    "\n",
    "Now for the filtering step, we'll define a kernel and then apply it with the convolution. The kernel in this case is an \"edge detection\" kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visiontools\n",
    "\n",
    "krn = tf.constant([\n",
    "    [-1, -1, -1],\n",
    "    [-1,  8, -1],\n",
    "    [-1, -1, -1],\n",
    "])\n",
    "\n",
    "visiontools.show_kernel(krn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that a kernel is a set of weights for a convolutional layer. We'll give the `conv2d` layer we've created these weights with its `set_weights` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat for batch compatibility.\n",
    "image = tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
    "image = tf.expand_dims(image, axis=0)\n",
    "kernel = tf.reshape(krn, [*krn.shape, 1, 1])\n",
    "\n",
    "# Apply the kernel to the layer. Since a conv layer can have many\n",
    "# kernels, we pass it in a list.\n",
    "conv2d.set_weights([kernel])\n",
    "\n",
    "# You can call the layer on the image just like a function.\n",
    "image_filter = conv2d(image)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image_filter))\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the detection step with the ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_detect = relu(image_filter)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image_detect))\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `relu` and `maxpool2d` layers have no trainable weights, so we can simply pass in the image.\n",
    "\n",
    "And last is condensing with maximum pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_condense = maxpool2d(image_detect)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image_condense))\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the whole process start-to-finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visiontools.show_extraction(img, krn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "In this tutorial, we saw how **kernels** represent the weights in a convnet, while **feature maps** represent activations. We saw how a convolutional network can engineer the complex features it needs to solve a classification problem through the application of convolution, ReLU, and pooling. In the exercises, you'll explore the operations more on your own!\n",
    "<!-- #endregion -->"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
