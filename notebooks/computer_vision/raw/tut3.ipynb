{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--TITLE: Maximum Pooling-->\n",
    "# Introduction #\n",
    "\n",
    "In Lesson 2 we began our discussion of how the base in a convnet performs feature extraction. We learned about how the first two operations in this process occur in a `Conv2D` layer with `relu` activation.\n",
    "\n",
    "In this lesson, we'll look at the third (and final) operation in this sequence: **condense** with **maximum pooling**.\n",
    "\n",
    "# Condense with Maximum Pooling #\n",
    "\n",
    "The condensing step occurs also occurs in a layer, a maximum pooling layer. Adding condensing step to the model we had before, will give us this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(filters=64, kernel_size=3), # activation is None\n",
    "    layers.MaxPool2D(pool_size=2),\n",
    "    # More layers follow\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at the extraction process.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/2-show-extraction.png\" width=\"1200\" alt=\"An example of the feature extraction process.\"> -->\n",
    "<img src=\"https://i.imgur.com/EZ2lipV.png\" width=\"1200\" alt=\"An example of the feature extraction process.\">\n",
    "</figure>\n",
    "\n",
    "Notice that after applying the ReLU function (**Detect**) the feature map ends up with a lot of \"dead space,\" that is, large areas containing only 0's (the black areas in the image). Carrying these 0 activations through the entire network would unnecessarily increase the number of parameters. Instead, we would like to *condense* the feature map to retain only the information of interest -- the feature itself.\n",
    "\n",
    "This in fact is what **maximum pooling** does. Max pooling takes a block of activations in the original feature map and replace them with the maximum activation in that block.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-max-pooling.png\" width=\"600\" alt=\"Maximum pooling replaces a block with the maximum value in that block.\"> -->\n",
    "<img src=\"https://imgur.com/hK5U2cd.png\" width=\"400\" alt=\"Maximum pooling replaces a block with the maximum value in that block.\">\n",
    "</figure>\n",
    "\n",
    "When applied after the ReLU activation, it has the effect of \"intensifying\" the feature. The pooling step increased the proportion of active pixels to zero pixels.\n",
    "\n",
    "# Example - Apply Maximum Pooling #\n",
    "\n",
    "Let's add the \"condense\" step to the feature extraction we did in the example in Lesson 2. This next hidden cell will take us back to where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.InteractiveSession(config=config)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import visiontools\n",
    "\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "\n",
    "# Read image\n",
    "# image_path = '/kaggle/input/computer-vision-resources/car_feature.jpg'\n",
    "image_path = '/home/jovyan/work/kaggle/computer-vision/images/car_feature.jpg'\n",
    "image = tf.io.read_file(image_path)\n",
    "image = tf.io.decode_jpeg(image)\n",
    "\n",
    "# Define kernel\n",
    "kernel = tf.constant([\n",
    "    [-1, -1, -1],\n",
    "    [-1,  8, -1],\n",
    "    [-1, -1, -1],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "# Reformat for batch compatibility.\n",
    "image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "image = tf.expand_dims(image, axis=0)\n",
    "kernel = tf.reshape(kernel, [*kernel.shape, 1, 1])\n",
    "\n",
    "# Filter step\n",
    "image_filter = tf.nn.conv2d(\n",
    "    input=image,\n",
    "    filters=kernel,\n",
    "    # we'll talk about these two in the next lesson!\n",
    "    strides=1,\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "# Detect step\n",
    "image_detect = tf.nn.relu(image_filter)\n",
    "\n",
    "# Show what we have so far\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(131)\n",
    "plt.imshow(tf.squeeze(image), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Input')\n",
    "plt.subplot(132)\n",
    "plt.imshow(tf.squeeze(image_filter))\n",
    "plt.axis('off')\n",
    "plt.title('Filter')\n",
    "plt.subplot(133)\n",
    "plt.imshow(tf.squeeze(image_detect))\n",
    "plt.axis('off')\n",
    "plt.title('Detect')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use another one of the functions in `tf.nn` to apply the pooling step, `tf.nn.pool`. This is a Python function that does the same thing as the `MaxPool2D` layer you use when model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "image_condense = tf.nn.pool(\n",
    "    input=image_detect, # image in the Detect step above\n",
    "    window_shape=(2, 2),\n",
    "    pooling_type='MAX',\n",
    "    # we'll see what these do in the next lesson!\n",
    "    strides=(2, 2),\n",
    "    padding='SAME',\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image_condense))\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool! Hopefully you can see how the pooling step was able to intensify the feature by removing the amount of \"dead space\" around the active pixels.\n",
    "\n",
    "**TODO** discussion\n",
    "\n",
    "# Translation Invariance #\n",
    "\n",
    "The pooling operation gives a convnet a property called **translation invariance**. This just means that it tends not to distinguish features by their *location* in the image. (\"Translation\" is the mathematical word for changing the position of something without rotating it or changing its shape or size.)\n",
    "\n",
    "Watch what happens when we repeatedly apply maximum pooling to the following feature map.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/4-two-dots.png\" width=\"800\" alt=\"Pooling tends to destroy positional information.\"> -->\n",
    "<img src=\"https://i.imgur.com/97j8WA1.png\" width=\"800\" alt=\"Pooling tends to destroy positional information.\">\n",
    "</figure>\n",
    "\n",
    "The two dots in the original image became indistinguishable after repeated pooling. In other words, pooling destroyed some of their positional information. Since the network can no longer distinguish between them in the feature maps, it can't distinguish them in the original image either: it has become *invariant* to that difference in position.\n",
    "\n",
    "In fact, pooling only creates translation invariance in a network *over small distances*, as with the two dots in the image. Features that begin far apart will remain distinct after pooling; only *some* of the positional information was lost, but not all of it.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/4-two-dots-2.png\" width=\"800\" alt=\"Pooling tends to destroy positional information.\"> -->\n",
    "<img src=\"https://i.imgur.com/kUMWdcP.png\" width=\"800\" alt=\"But only over small distances. Two dots far apart stay separated\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This invariance to small differences in the positions of features is a nice property for an image classifier to have. Just because of differences in perspective or framing, the same kind of feature might be positioned in various parts of the original image, but we would still like for the classifier to recognize that they are the same. Because this invariance is *built into* the network, we can get away with using much less data for training: we no longer have to teach it to ignore that difference. This gives convolutional networks a big effeciency advantage over a network with only dense layers. (You'll see another way to get invariance for free in **Lesson 6** with **Data Augmentation**!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "**TODO**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
