{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In the last lesson we saw that a convnet performs feature extraction through a sequence of three operations -- filter, detect, condense. In this lesson, we'll describe these operations in terms of **activations** and **weights** as you learned about in *Introduction to Deep Learning*. This will also be an introduction to **convolution**, **ReLU**, and **pooling**, which we will continue in Lesson 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter #\n",
    "\n",
    "Recall that a **convolutional layer** will typically carry out the filtering step. The **weights** a network learns during training are primarily contained in its convolutional layers. We can represent them as small arrays called **kernels**.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-emboss.png\" width=\"600\" alt=\"The channels of a color image.\"> -->\n",
    "<img src=\"https://i.imgur.com/Hxboo61\" width=\"150\" alt=\"A 2x2 kernel.\">\n",
    "</figure>\n",
    "\n",
    "A convolutional layer will usually contain many kernels. They are what determine the kind of filtering that occurs.\n",
    "\n",
    "The pattern of numbers in the array defines the effect of a kernel. You can think about a kernel as a kind of polarized lens, letting through only a certain pattern of information. The kernel above will filter for vertical lines. This kernel gives an \"embossing\" effect:\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-emboss.png\" width=\"600\" alt=\"The channels of a color image.\"> -->\n",
    "<img src=\"https://i.imgur.com/SFNtWkb.png\" width=\"400\" alt=\"An embossing kernel and the feature map it produces.\">\n",
    "</figure>\n",
    "\n",
    "The **activations** in the network we call **feature maps**. They are what result when we apply a filter to an image; they are the visual features the network extracts.\n",
    "\n",
    "The first feature maps are the color channels of the image. A grayscale image has one channel, the gray value. A color image will have three channels: red, green, and blue.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-channels-rgb.png\" width=\"600\" alt=\"The channels of a color image.\"> -->\n",
    "<img src=\"https://i.imgur.com/JUhUdgz.png\" width=\"600\" alt=\"The channels of a color image.\">\n",
    "</figure>\n",
    "\n",
    "As more extraction operations are applied, the feature maps become increasingly refined.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-simple-to-complex.png\" width=\"800\" alt=\"Feature maps, simple to complex.\"> -->\n",
    "<img src=\"https://i.imgur.com/VqmC1rm.png\" width=\"600\" alt=\"Feature maps, simple to complex.\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features: The Depth Dimension ##\n",
    "\n",
    "*(This section makes some terminology we'll be using a bit more precise. It's not essential to anything that follows, so feel free to skip it if you want. The important thing is just to understand that a convolutional layer contains many kernels.)*\n",
    "\n",
    "When an image first enters a network, it exists as a set of channels. We usually think about the channels as being the *depth* dimension, with the width and height as the two spatial dimensions. In the language of TensorFlow, an image is a tensor with shape `[height, width, channels]`.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-channels-stack.png\" width=\"300\" alt=\"Channels form the depth dimension.\"> -->\n",
    "<img src=\"https://i.imgur.com/JiUjn7o.png\" width=\"300\" alt=\"Channels form the depth dimension.\">\n",
    "</figure>\n",
    "\n",
    "A convolutional layer will apply a kernel to each channel in the input, and the collection of these kernels is what we call a **filter**. One filter produces one feature map.\n",
    "\n",
    "<!--TODO: filter to feature map-->\n",
    "\n",
    "A convolutional layer may produce many feature maps. So, if a convolutional layer producing 16 feature maps is applied to an image with 3 channels, it will contain 16*3=48 kernels.\n",
    "\n",
    "As mentioned, the channels are the first set of feature maps. More generally then, then depth dimension of the activation tensors contains the feature maps: `[height, width, features]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect #\n",
    "\n",
    "After filtering, the feature maps pass through the activation function. The **ReLU activation** has a graph like this:\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-channels-stack.png\" width=\"300\" alt=\"Channels form the depth dimension.\"> -->\n",
    "<img src=\"https://i.imgur.com/3Ud5xhK.png\" width=\"300\" alt=\"Graph of the ReLU activation function.\">\n",
    "</figure>\n",
    "\n",
    "(*ReLU* stands for *Rectified Linear Unit*.)\n",
    "\n",
    "You could think about the activation function as normalizing the pixel values according to some measure of importance. The ReLU function says that negative values are not important and so sets them to 0. (\"Everything unimportant is equally unimportant.\")\n",
    "\n",
    "Like other activation functions used in neural networks, the ReLU function is *nonlinear*. Essentially this means that the total effect of all the layers in the network is different than what we would get by just adding the effects together, becoming, in effect, a network with one layer only.\n",
    "\n",
    "The ReLU function ensures that only pixels with positive activation remain in the feature map. This is desireable because we don't want any negative activations destroying the features we detect deeper in the network, which is what would happen if we simply added them together.\n",
    "\n",
    "Here is ReLU applied to some feature maps. Notice how it succeeds at isolating the feature of interest.\n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/3-relu-applied.png\" width=\"800\" alt=\"ReLU applied to feature maps.\"><!--TODO: ReLU applied to feature maps-->\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condense #\n",
    "\n",
    "Notice that after applying the ReLU function the feature map ends up with a lot of \"dead space,\" that is, large areas containing only 0's (the black areas in the image). Carrying these 0 activations through the entire network would unnecessarily increase the number of parameters. Instead, we would like to **condense** the feature map to retain only the information of interest -- the feature itself.\n",
    "\n",
    "This is what the pooling operation known as **maximum pooling** does. Max pooling will take a block of activations in the original feature map and replace them with the maximum activation in that block.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-max-pooling.png\" width=\"600\" alt=\"Maximum pooling replaces a block with the maximum value in that block.\"> -->\n",
    "<img src=\"https://i.imgur.com/5V5z7lP.png\" width=\"600\" alt=\"Maximum pooling replaces a block with the maximum value in that block.\">\n",
    "</figure>\n",
    "\n",
    "You can see that pooling reduces the dimensions of the image. In this sense, pooling is a kind of downsampling applied to the feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Apply Operations #\n",
    "\n",
    "Let's apply these operations to an image to get a feel for what they do.\n",
    "\n",
    "Here is the image we'll use for this example:\n",
    "<!-- #endregion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "img_path = './images/car_feature.jpg'\n",
    "img = tf.io.read_file(img_path)\n",
    "img = tf.io.decode_jpeg(img)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(img), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by creating a simple model that performs the feature extraction. Then we'll pull out the layers so that we can apply them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "# Create Layers\n",
    "model = Sequential([\n",
    "    layers.Conv2D(filters=1,\n",
    "                  kernel_size=3,\n",
    "                  padding='same',\n",
    "                  use_bias=False,\n",
    "                  input_shape=img.shape),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2,\n",
    "                     padding='same'),\n",
    "])\n",
    "\n",
    "conv2d, relu, maxpool2d = model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is essentially the convolutional block you learned about in Lesson 2.\n",
    "\n",
    "Now for the filtering step, we'll define a kernel and then apply it with the convolution. The kernel in this case is an \"edge detection\" kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visiontools\n",
    "\n",
    "krn = tf.constant([\n",
    "    [-1, -1, -1],\n",
    "    [-1,  8, -1],\n",
    "    [-1, -1, -1],\n",
    "])\n",
    "\n",
    "visiontools.show_kernel(krn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that a kernel is a set of weights for a convolutional layer. We'll give the `conv2d` layer we've created these weights with its `set_weights` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat for batch compatibility.\n",
    "image = tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
    "image = tf.expand_dims(image, axis=0)\n",
    "kernel = tf.reshape(krn, [*krn.shape, 1, 1])\n",
    "\n",
    "# Apply the kernel to the layer. Since a conv layer can have many\n",
    "# kernels, we pass it in a list.\n",
    "conv2d.set_weights([kernel])\n",
    "\n",
    "# You can call the layer on the image just like a function.\n",
    "image_filter = conv2d(image)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image_filter))\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the detection step with the ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_detect = relu(image_filter)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image_detect))\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `relu` and `maxpool2d` layers have no trainable weights, so we can simply pass in the image.\n",
    "\n",
    "And last is condensing with maximum pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_condense = maxpool2d(image_detect)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image_condense))\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the whole process start-to-finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visiontools.show_extraction(img, krn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Visualize Activations #\n",
    "\n",
    "It can be instructive to look at the activations an image produces in a network throughout its layers. We've included a function the `visiontools` module that will plot them for you. Let's look at some activations in the network from Lesson 2 (using the same image as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('custom_convnet_512.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the image we'll run through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.io.read_file('images/car_0.jpg')\n",
    "image = tf.io.decode_image(image, channels=3)\n",
    "\n",
    "SIZE = [512, 512]\n",
    "image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "image = tf.image.resize(image, size=SIZE, method='nearest')\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following displays the first few feature maps within each convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from visiontools import show_feature_maps\n",
    "\n",
    "layer_names = [layer.name for layer in model.layers\n",
    "               if layer.__class__.__name__ is 'Conv2D']\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    show_feature_maps(image, model, layer_name, rows=2, cols=4, width=16, gamma=0.25)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the features extracted become more and more refined as the activations flow deeper into the network. You'll explore some other models in the exercises!\n",
    "\n",
    "# Conclusion #\n",
    "\n",
    "In this tutorial, we saw how **kernels** represent the weights in a convnet, while **feature maps** represent activations. We saw how a convolutional network can engineer the complex features it needs to solve a classification problem through a repeated application of convolution, ReLU, and pooling. In the exercises, you'll explore the operations more on your own."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
