{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Categorical Encodings\n",
    "\n",
    "In this exercise you'll be applying more advanced encodings to the categorical variables. The goal is to encode the categorical variables in a way that provides more information for the classifier model. The encodings you will implement are:\n",
    "\n",
    "- Count Encoding\n",
    "- Target Encoding\n",
    "- Leave-one-out Encoding\n",
    "- CatBoost Encoding\n",
    "- Feature embedding with SVD \n",
    "\n",
    "After each encoding, you'll refit the classifier and check its performance on hold-out data. First, run the next cell to repeat the work you did in the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Ignoring repeated attempt to bind to globals\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, metrics\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Set up code checking\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.machine_learning.ex2 import *\n",
    "\n",
    "# Create features from timestamps\n",
    "click_data = pd.read_csv('../input/train_sample.csv', parse_dates=['click_time'])\n",
    "click_times = click_data['click_time']\n",
    "clicks = click_data.assign(day=click_times.dt.day.astype('uint8'),\n",
    "                           hour=click_times.dt.hour.astype('uint8'),\n",
    "                           minute=click_times.dt.minute.astype('uint8'),\n",
    "                           second=click_times.dt.second.astype('uint8'))\n",
    "\n",
    "# Label encoding for categorical features\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "for feature in cat_features:\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    clicks[feature] = label_encoder.fit_transform(clicks[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll define a couple functions to help test the new encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "    \"\"\" Splits a dataframe into train, validation, and test sets. First, orders by \n",
    "        the column 'click_time'. Set the size of the validation and test sets with\n",
    "        the valid_fraction keyword argument.\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe = dataframe.sort_values('click_time')\n",
    "    valid_rows = int(len(dataframe) * valid_fraction)\n",
    "    train = dataframe[:-valid_rows * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_rows * 2:-valid_rows]\n",
    "    test = dataframe[-valid_rows:]\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "def train_model(train, valid, test, feature_cols=None):\n",
    "    if feature_cols is None:\n",
    "        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n",
    "                                           'is_attributed'])\n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "    dtest = lgb.Dataset(test[feature_cols], label=test['is_attributed'])\n",
    "    \n",
    "    param = {'num_leaves': 64, 'objective': 'binary', \n",
    "             'metric': 'auc', 'seed': 7}\n",
    "    num_round = 1000\n",
    "    print(\"Training model!\")\n",
    "    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=20, verbose_eval=False)\n",
    "    \n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n",
    "    \n",
    "    test_pred = bst.predict(test[feature_cols])\n",
    "    test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n",
    "    print(f\"Validation AUC score: {valid_score}\")\n",
    "    \n",
    "    return bst, valid_score, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to get a baseline score. If your encodings do better than this, you can keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model\n",
      "Training model!\n",
      "Validation AUC score: 0.9622743228943659\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline model\")\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "_ = train_model(train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Count encodings\n",
    "\n",
    "Here, encode the categorical features `['ip', 'app', 'device', 'os', 'channel']` using the count of each value in the data set. Then, retrain the model to measure the score with the new encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountEncoder:\n",
    "    def __init__(self):\n",
    "        self.mapping = {}\n",
    "        \n",
    "    def fit(self, df):\n",
    "        for col in df.columns:\n",
    "            self.mapping[col] = df.groupby(col).count().iloc[:, 0]\n",
    "        \n",
    "    def transform(self, df):\n",
    "        out_df = df.copy()\n",
    "        for col, encoding in self.mapping.items():\n",
    "            out_df[col] = df[col].map(self.mapping[col]).fillna(0)\n",
    "        \n",
    "        return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the count encoder\n",
    "count_enc = ____\n",
    "\n",
    "# Learn encoding from the training set\n",
    "____\n",
    "\n",
    "# Apply encoding to each set and train a new model\n",
    "# Add encoded features as new \n",
    "for each in (train, valid, test):\n",
    "    ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the count encoder\n",
    "count_enc = CountEncoder()\n",
    "\n",
    "# Learn encoding from the training set\n",
    "count_enc.fit(train[cat_features])\n",
    "\n",
    "# Apply encoding to each set and train a new model\n",
    "for each in (train, valid, test):\n",
    "    encoded = count_enc.transform(each[cat_features])\n",
    "    for col in encoded:\n",
    "        each.insert(0, col + '_count', encoded[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9653049481211093\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the encoded datasets\n",
    "_ = train_model(train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, count encoding improved our model's score. Now we can add it to the whole dataset and try more encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = count_enc.transform(clicks[cat_features])\n",
    "for col in encoded:\n",
    "    clicks.insert(len(clicks.columns), col + '_count', encoded[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Target encoding\n",
    "\n",
    "Now you'll try some supervised encodings that use the labels (the targets) to transform categorical features. The first one is target encoding. Create the target encoder from the `category_encoders` library. Then, learn the encodings from the training dataset, apply the encodings to all the datasets and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the target encoder\n",
    "tenc = ____\n",
    "\n",
    "# Learn encoding from the training set\n",
    "____\n",
    "\n",
    "# Apply encoding to each set and train a new model\n",
    "for each in (train, valid, test):\n",
    "    ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "cat_features = ['app', 'device', 'os', 'channel']\n",
    "tenc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "tenc.fit(train[cat_features], train['is_attributed'])\n",
    "\n",
    "for each in (train, valid, test):\n",
    "    encoded = tenc.transform(each[cat_features])\n",
    "    for col in encoded:\n",
    "        each.insert(0, col + '_target', encoded[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9650694152351313\n"
     ]
    }
   ],
   "source": [
    "_ = train_model(train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Remove IP encoding\n",
    "\n",
    "Try leaving `ip` out of the encoded features and retrain the model with target encoding again. You should find that the score increases and is above the baseline score! Why do you think the score is below baseline when we encode the IP address but above baseline when we don't?\n",
    "\n",
    "**Answer:** (This is my guess) Target encoding attempts to measure the population mean of the target for each level in a categorical feature. This means when there is less data per level, the estimated mean will be further, there will be more variance. There is little data per IP address so it's likely that the estimates are much noisier than for the other features. Going forward, we'll leave out the IP feature when trying different encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Leave-One-Out Encoding\n",
    "\n",
    "Try leave-one-out encoding which might work better since it leaves out some data that can reduce overfitting. Again, create the leave-one-out encoder, fit it on the training dataset, and apply the encodings to all the datasets. Then, retrain the model to see if the new encodings improve the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the leave-one-out encoder. Use random_state=7.\n",
    "loo_enc = ____\n",
    "\n",
    "# Learn encoding from the training set\n",
    "____\n",
    "\n",
    "# Apply encoding to each set and train a new model\n",
    "for each in (train, valid, test):\n",
    "    ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "cat_features = ['app', 'device', 'os', 'channel']\n",
    "loo_enc = ce.LeaveOneOutEncoder(cols=cat_features, sigma=1, random_state=7)\n",
    "\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "loo_enc.fit(train[cat_features], train['is_attributed'])\n",
    "\n",
    "for each in (train, valid, test):\n",
    "    encoded = loo_enc.transform(each[cat_features])\n",
    "    for col in encoded:\n",
    "        each.insert(0, col + '_loo', encoded[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9650772281075612\n"
     ]
    }
   ],
   "source": [
    "_ = train_model(train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) CatBoost Encoding\n",
    "\n",
    "The CatBoost encoder is supposed to working well with the LightGBM model. Encode the categorical features with `CatBoostEncoder` and train the model on the encoded data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the CatBoost encoder\n",
    "cb_enc = ____\n",
    "\n",
    "# Learn encoding from the training set\n",
    "____\n",
    "\n",
    "# Apply encoding to each set and train a new model\n",
    "for each in (train, valid, test):\n",
    "    ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "cat_features = ['app', 'device', 'os', 'channel']\n",
    "cb_enc = ce.CatBoostEncoder(cols=cat_features, random_state=7)\n",
    "\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "# Learn encodings on the train set\n",
    "cb_enc.fit(train[cat_features], train['is_attributed'])\n",
    "\n",
    "# Apply encodings to each set\n",
    "for each in [train, valid, test]:\n",
    "    encoded = cb_enc.transform(each[cat_features])\n",
    "    for col in encoded:\n",
    "        each.insert(0, col + '_cb', encoded[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9651637372380955\n"
     ]
    }
   ],
   "source": [
    "_ = train_model(train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = cb_enc.transform(clicks[cat_features])\n",
    "for col in encoded:\n",
    "    clicks.insert(len(clicks.columns), col + '_cb', encoded[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical feature embeddings with SVD\n",
    "\n",
    "Now you'll create embeddings from pairs of columns using SVD to learn from a count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Learn SVD components as embeddings.\n",
    "\n",
    "Here you'll use SVD to learn embeddings for the categorical features from a matrix of counts. First, create the SVF transformer. Then for each pair of features, create a count matrix and learn the SVD components. Remember you should be learning the embeddings from the train datatset to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = get_data_splits(clicks)\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "\n",
    "# Create the SVD transformer\n",
    "svd = ____\n",
    "\n",
    "# Learn SVD feature vectors and store in svd_components as DataFrames\n",
    "svd_components = {}\n",
    "for col1, col2 in itertools.permutations(cat_features, 2):\n",
    "    ____\n",
    "    svd_components['_'.join([col2, col1])] = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Learn SVD feature vectors\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "svd_components = {}\n",
    "svd = TruncatedSVD(n_components=5)\n",
    "# Loop through each pair of categorical features\n",
    "for col1, col2 in itertools.permutations(cat_features, 2):\n",
    "    # For a pair, create a sparse matrix with cooccurence counts\n",
    "    pair_counts = train.groupby([col1, col2])['is_attributed'].count()\n",
    "    pair_matrix = pair_counts.unstack(fill_value=0)\n",
    "    \n",
    "    # Fit the SVD and store the components\n",
    "    # Note: these components represent column 2\n",
    "    svd.fit(pair_matrix)\n",
    "    svd_components['_'.join([col2, col1])] = pd.DataFrame(svd.components_.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Encode categorical features with SVD components\n",
    "\n",
    "With the components learned from the train dataset, encode the categorical features and add the new features to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply encodings to each set\n",
    "for each in [train, valid, test]:\n",
    "    ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "svd_encodings = pd.DataFrame(index=clicks.index)\n",
    "for feature in svd_components:\n",
    "    # Get the feature column the SVD components are encoding\n",
    "    col = feature.split('_')[0]\n",
    "\n",
    "    # Use SVD components to encode the categorical features\n",
    "    comp_cols = svd_components[feature].reindex(clicks[col]).set_index(clicks.index)\n",
    "    \n",
    "    # Add encoded features to the DataFrame\n",
    "    for component, values in comp_cols.T.iterrows():\n",
    "        svd_encodings.insert(len(svd_encodings.columns), feature + \"_svd_\" + str(component), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values and add to the dataframe\n",
    "svd_encodings = svd_encodings.fillna(svd_encodings.mean())\n",
    "clicks = clicks.join(svd_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9651369718983184\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = get_data_splits(clicks)\n",
    "_ = train_model(train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, you'll start generating completely new features from the data itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
