{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "# Activation Functions #\n",
    "\n",
    "There is a whole family of variants of `relu`: `elu`, `gelu`, `selu`, `swish`, all of which you can use in Keras. On some datasets, models seem to perform better with one activation more than another. The ReLU function is a good one to start with, but you could experiment with the others as you develop your models.\n",
    "\n",
    "<!-- note box -->\n",
    "There is a second family of activation functions which we might call the \"S-shaped\" fuctions. While the ReLU functions are only bounded at the negative end, these functions are bounded at both ends. In the hidden layers, these haven't been as successful as the ReLU type. The **sigmoid** function, however, is often used in the head of a classifier network to convert real-valued numbers into probabilities. We'll see the sigmoid function again in Lesson 6 when we learn about classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Sequential Models #\n",
    "\n",
    "### 2a) Linear Model\n",
    "\n",
    "### 2b) Deep Model\n",
    "\n",
    "# Representation Learning #\n",
    "\n",
    "### 3) Fitting a Quadratic\n",
    "\n",
    "Run this cell to see how the layers shape the input to match the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_3.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways to adapt a linear model so that it can learn non-linear relationships. One way is to transform features directly -- squaring a feature to get a quadratic relationship, for instance.\n",
    "\n",
    "What these methods have in common is that they *transform* the data before applying the linear regression. By and large, these classical methods are limited to a single relatively simple transformation. Deep neural networks, however, pass the data through multiple transformations -- sometimes hundreds! This stack of layers is what makes deep learning *deep*.\n",
    "\n",
    "# The Weight Matrix #\n",
    "\n",
    "# Conclusion #\n",
    "\n",
    "# Keep Going #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py,md,ipynb",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
