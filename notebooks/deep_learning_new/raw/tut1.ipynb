{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Welcome to Deep Learning! #\n",
    "\n",
    "- do deep learning for **regression** and **classification**\n",
    "- design **neural network architectures**\n",
    "- navigate the **loss landscape**\n",
    "- master **stochastic gradient descent**\n",
    "- solve real world problems\n",
    "\n",
    "You'll be prepared for deep learning if you've taken our *Introduction to Machine Learning* course.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "# A New Kind of Model #\n",
    "\n",
    "Deep learning is fundamentally about neural networks. But it's also a new way of building and training machine learning models.\n",
    "\n",
    "In addition to a dataset, building and training a deep learning model means deciding on three things:\n",
    "1. A **model architecture** built of layers of data transformations\n",
    "2. A **loss function** that defines the solution, and\n",
    "3. A method of **optimization** that fits the model to the data\n",
    "\n",
    "With classical machine learning models, these three things are usually bundled together in a single algorithm. In `scikit-learn`, for instance, you define a linear model with a single function, like `model = LinearRegression()`.\n",
    "\n",
    "With the classical approach, you choose from a library of distinct algorithms. With the deep learning approach, instead of choosing from a library of predefined models, you design the model yourself.\n",
    "\n",
    "These three things -- architecture, loss, and optimization -- are the three central ideas of this course. Each of our future lessons will explore some aspect of these three ideas, how they interact, and how your decisions about them will ultimately decide the success of your project. We hope to develop strong intuitions about what's actually happening when you train a neural network so that you can intelligently diagnose problems and quickly iterate towards a successful solution.\n",
    "\n",
    "As an introduction, we'll implement a linear regression model in our new framework. Then, we'll apply our model to the [Red Wine Quality](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009) dataset.\n",
    "\n",
    "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n",
    "    <strong>Keras and TensorFlow</strong><br>\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD\">Tensorflow</a> is a large and robust machine learning platform.\n",
    "<a href=\"https://keras.io/\">Keras</a> is TensorFlow's deep-learning API.\n",
    "</blockquote>\n",
    "\n",
    "# The Linear Model #\n",
    "\n",
    "In our *Introduction to Machine Learning* course you predicted the price of a home using features like its number of bedrooms or the size of its lot. When we use data to predict a continuous quantity like this we are solving a **regression** problem. (With *classification* we'd be trying to predict some unordered set of class labels.)\n",
    "\n",
    "## Architecture ##\n",
    "\n",
    "In linear regression we model the target as a linear function of the features. So, the features become the inputs $x$ and the target becomes the output $y$ of some function like:\n",
    "$y = W x + b$.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.png\" width=\"600\" alt=\"Graphs illustrating linear regression models.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center><strong>Left: </strong>With one input feature, linear regression fits a line. <strong>Right: </strong>With two inputs, it fits a plane.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "The variables $W$ and $b$ are the parameters we determine when we fit the model to the training data. $W$ is a matrix we call the **weights** and $b$ is a vector we call the **bias**.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.png\" width=\"600\" alt=\"Illustration of weights as slope and bias as y-intercept..\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>The weights determine the slope and the bias determines the vertical intercept. <strong>Left:</strong>Without weights. <strong>Right:</strong> Without bias.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "For our first decision then, we choose a linear architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Build a model by stacking layers inside of Sequential\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer in a Keras model represents some kind of data transformation. The `Dense` layer represents a transformation like $W x + b$ -- exactly what we want for linear regression. The `units` are how many outputs you want the layer to produce. In a regression problem, every input example outputs just a single value, so we choose `units=1`.\n",
    "\n",
    "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n",
    "    <strong>Example: House Prices</strong><br>\n",
    "In the <a href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\">House Prices: Advanced Regression Techniques</a> competition, each home has a set of features, like its size in square feet or its number of bedrooms. From this set of features, you are trying to predict a single value, its selling price. A simple linear regression model might look like:\n",
    "\n",
    "<code>\n",
    "Price = w_0 * LotArea + w_1 * Bedroom + b\n",
    "</code>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss ##\n",
    "\n",
    "The second thing we need to choose is a *loss function*. The \"loss\" is simply a measure of how well the model fits the training data. Using the optimizer, Keras will try to choose model parameters that make the loss as small as possible.\n",
    "\n",
    "The `LinearRegression` model in scikit-learn minimizes **mean squared error (MSE)**. Mathematically, this is `(y_true - y_pred) ** 2`.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.png\" width=\"600\" alt=\"Graph illustrating MSE.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>The MSE function. At $x=0$, <code>y_true == y_pred</code>.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Keras includes a number of loss functions in its `keras.losses` module. *Mean absolute error*, for instance, would also be a fine choice.\n",
    "\n",
    "Training a model means *optimizing* the loss function, that is, making the loss smaller. We define the loss at the same time as the optimization method, so let's take a look at that now.\n",
    "\n",
    "## The Optimizer ##\n",
    "\n",
    "**Stochastic gradient descent (SGD)** is the method of optimization univerally used in practice with deep learning models. To minimize the loss, SGD will go through a process of *iterative refinement*:\n",
    "1. take a random sample from the training data (a **minibatch**)\n",
    "2. measure the loss on that sample\n",
    "3. adjust the weights and biases in way that makes the loss smaller\n",
    "\n",
    "One round of this is called a **step**, while one run through the entire dataset is an **epoch**. It's not uncommon to train deep learning models for hundreds or even thousands of epochs.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.imgur.com/TTxs4y2.mp4\" width=\"600\" alt=\"A linear regression model iteratively trained using SGD.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>A linear regression model iteratively trainined using SGD. The fit improves batch by batch.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Ordinary least squares regression, for instance, finds its line of best fit by solving a certain matrix equation using the entire dataset at once. \n",
    "\n",
    "Stochastic gradient descent actually comprises a whole family of algorithms. They differ primarily in their strategy of updating the model weights (step 3 above). We'll consider some of them in future lessons, but for now our choice of optimizer will be the one called simply `'SGD'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the loss and optimizer with the model's compile method\n",
    "model.compile(\n",
    "    optimizer='SGD',\n",
    "    loss='MSE',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a string for the arguments like this will use the defaults for the loss and optimizer, which generally work well. In later lessons we'll configure the optimizer a bit by instead using an object from the [`keras.optimizers` module](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD). (There's nothing to configure for MSE.)\n",
    "\n",
    "You define the number of training epochs and the size of the minibatches in the `fit` method, which trains the model. Let's fit it on some fake data just to see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.normal(0, 1, 256)\n",
    "err = np.random.normal(0, 0.2, 256)\n",
    "y = 2*x + 1 + err\n",
    "\n",
    "model.fit(x=x, y=y, batch_size=16, epochs=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x, y, alpha=0.25)\n",
    "plt.plot(x, model.predict(x), color='r')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Red Wine Quality #\n",
    "\n",
    "Now that we know how to create linear regression models in Keras, let's see it in action on an actual dataset. We'll use the *Red Wine Quality* dataset. \n",
    "\n",
    "Our goal in this example will be to predict the quality of a wine (on a scale of 3-8) given some of its chemical properties (numeric measurements). As we'll discuss later, neural networks perform best when your data is put on a common scale -- we will rescale each feature into the interval $[0, 1]$. (Check out our [course on Pandas](https://www.kaggle.com/learn/pandas) for a review of working with dataframes!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "red_wine = pd.read_csv('../input/dl-course-data/dl-course-data/red-wine.csv')\n",
    "display(red_wine.head())\n",
    "\n",
    "# Create training and validation splits\n",
    "df_train = red_wine.sample(frac=0.7, random_state=0)\n",
    "df_valid = red_wine.drop(df_train.index)\n",
    "\n",
    "# Scale to [0, 1]\n",
    "max_ = df_train.max(axis=0)\n",
    "min_ = df_train.min(axis=0)\n",
    "df_train = (df_train - min_) / (max_ - min_)\n",
    "df_valid = (df_valid - min_) / (max_ - min_)\n",
    "\n",
    "# Split features and target\n",
    "x_train = df_train.copy()\n",
    "y_train = x_train.pop('quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how we define and train the linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_OUTPUT$\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(1),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='SGD',\n",
    "    loss='MSE',\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=x_train, y=y_train,\n",
    "    batch_size=16,\n",
    "    epochs=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've hidden the output here since, with 200 epochs, it's rather long. If you'd like to take a look just click the **Output** button to the upper right of the cell.\n",
    "\n",
    "Often, a better way to view the loss is to plot it. The `fit` method in fact keeps a record of the loss produced during training in a `History` object. We'll convert the data to a Pandas dataframe, which makes the plotting easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the training history to a dataframe\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# use Pandas native plot method\n",
    "history_df['loss'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the loss levels off as the epochs go by. When the loss curve becomes horizontal like that, it means the model has learned all it can and there would be no reason to train for additional epochs.\n",
    "\n",
    "# Conclusion #\n",
    "\n",
    "Deep learning, as it's practiced in Keras and other libraries, is more like a general framework that you can use to solve a great variety of machine learning problems. This deep learning framework is very powerful and very flexible. It's flexibility, however, means that there are more choices you have to make, and the quality of your model will ultimately depend on how well you understand those choices.\n",
    "\n",
    "In this first lesson, we introduced the deep learning framework by implementing a linear regression model. We made appropriate choices for the **architecture**, **loss**, and **optimizer**. These three things are the start of every deep learning model.\n",
    "\n",
    "In the remainder of this course, we'll investigate these three components more deeply. In Lesson 2, we go beyond linear models by stacking layers into **neural networks**. Lessons 3 and 5 build skill with **stochastic gradient descent**, the universal deep learning optimizer. We learn methods of developing models to control overfitting in Lesson 4. In Lesson 6 we solve a **binary classification** problem by using a new kind of loss function.\n",
    "\n",
    "By the end of this course, you should have a strong grasp of the fundamentals of deep learning. You'll know how to design deep learning models to solve practical problems and feel confident in the choices you make while developing your projects.\n",
    "\n",
    "# Your Turn #\n",
    "\n",
    "During this course, you'll develop a deep learning model to *solve a real-world problem*. Move on to the first exercise to get started!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
