{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Deep Learning! #\n",
    "\n",
    "This course has everything you need to get started with deep learning in Keras. You'll learn how to:\n",
    "- design neural networks to perform **regression** and **classification**\n",
    "- effectively train a network with **stochastic gradient descent**\n",
    "- tune a model with **early stopping** and **dropout**\n",
    "- improve training behavior with **Adam** and **batch normalization**\n",
    "\n",
    "# What Makes Deep Learning Different? #\n",
    "- layers of simple transformations\n",
    "- optimization through SGD\n",
    "\n",
    "In this lesson, we're going to introduce a new way of building and training machine-learning models. We typically conceive of classical machine learning models as performing a single transformation, one transformation direct from input to output. Moreover, the classical model is typically trained on the entire dataset at once. This is true of almost all the models in `scikit-learn`, for instance.\n",
    "\n",
    "To illustrate this new way of doing machine learning, we're going to reconceive linear regression as a neural network. As we'll see, the solution we arrive at is essentially identical to that produced by classical methods. This new framework, however, is much more powerful and much more flexible, and starting in Lesson 2, we'll see how we can build off of our simple linear regression model to produce sophisticated deep learning networks that can be trained on very large data sets.\n",
    "\n",
    "# Linear Regression #\n",
    "\n",
    "So let's review linear regression. In linear regression we model the target as a linear function of the features. So, the features become the inputs $x$ and the target becomes the output $y$ of some function like:\n",
    "\\[y = W x + b\\]\n",
    "\n",
    "When there is a single feature, you could think of linear regression as fitting a line through the $(x, y)$ data points. When there are multiple features, it will fit a plane or hyperplane. (Let's just say \"line\" for now.)\n",
    "\n",
    "<!-- fitting a line and plane -->\n",
    "\n",
    "In the equation above, the variable $W$ represents the **weights**. The weights tell you how much the output is changing for each input. $W$ is a vector with one weight for each feature. The other variable $b$ is called the **bias**. The bias defines what the output $y$ should be when the input is 0. It gives a vertical shift to the regression line.\n",
    "\n",
    "<!-- effect of weights and bias -->\n",
    "\n",
    "As you can see, both the weights and the bias are needed to find a well-fitting line. (We'll talk more about what \"well-fitting\" means in a moment.)\n",
    "\n",
    "# The Linear Layer #\n",
    "\n",
    "In Keras, we build models in **layers**. A layer essentially is just something that takes some inputs, does a computation on them, and produces some outputs. In Keras, layers are implemented so as to be easy to combine. Typically, you won't need to keep track of how many inputs or outputs a layer has. Most of that is taken care of for you.\n",
    "\n",
    "The most general kind of layer is the `Dense` layer. A `Dense` layer, in fact, is just a layer that does a computation like $y = W x + b$. Training a `Dense` layer means to find values for $W$ and $b$ that fit the inputs to the outputs. So a `Dense` layer is essentially calculating a linear regression.\n",
    "\n",
    "Here is how we could define a linear regression model in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([ # this creates a model that we can put a stack of layers in\n",
    "    layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument given to the `Dense` layer defines how many outputs it has. In this course, we'll just do regression on a single output variable, but if you were doing multivariate regression, you could have more.\n",
    "\n",
    "# Optimization #\n",
    "\n",
    "A well-fitting line in linear regression is one that minimizes the distance between itself and the data points. Most commonly, the line is chosen to minimize **mean-squared error** (MSE): for every input $x$, take the difference between its $y$-value and the $y$-value on the line, square each difference, and then sum them all together. This is called *ordinary least-squares* (OLS).\n",
    "\n",
    "An **optimization** problem is a problem of finding parameter values for a function that will produce a minimum or a maximum. Ordinary least squares is the problem of choosing a $W$ and $b$ that will minimize MSE for the given data set. Most commonly, the OLS problem is solved by applying certain matrix transformations on the training data, transforming the entire dataset at once. This method is guaranteed to find the actual optimal solution, the guaranteed \"best\" values for $W$ and $b$.\n",
    "\n",
    "Methods for finding guaranteed optimal parameter values only exist for models that are relatively simple. Deep learning models can be extremely complex and so there's no hope of finding a simple formula that could produce a solution. Instead, we train the model a little bit at a time on small samples of the dataset, attempting to drive down the error a little at a time. These samples we call **minibatches** (or sometimes just \"batches\"), and the full optimiation process **stochastic gradient descent** (SGD). We'll look at the details of SGD in Lesson 3 and how you can tune it to get good results.\n",
    "\n",
    "<!-- animation of sgd -->\n",
    "\n",
    "You set up SGD in Keras by \"compiling\" your model with a loss function and an optimizer. Taking the model we defined before, we would then do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"sgd\",\n",
    "    loss=\"mse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells Keras we want to minimize mean-squared error using stochastic gradient descent. \n",
    "\n",
    "Usually, you'll run through the dataset multiple times. Each complete run of the dataset is called an **epoch**. To train the model for 20 epochs on batches of 64 examples at a time, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X, y=y, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- note -->\n",
    "<strong>Linear Transformation?</strong>\n",
    "If you've had some linear algebra, you might know that a linear transformation is what you get when you multiply something by a matrix. Geometrically, this means some combination of reflections, rotations, and constant stretches or shrinks. Technically, the addition of the bias makes the transformation *affine*, but since (it turns out) an affine transformation is just a linear transformation one dimension higher, it will be okay if we just stick with \"linear\".\n",
    "\n",
    "# Example - Linear Regression in Keras #\n",
    "\n",
    "Now let's carry this out on an actual dataset.\n",
    "\n",
    "First let's load the data. We've hidden the cell since the details aren't important for this example, but feel free to take a look if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the loss and the optimizer, you can also include **metrics**. These are additional functions run at the same time as the loss, but that don't affect the training. We might be interested in the mean-absolute error as well, so let's include that as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='SGD',\n",
    "    loss'mse',\n",
    "    metrics=['mae'],\n",
    ")\n",
    "\n",
    "history = model.fit(x=x, y=y, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method produces a record of the loss and metrics produced during training. It's nice to save this to produce some plots afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# convert the training history to a dataframe\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['mae', 'val_mae']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "In this tutorial, we introduced the basic framework of deep learning. We learned that deep learning uses models built with layers and iteratively trained with SGD. All of the developments in this course will be essentially in either of these two things: how we compose the layers, or how we perform the optimization.\n",
    "\n",
    "# Your Turn #\n",
    "\n",
    "During this course, you'll develop a deep learning model to *solve a real-world problem*. Move on to the first exercise to get started!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
