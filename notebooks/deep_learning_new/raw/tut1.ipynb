{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Welcome to Deep Learning! #\n",
    "\n",
    "- do deep learning for **regression** and **classification**\n",
    "- design **neural network architectures**\n",
    "- navigate the **loss landscape**\n",
    "- master **stochastic gradient descent**\n",
    "- solve real world problems\n",
    "\n",
    "You'll be prepared for deep learning if you've taken our *Introduction to Machine Learning* course.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "# A One Neuron Neural Network #\n",
    "\n",
    "As an introduction to the fundamental ideas behind deep learning, we're going to build a neural \"network\" with just *one* neuron. With the flexibility of the deep-learning framework, we'll see that we can push even this one neuron beyond what many classical models can do. In Lesson 2 we'll start building full networks of hundreds or thousands of neurons!\n",
    "\n",
    "There will be three parts to our model:\n",
    "1. a linear neural unit, the model **architecture**\n",
    "2. a mean squared error **loss function**, and\n",
    "3. the SGD **optimizer**\n",
    "\n",
    "We'll look briefly at each of these and see what they contribute.\n",
    "\n",
    "# The Linear Unit #\n",
    "\n",
    "A single neuron with one input looks like:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/xxS8rzf.png\" width=\"250\" alt=\"Diagram of a linear unit.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>The Linear Unit\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "When reading this diagram think about the computation as flowing from left to right. The numbers on the connections we call **weights** and the values that flow from input to output we call **activations**. Notice that this neuron has a constant input of 1 attached; its connection has a special weight called the **bias**. This neuron has two weights, `w` and `b`.\n",
    "\n",
    "The rule is that whenever an activation flows through a connection, you multiply it by the weight, and to get the output of the unit you just sum up all of the inputs. So, this unit computes a function like $y = w x + b$, or in Python `output = w * input + b`.\n",
    "\n",
    "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n",
    "    <strong>The Linear Unit Makes a Line</strong><br>\n",
    "Does the formula $y=w x + b$ look familiar? It's an equation of a line! It's the slope-intercept equation, where $w$ is the slope and $b$ is the y-intercept. That's why we call it the <em>linear</em> unit.\n",
    "</blockquote>\n",
    "\n",
    "## Example ##\n",
    "\n",
    "Say the weights on our neuron happened to be `w=3` and `b=2`. What would we get if we plug in `x=-4`?\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.png\" width=\"300\" alt=\"Diagram of neural computation.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Computing with the linear unit.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Which checks with our formula: $y = 3(-4) + 2 = -10$.\n",
    "\n",
    "(By the way, running all of your training data through a network like this is sometimes called doing the *forward pass*.)\n",
    "\n",
    "# Training the Network #\n",
    "\n",
    "When we first create a neuron, the weights are set randomly. Our goal is to use the training data to find values for the weights that will create the \"best fitting\" line to the data. We want, in other words, to find the right value for the slope and the y-intercept.\n",
    "\n",
    "To find the right values for the weights, all neural networks use a procedure that goes more or less like this: Run some training data through the network to make predictions. Measure the difference between the predictions and the true values. Then, adjust the weights in a direction that makes the difference smaller. Do this over and over until you're satisfied.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/rFI1tIk.gif\" width=\"1200\" alt=\"Fitting a line batch by batch. The loss decreases and the weights approach their true values.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Training a neural network with Stochastic Gradient Descent.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "You can see from this animation how the training goes. When adjusting the weights, don't use the entire dataset at once, just a sample from it called a **minibatch**. To measure the difference between prediction and truth, we're using the MSE function, called the **loss function**. If everything goes to plan, the loss goes down as we feed in more minibatches, and the weights approach their true values.\n",
    "\n",
    "# Example - Red Wine Quality #\n",
    "\n",
    "Our goal in this example will be to predict the perceived quality of a wine (on a scale of 3-8) given its *residual sugar* content, which is the amount of grape sugar remaining after fermentation. High levels of residual sugar make a wine *sweet* while low levels make it *dry*. The data is from the *Red Wine Quality* dataset.\n",
    "\n",
    "As we'll discuss later, neural networks perform best when your data is put on a common scale -- we will rescale each feature into the interval $[0, 1]$. (Check out our [course on Pandas](https://www.kaggle.com/learn/pandas) for a review of working with dataframes!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE$\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "red_wine = pd.read_csv('../input/dl-course-data/dl-course-data/red-wine.csv')\n",
    "\n",
    "# Create training and validation splits\n",
    "df_train = red_wine.sample(frac=0.7, random_state=0)\n",
    "\n",
    "# Scale to [0, 1]\n",
    "max_ = df_train.max(axis=0)\n",
    "min_ = df_train.min(axis=0)\n",
    "df_train = (df_train - min_) / (max_ - min_)\n",
    "\n",
    "# Split features and target\n",
    "x_train = df_train['residual sugar']\n",
    "y_train = df_train['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, you can create a model with a single linear unit using what's called a `Dense` layer. Most neural networks are built by stacking layers of neurons that connect in a particular way, which we'll learn about in Lesson 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a network with 1 linear unit\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we first need to choose a loss function and an optimizer. We'll learn more about these in Lesson 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the optimizer and loss function\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='mse',\n",
    ")\n",
    "\n",
    "# Fit the network to the training data\n",
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=256,\n",
    "    epochs=50,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Keras' fitting method returns a record of the training in a `History` object. One of the goals of this course is to show you how you can use this loss history to guide your model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Plot the loss history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df['loss'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our single linear unit was able to fit this training data very well, attaining an MSE loss less than **TODO**. There are a couple objections we might make though. The first is that this is the MSE loss on the *training* data. We don't know necessarily that it will perform this well on data it hasn't seen before. You might recall that the right way to check how well the model generalizes is by using a [*validation set*](https://www.kaggle.com/dansbecker/model-validation). You can actually include this in the `fit` method with a `validation_data` argument, and then you'll get two sets of loss curves, one for loss on the training set and one for loss on the validation set. (We'll start doing this from now on.)\n",
    "\n",
    "The second objection is that MSE might not be the right kind of loss for this data. The target, remember, is a set of ranks from 3 to 8. Is being 2 ranks off really 4 times as bad as being 1 rank off? It's a matter of interpretation, but it could have practical consequences if you were trying to use this model in the real-world. Fortunately, it's easy to swap in a different loss function to suit your circumstance. Keras includes 'MAE', 'MSLE', 'Huber', and many more -- or you could even define your own.\n",
    "\n",
    "# Conclusion #\n",
    "\n",
    "This lesson introduced you to the basic ideas behind deep learning. We learned about the parts of a neuron and how they compute a linear function. We learned that you can use stochastic gradient descent to train a network against your choice of a loss function. The remainder of this course is just about developing everything you've just seen in this tutorial.\n",
    "\n",
    "# Your Turn #\n",
    "\n",
    "Now [move on]() to the Exercise, where you'll **TODO**."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
