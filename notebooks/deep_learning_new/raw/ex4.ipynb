{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "# Early Stopping #\n",
    "\n",
    "# Dropout #\n",
    "\n",
    "# Learning Rate Schedules (???) #\n",
    "\n",
    "# Weight Regularization (???) #\n",
    "- l1/l2/l1_l2 :: Lasso/Ridge/Elastic\n",
    "\n",
    "# The Dropout Trick #\n",
    "\n",
    "We mentioned in the tutorial that dropout creates an ensemble network during training. It turns out that you can also use a model with dropout to create an ensemble during *inference*. This is a neat trick that can sometimes boost your model's performance a bit. Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development #\n",
    "\n",
    "# Conclusion #\n",
    "\n",
    "How you construct your neural network determines the *hypothesis space*. You could think of regularization as weighting the hypothesis space towards a certain kind of solution. Early stopping favors solutions with the most \"obvious\" representations -- the features in the data the optimizer is most likely to discover first. Dropout favors solutions with robust representations.\n",
    "\n",
    "There are many ways of regularizing a model.\n",
    "\n",
    "You've learned about two of the most effective, but it's fun to learn about more!\n",
    "\n",
    "# Keep Going #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
