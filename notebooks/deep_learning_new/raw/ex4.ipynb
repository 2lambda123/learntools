{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction #\n",
    "\n",
    "In these exercises we'll explore some ways of improving training outcomes.\n",
    "\n",
    "First load the *Spotify* dataset. Your task will be to predict the popularity of a song based on various audio features.\n",
    "\n",
    "# Load Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScalar, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "spotify = pd.read_csv('../input/dl-course-data/spotify.csv')\n",
    "\n",
    "X = spotify.copy().dropna()\n",
    "y = X.pop('track_popularity')\n",
    "artists = X['track_artist']\n",
    "\n",
    "features_num = ['danceability', 'energy', 'key', 'loudness', 'mode',\n",
    "                'speechiness', 'acousticness', 'instrumentalness',\n",
    "                'liveness', 'valence', 'tempo', 'duration_ms']\n",
    "features_cat = ['playlist_genre']\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(), features_cat),\n",
    ")\n",
    "\n",
    "def group_split(X, y, group, train_size=0.75):\n",
    "    splitter = GroupShuffleSplit(train_size=train_size)\n",
    "    train, test = next(splitter.split(X, y, groups=group))\n",
    "    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = group_split(X, y, artists)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_valid = preprocessor.transform(X_valid)\n",
    "y_train = y_train / 100\n",
    "y_valid = y_valid / 100\n",
    "\n",
    "input_shape = X_train.shape[1]\n",
    "print(\"Input shape: [{}]\".format(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capacity #\n",
    "\n",
    "You'll do training and observe the curves.\n",
    "\n",
    "Let's say you were starting your model development with a simple linear network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(1, input_shape=input_shape),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about these learning curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Add capacity\n",
    "\n",
    "You suspect this model is underfitting the training data, so you want to add capacity. You add one hidden layer with 128 units and ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation=\"relu\", input_shape=input_shape),\n",
    "    layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about these learning curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Add more capacity\n",
    "\n",
    "Could the model still be underfitting? Try adding two more hidden layers to the network. Your model should have three hidden layers of 128 units and ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(128, activation='relu'),    \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about these learning curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping #\n",
    "\n",
    "Let's take the last model and add an early stopping callback.\n",
    "\n",
    "### 2a) Define a callback\n",
    "\n",
    "Define an early stopping callback with <mark>TODO</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(patience=5, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Interpret results\n",
    "\n",
    "Now train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it solve the overfitting problem? Do you think the parameters of the callback could be improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOUGHT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Schedules #\n",
    "\n",
    "Let's make one final improvement to this model. We're going to define what's called a **learning rate schedule**. In Keras we can do this with a callback, just like with early stopping.\n",
    "\n",
    "### 3a) Define a learning rate schedule\n",
    "\n",
    "You can often get lower loss by decreasing the learning rate during training. Let's define a learning rate scheduler and rerun the model from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(8, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the schedule using a \"callback\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    callbacks=[lr_schedule],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we see an improvement?\n",
    "\n",
    "# Conclusion #\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py,md,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
