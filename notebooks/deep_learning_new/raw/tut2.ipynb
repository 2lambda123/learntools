{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TITLE: Making Networks Deep -->\n",
    "\n",
    "- [ ] Introduction\n",
    "- [ ] Illustration: A dense layer\n",
    "- [ ] Illustration: ReLU graph\n",
    "- [ ] Illustration: Network with ReLU\n",
    "- [ ] Illustration: Fully connected network\n",
    "- [ ] Conclusion\n",
    "\n",
    "# Introduction #\n",
    "\n",
    "In this lesson we're going to expand on the one-neuron architecture we saw in Lesson 1. We're going to see how we can build neural networks capable of learning the complex kinds of relationships needed for advanced applications.\n",
    "\n",
    "# Dense Layers #\n",
    "\n",
    "Neural networks typically organize their neurons into **layers**. When we collect linear units together, we get a **dense** layer.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/EmHaEOK.png\" width=\"400\" alt=\"A stack of three circles in an input layer connected to two circles in a dense layer.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>A dense layer of two linear units receiving two inputs and a bias.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Notice here that each unit has a full set of input connections. This is the defining feature of dense layers, and why you sometimes hear of networks with only dense layers as being **fully connected**.\n",
    "\n",
    "To make a linear model that produces three output values, we would just change to `units` argument to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units=2, input_shape=[2])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, you could think of a layer in Keras as some kind of *data transformation*. To create the kinds of complicated transformations they need, neural networks will often create very deep stacks of layers.\n",
    "\n",
    "It turns out, however, that a deep stack of dense layers with nothing in between can't do anything more than just a single dense layer can. Dense layers can never move us out of the world of lines and planes. What we need is something *nonlinear*. What we need are activation functions.\n",
    "\n",
    "# The Activation Function #\n",
    "\n",
    "An **activation function** is simply some function we apply to each of a layer's outputs. The most common is the **Rectified Linear Unit** or **ReLU** function.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.png\" width=\"400\" alt=\"A graph of the ReLU. The line y=x when x>0 and y=0 when x<0, making a 'hinge' shape.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>The ReLU activation.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "The ReLU function has a graph that's a line with the negative part \"rectified\" to zero: `max(0, x)`. As you can see, applying this ReLU function to the outputs of a neuron will put a *bend* in the data, moving us away from simple lines. In the network diagram, it might look like this:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.png\" width=\"400\" alt=\"A network diagram with the ReLU activation added. We draw a small circle containing the hinge graph immediately before the output.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Adding the ReLU activation.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "## Adding an Activation Function ##\n",
    "\n",
    "You can include an activation function in its own layer.\n",
    "```\n",
    "layers.Dense(units=3),\n",
    "layers.Activation('relu')\n",
    "```\n",
    "\n",
    "Or you can add it as part of the definition of another layer.\n",
    "```\n",
    "layers.Dense(units=3, activation='relu')\n",
    "```\n",
    "\n",
    "These two ways of adding activations are completely equivalent.\n",
    "\n",
    "# Stacking Dense Layers #\n",
    "\n",
    "Now that we have some nonlinearity, let's see how we can stack layers to get complex data transformations.\n",
    "\n",
    "As we've mentioned, what defines a dense layer is a full set of connections to its inputs. So when we stack on a dense layer, it will \"fully connect\" all of its neurons to whatever came before -- it could be input data or it could be other neurons.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/x4Cqrh6.png\" width=\"600\" alt=\"An input layer, two hidden layers, and an output.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>A stack of dense layers makes a \"fully-connected\" network.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "The layers before the output layer are sometimes called **hidden**, since we never see their outputs directly. Though we haven't shown them in this diagram, each of these neurons would also be receiving a bias.\n",
    "\n",
    "The `Sequential` model we've been using will connect together a list of layers sequentially from first to last: the first layer gets the input, the last layer produces the output. This will create the model in the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=4, activation='relu'),\n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The body and head serve different functions in the network. You should think about the hidden layers as learning a data transformation that will make it easy for the final layer to produce the correct outputs. The hidden layers learn how to do *feature engineering*, while the final layer learns how to fit a curve to the outputs.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/OLSUEYT.png\" width=\"400\" alt=\" \">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>The hidden layers can learn to use the ReLU function to put \"bends\" in the data. This allows the network to learn non-linear relationships like this one, a quadratic.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "# Conclusion #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
