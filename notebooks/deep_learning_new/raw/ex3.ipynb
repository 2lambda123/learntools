{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for a Neural Network #\n",
    "\n",
    "The data we'll use in this course will be *structured* data, or more specifically, *tabular* data, the kind you'd find in CSV files and Pandas DataFrames. We won't get into the details of data preparation in this course, but let's outline the important points. Take a look at the hidden cell if you'd like to see how it's done.\n",
    "\n",
    "Neural nets need numeric inputs and produce numeric outputs and generally perform best when all the features are all on a common scale near 0. This means you'll need to encode any non-numeric features and scale any numeric features. For numerics, [standardization](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) and [min-max scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) to $[0, 1]$ can both be good choices. For categorical features with a moderate number of categories, [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) is a good choice. The [preprocessing module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) in scikit-learn has almost everything you might need for preparing tabular data for neural networks.\n",
    "\n",
    "<mark><strong>TODO - add resources on Kaggle</strong>\n",
    "[Data Cleaning](https://www.kaggle.com/learn/data-cleaning)\n",
    "[Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)\n",
    "</mark>\n",
    "\n",
    "Let's walk through preprocessing.\n",
    "\n",
    "### 1a) Load and Process Dataset\n",
    "\n",
    "In the *Fuel Economy* dataset your task is to predict the fuel economy of an automobile given features like its type of engine or the year it was made.\n",
    "\n",
    "First let's load the *Fuel Economy* dataset. Our target is the `FE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fuel = pd.read_csv('../input/dl-course-data/fuel.csv')\n",
    "display(fuel.head())\n",
    "display(fuel.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features with `object` type are categorical, which we will one-hot encode. The numeric features we'll standardize. It's not as essential that the target be transformed, though doing so can significantly speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = fuel.copy()\n",
    "y = X.pop('FE')\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(),\n",
    "     make_column_selector(dtype_include=np.number)),\n",
    "    (OneHotEncoder(sparse=False),\n",
    "     make_column_selector(dtype_include=object)),\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = \\\n",
    "    train_test_split(X, y, train_size=0.75)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_valid = preprocessor.transform(X_valid)\n",
    "y_train = np.log(y_train) # log transform target instead of standardizing\n",
    "y_valid = np.log(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now our data is ready for the network!\n",
    "\n",
    "### 1b) Input Shape\n",
    "\n",
    "What will be the value of `input_shape` in the first layer of the network? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Think about whether you should look at the processed data `X` or the original data `fuel`.\n",
    "# Hint 2: You should look at the processed data `X`, since that is the data actually going into the network.\n",
    "input_shape = [50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuel Economy Prediction #\n",
    "\n",
    "### 2a) Define Model\n",
    "\n",
    "Define a model with three hidden layers, each having 64 units and a ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(64, activation='relu'),    \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Add Loss and Optimizer\n",
    "\n",
    "Now, using the `compile` method, add the Adam optimizer and MAE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "____\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) Train Model\n",
    "\n",
    "Now train the network for 100 epochs with a batch size of 128. The input data is `X_train` and the target is `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d) Evaluate Training\n",
    "\n",
    "Finally, run the cell below to a plot of the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[10:, ['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained the model longer, would you expect the loss to decrease further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# THOUGHT QUESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer: No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate and Batch Size #\n",
    "\n",
    "Let's see how the learning rate and batch size affect how the training proceeds.\n",
    "\n",
    "### 3) Observe changes in the loss curve\n",
    "\n",
    "Change the values for `learning_rate` and `batch_size` and then run the cell. Pay attention to how the loss curve changes. Try the following combinations:\n",
    "\n",
    "| `learning_rate` | `batch_size` |\n",
    "|-----------------|--------------|\n",
    "| 0.01            | 128          |\n",
    "| 0.0001          | 128          |\n",
    "| 1.0             | 128          |\n",
    "| 0.01            | 8            |\n",
    "| 0.01            | 1024         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "learning_rate = 0.01\n",
    "batch_size = 2048\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------#\n",
    "bias_init = keras.initializers.constant(y_train.median()) # you can ignore!\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),    \n",
    "    layers.Dense(1, bias_initializer=bias_init)\n",
    "                 \n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='mae'\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=100,\n",
    "    verbose=0, # turn off output\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[0:, 'loss'].plot()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What effect did changing the learning rate have? What effect does changing the batch size have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Thought Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py,md,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
