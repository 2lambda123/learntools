{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--TITLE: Improving Optimization-->\n",
    "# Introduction #\n",
    "\n",
    "# The SGD Family #\n",
    "- momentum\n",
    "- Adam\n",
    "- mention others\n",
    "\n",
    "Adam and RMSProp are both good general-purpose optimizers. They tend to perform well on most problems and don't usually require much hyperparameter tuning. On some problems, they can be outperformed by ordinary SGD can sometimes outperform them, though it will often require more tuning.\n",
    "\n",
    "In the `tf.keras.optimizers` module, Keras includes a number of other methods worth exploring. As you expand your deep learning knowledge, it could be worth your time to become familiar with them. It seems that many practitioners settle on a few favorites they understand well.\n",
    "\n",
    "# Batch N"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
