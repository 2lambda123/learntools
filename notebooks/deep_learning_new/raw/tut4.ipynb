{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TITLE: Underfitting and Overfitting -->\n",
    "\n",
    "- [ ] Illustration: Learning curves\n",
    "- [ ] Early Stopping\n",
    "- [ ] Adding Capacity\n",
    "- [ ] Animation: Underfitting\n",
    "- [ ] Animation: Fitting with more capacity\n",
    "- [ ] Example discussion\n",
    "- [ ] Conclusion\n",
    "\n",
    "# Introduction #\n",
    "\n",
    "Recall from the example in the previous lesson that Keras will keep a history of the training and validation loss over the epochs that it is training the model. In this lesson, we're going to learn how to interpret these learning curves and how we can use them to guide model development. In particular, we'll examine at the learning curves for evidence of *underfitting* and *overfitting* and look at a couple of strategies for correcting it.\n",
    "\n",
    "# Interpreting the Learning Curves #\n",
    "\n",
    "You might remember graphs like these from Intro to ML when you were choosing hyperperameters for a decision tree. The learning curves play an especially important role in deep learning, so let's take a moment to review.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.gif\" width=\"1200\" alt=\"A graph of training and validation loss.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Learning curves. Underfitting. Overfitting a little. Overfitting a lot.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "The first problem your model can have is underfitting the training data. **Underfitting** just means the network hasn't learned all it could have learned -- there's still good information in the training data the network didn't detect. The cause of underfitting is most often a model that's not flexible enough, that doesn't have enough *capacity*, in other words.\n",
    "\n",
    "The second problem your model can have is overfitting, which is when it leans spurious patterns from the training data that don't generalize. The gap between the curves for training loss and validation loss gives you an estimate of the prediction error the model has created by learning these spurious patterns -- that is, the gap gives you evidence of **overfitting**.\n",
    "\n",
    "Overfitting, in small amounts, isn't necessarily bad. As long as the validation loss keeps going down, you can be confident that the model is still learning \"good\" information, even if it happens to pick up some of the bad as well. Your best performing model will often have a bit of a gap remaining.\n",
    "\n",
    "It can happen though that, in its search to drive down the loss, the network will start *unlearning* the useful true patterns in preference for the false spurious patterns. It starts throwing out the good to make way for the bad. When this happens you need to take action.\n",
    "\n",
    "Let's look at a couple ways we can reach the kind of learning curves we want: *adding capacity* to fix underfitting and *early stopping* to fix overfitting.\n",
    "\n",
    "# Adding Capacity #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping #\n",
    "\n",
    "With the decision tree, you chose the number of nodes that would minimize validation loss. With SGD, you can choose the number of *epochs* that minimize the validation loss. Choosing the epochs that minimize training loss is called **early stopping**. It's a simple and effective technique that you should almost always use.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.png\" width=\"400\" alt=\" \">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Stop the training before the validation loss begins to rise.\n",
    "</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.png\" width=\"400\" alt=\" \">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>We stop the training when the curve achieves its best fit to the validation data. <strong>Left: </strong>Without early stopping. <strong>Right: </strong>With early stopping.\n",
    "</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Train a Model with Early Stopping ##\n",
    "\n",
    "*Red Wine* dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "red_wine = pd.read_csv('../input/dl-course-data/dl-course-data/red-wine.csv')\n",
    "\n",
    "# Create training and validation splits\n",
    "df_train = red_wine.sample(frac=0.7, random_state=0)\n",
    "df_valid = red_wine.drop(df_train.index)\n",
    "display(df_train.head(4))\n",
    "\n",
    "# Scale to [0, 1]\n",
    "max_ = df_train.max(axis=0)\n",
    "min_ = df_train.min(axis=0)\n",
    "df_train = (df_train - min_) / (max_ - min_)\n",
    "df_valid = (df_valid - min_) / (max_ - min_)\n",
    "\n",
    "# Split features and target\n",
    "X_train = df_train.drop('quality', axis=1)\n",
    "X_valid = df_valid.drop('quality', axis=1)\n",
    "y_train = df_train['quality']\n",
    "y_valid = df_valid['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a **callback**. A callback in Keras is just a function you want run every so often during training. Keras has [a variety of useful callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks) pre-defined, but you can [define your own](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback), too.\n",
    "\n",
    "Here's how to define the `EarlyStopping` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters say: \"If there hasn't been at least an improvement of 0.01 in the validation loss over 5 epochs, then stop the training and keep the best model we found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(1024, activation='relu', input_shape=[11]),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the callback as an argument in `fit` (you can have several, so put it in a list). Choose a large number of epochs when using early stopping, more than you'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=256,\n",
    "    epochs=500,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0,  # turn off training log\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
