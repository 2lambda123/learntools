{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TITLE: Underfitting and Overfitting -->\n",
    "\n",
    "- [x] Early Stopping\n",
    "- [x] Adding Capacity\n",
    "- [ ] Illustration: Learning curves\n",
    "- [ ] Animation: Underfitting\n",
    "- [ ] Animation: Fitting with more capacity\n",
    "- [ ] Example discussion\n",
    "- [ ] Conclusion\n",
    "\n",
    "# Introduction #\n",
    "\n",
    "Recall from the example in the previous lesson that Keras will keep a history of the training and validation loss over the epochs that it is training the model. In this lesson, we're going to learn how to interpret these learning curves and how we can use them to guide model development. In particular, we'll examine at the learning curves for evidence of *underfitting* and *overfitting* and look at a couple of strategies for correcting it.\n",
    "\n",
    "# Interpreting the Learning Curves #\n",
    "\n",
    "You might remember graphs like these from Intro to ML when you were choosing hyperperameters for a decision tree. The learning curves play an especially important role in deep learning, so let's take a moment to review.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.gif\" width=\"1200\" alt=\"A graph of training and validation loss.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Learning curves. Underfitting. Overfitting a little. Overfitting a lot.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "The first problem your model can have is underfitting the training data. **Underfitting** just means the network hasn't learned all it could have learned -- there's still good information in the training data the network didn't detect. The cause of underfitting is most often a model that's not flexible enough, that doesn't have enough *capacity*, in other words.\n",
    "\n",
    "The second problem your model can have is overfitting, which is when it leans spurious patterns from the training data that don't generalize. The gap between the curves for training loss and validation loss gives you an estimate of the prediction error the model has created by learning these spurious patterns -- that is, the gap gives you evidence of **overfitting**.\n",
    "\n",
    "Overfitting, in small amounts, isn't necessarily bad. As long as the validation loss keeps going down, you can be confident that the model is still learning \"good\" information, even if it happens to pick up some of the bad as well. Your best performing model will often have a bit of a gap remaining.\n",
    "\n",
    "It can happen though that, in its search to drive down the loss, the network will start *unlearning* the useful true patterns in preference for the false spurious patterns. It starts throwing out the good to make way for the bad. When this happens you need to take action.\n",
    "\n",
    "Let's look at a couple ways we can reach the kind of learning curves we want: *adding capacity* to fix underfitting and *early stopping* to fix overfitting.\n",
    "\n",
    "# Adding Capacity #\n",
    "\n",
    "A network's **capacity** refers to the size and complexity of the patterns it is able to learn. As a rule of thumb, more neural units in a network means more capacity. Underfitting occurs when a network lacks the capacity to learn all the useful information from the training data. The cure for underfitting, then, is to add more capacity.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.png\" width=\"400\" alt=\" \">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>This model underfits the training data.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "You can increase the capacity of a network either by making it *wider* (more units to existing layers) or by making it *deeper* (adding more layers). Wider networks have an easier time learning linear relationships, while deeper networks prefer nonlinear ones. We'll explore adding capacity to a network in the exercises.\n",
    "\n",
    "# Early Stopping #\n",
    "\n",
    "We mentioned that when a model begins overfitting on the training set, it is in danger of \"forgetting\" the useful information it has already learned, causing the validation loss to start increasing. To prevent this, we can simply stop the training whenever it seems the validation loss isn't decreasing anymore. We know then that the network has learned everything useful that it can from the training set. Interrupting the training this way is called **early stopping**.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.png\" width=\"400\" alt=\" \">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Stop the training before the validation loss begins to rise.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "One of the advantages of early stopping is that it gives you some leeway in increasing capacity. It won't matter so much if the model is a bit too big, since you can stop the training before anything bad happens. Early stopping, however, isn't the last word in overfitting. As we'll see, it can still be worthwhile to \"close the gap.\" A large gap between training and validation loss can mean there is still useful information in the training set that the model hasn't learned. We'll explore this in the exercises and in the next lesson.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.png\" width=\"400\" alt=\" \">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>We stop the training when the curve achieves its best fit to the validation data. <strong>Left: </strong>Without early stopping. <strong>Right: </strong>With early stopping.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "## Adding Early Stopping ##\n",
    "\n",
    "In Keras, we include early stopping in our training through a callback. A **callback** is just a function you want run every so ofter while the network trains. The early stopping callback will run after every epoch. (Keras has [a variety of useful callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks) pre-defined, but you can [define your own](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback), too.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters say: \"If there hasn't been at least an improvement of 0.01 in the validation loss over the previous 5 epochs, then stop the training and keep the best model you found.\" As we'll see in our example, we'll pass this callback to the `fit` method along with the loss and optimizer.\n",
    "\n",
    "# Example - Train a Model with Early Stopping #\n",
    "\n",
    "*Red Wine* dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "red_wine = pd.read_csv('../input/dl-course-data/dl-course-data/red-wine.csv')\n",
    "\n",
    "# Create training and validation splits\n",
    "df_train = red_wine.sample(frac=0.7, random_state=0)\n",
    "df_valid = red_wine.drop(df_train.index)\n",
    "display(df_train.head(4))\n",
    "\n",
    "# Scale to [0, 1]\n",
    "max_ = df_train.max(axis=0)\n",
    "min_ = df_train.min(axis=0)\n",
    "df_train = (df_train - min_) / (max_ - min_)\n",
    "df_valid = (df_valid - min_) / (max_ - min_)\n",
    "\n",
    "# Split features and target\n",
    "X_train = df_train.drop('quality', axis=1)\n",
    "X_valid = df_valid.drop('quality', axis=1)\n",
    "y_train = df_train['quality']\n",
    "y_valid = df_valid['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(1024, activation='relu', input_shape=[11]),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the callback as an argument in `fit` (you can have several, so put it in a list). Choose a large number of epochs when using early stopping, more than you'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=256,\n",
    "    epochs=500,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0,  # turn off training log\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
