{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In this lesson, you'll learn how to diagnose **overfitting** and **underfitting** and learn some strategies to correct them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a trade-off between how flexible a machine learning model is and how in danger it is of overfitting.\n",
    "\n",
    "# Interpreting the Learning Curves #\n",
    "\n",
    "The error between the training set and the test set is the **generalization error**. This is a measure of what the model has learned about the training data that isn't true in general. For instance, it might come by chance that **TODO** cars in the training set which are blue and owned by someone whose name starts with 'N' all have especially low gas milage. **END** A neural network might learn this as a rule even though there's no real connection.\n",
    "\n",
    "The smaller a dataset is, the more likely are chance correspondences like this.\n",
    "\n",
    "# Early Stopping #\n",
    "\n",
    "If you've taken the Introduction to Machine Learning Course, you might remember learning about overfitting in Lesson 4, where you saw how to choose the parameters of a decision tree that gave you the best validation loss. You saw a curve like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which looks pretty much the same as the learning curves we've produced while training our neural nets.\n",
    "\n",
    "With the decision tree, you chose the number of nodes that would minimize validation loss. With SGD, you can choose the number of *epochs* that minimize the validation loss. Choosing the epochs that minimize training loss is called **early stopping**. It's a simple and effective technique that you should almost always use.\n",
    "\n",
    "## Example - Early Stopping in Keras ##\n",
    "\n",
    "We use a **callback**. A callback in Keras is just a function you want run every so often during training. Keras has [a variety of useful callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks) pre-defined, but you can [define your own](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback), too.\n",
    "\n",
    "Here's how to define the `EarlyStopping` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.01, # minimium amount of change to count as an improvement\n",
    "    patience=5, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters say: \"If there hasn't been at least an improvement of 0.01 in the validation loss over 5 epochs, then stop the training and keep the best model we found.\"\n",
    "\n",
    "We'll use it later when we train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout #\n",
    "\n",
    "You might be familiar with the Random Forest model. (You might have learned about it in Lesson **TODO: LESSON** of [Introduction to Machine Learning]()**TODO: LINK**.) Recall that a random forest creates an *ensemble* of decision trees by combining together the predictions of many individual trees.\n",
    "\n",
    "In effect, dropout creates an ensemble of sub-networks during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Train a Model with Regularization #\n",
    "\n",
    "# Conclusion #\n",
    "\n",
    "# Your Turn #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
