{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "*stochastic gradient descent*\n",
    "\n",
    "In deep learning, SGD isn't just a technical distraction. It's something that you have a great deal of control over, and your skill in controlling it will determine your success in real deep learning problems.\n",
    "\n",
    "The reason we need to know about SGD \n",
    "\n",
    "# The Loss Surface #\n",
    "\n",
    "Let's look at the training animation from Lesson 1 again:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/rFI1tIk.gif\" width=\"1600\" alt=\"Fitting a line batch by batch. The loss decreases and the weights approach their true values.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Training a neural network with Stochastic Gradient Descent.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Recall from Lesson 1 that to train a network, we use samples from the training data called *batches* (or *minibatches*) to make small shifts to the weights in the direction of their true values. This process is what we called *stochastic gradient descent*.\n",
    "\n",
    "To help us understand the SGD procedure we're going to plot something called a **loss surface**. The loss surface will show us exactly how the loss on the dataset depends on the weights we give the model. Let's see the training again, but this time with the loss surface.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/.gif\" width=\"800\" alt=\"The weights traverse the loss surface in the direction of their true values as the training progresses.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>The Loss Surface of the linear model\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "How are we to understand this loss surface? A point on the loss surface is a weight and a bias $(w, b)$. To get the loss value, plot the corresponding line and measure the error between the line and the points. That's the value of the contour.\n",
    "\n",
    "Do you see how the path the weights trace out during training corresponds to the fitted line? And look at the values on the contour lines. These are the loss values. Do you see how the path goes towards the minimum loss value as the line gets a better fit?\n",
    "\n",
    "The path the weights trace out is the **optimization path**.\n",
    "\n",
    "<!-- blockquote: contour plots -->\n",
    "\n",
    "# What's in a Name? #\n",
    "\n",
    "Let's use this loss surface to understand SGD.\n",
    "\n",
    "The **gradient** is simply a vector that tells us what direction the weights need to go. More precisely, it will tell us in what direction the loss is changing *fastest*. On the loss surface, this is where the contour lines are closest together. We call our process gradient **descent** because we use the gradient to *descend* the loss surface toward a minimum.\n",
    "\n",
    "<!-- FIGURE: the gradient along contour lines -->\n",
    "\n",
    "Now, the word **stochastic** simply means \"determined by chance.\" So in SGD, this just refers to our use of randomly sampled minibatches during training. If instead of random minibatches, we used the whole dataset at once, we would then have just ordinary gradient descent.\n",
    "\n",
    "And now you know what SGD means!\n",
    "\n",
    "# Batch Size and Learning Rate #\n",
    "\n",
    "When training a neural network, there are two parameters you need to choose that will have a large effect on the outcome: the *batch size* and the *learning rate*. These two parameters are what largely determine the behavior of the optimization path.\n",
    "\n",
    "The batch size, as you know, is how large of a sample each minibatch is. During SGD, we don't actually find the exact gradient vector. TensorFlow instead will *estimate* the gradient using a minibatch. If you've had some statistics, you probably know that the size of a sample determines how much noise there is in an estimate. So, size of the minibatch determines how much noise there is in the gradient.\n",
    "\n",
    "<!-- FIGURE: gradient noise and batch size -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Red Wine Quality #\n",
    "\n",
    "Show bad batch size/learning rate. Then show how to improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Schedules ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "# Your Turn #"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
