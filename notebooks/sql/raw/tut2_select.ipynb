{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT, FROM & WHERE\n",
    "\n",
    "SQL uses the keywords **SELECT**, **FROM** and **WHERE** to get data from a specific column based on conditions you specify. For this explanation, we'll use this imaginary database, `pet_records` which has just one table in it, called `pets`, which looks like this:\n",
    "\n",
    "![](https://i.imgur.com/Ef4Puo3.png)\n",
    "\n",
    "### SELECT ... FROM\n",
    "___\n",
    "\n",
    "The most basic SQL query is to select a single column from a specific table. To do this, you need to tell SELECT which column to select and then specify what table that column is from using from. \n",
    "\n",
    "> **Do you need to capitalize SELECT and FROM?** No, SQL doesn't care about capitalization. However, it's customary to capitalize your SQL commands and it makes your queries a bit easier to read.\n",
    "\n",
    "So, if we wanted to select the \"Name\" column from the pets table of the pet_records database (if that database were accessible as a BigQuery dataset on Kaggle , which it is not, because I made it up), we would do this:\n",
    "\n",
    "    SELECT Name\n",
    "    FROM `bigquery-public-data.pet_records.pets`\n",
    "\n",
    "Which would return the highlighted data from this figure.\n",
    "\n",
    "![](https://i.imgur.com/8FdVyFP.png)\n",
    "\n",
    "### WHERE ...\n",
    "___\n",
    "\n",
    "When you're working with BigQuery datasets, you'll usually want to return only certain rows, usually based on the value of a different column. You can do this using the WHERE clause, which will only return the rows where the WHERE clause evaluates to true.\n",
    "\n",
    "Let's look at an example:\n",
    "\n",
    "    SELECT Name\n",
    "    FROM `bigquery-public-data.pet_records.pets`\n",
    "    WHERE Animal = \"Cat\"\n",
    "\n",
    "This query will only return the entries from the \"Name\" column that are in rows where the \"Animal\" column has the text \"Cat\" in it. Those are the cells highlighted in blue in this figure:\n",
    "\n",
    "![](https://i.imgur.com/Va52Qdl.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: What are all the U.S. cities in the OpenAQ dataset?\n",
    "___\n",
    "\n",
    "Now that you've got the basics down, let's work through an example with a real dataset. Today we're going to be working with the OpenAQ dataset, which has information on air quality around the world. (The data in it should be current: it's updated weekly.)\n",
    "\n",
    "To help get you situated, I'm going to run through a complete query first. Then it will be your turn to get started running your queries!\n",
    "\n",
    "First, I'm going to set up everything we need to run queries and take a quick peek at what tables are in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import package with helper functions \n",
    "import bq_helper\n",
    "\n",
    "# create a helper object for this dataset\n",
    "open_aq = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",\n",
    "                                   dataset_name=\"openaq\")\n",
    "\n",
    "# print all the tables in this dataset (there's only one!)\n",
    "open_aq.list_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to take a peek at the first couple of rows to help me see what sort of data is in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the first couple rows of the \"global_air_quality\" dataset\n",
    "open_aq.head(\"global_air_quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, everything looks good! Now that I'm set up, I'm going to put together a query. I want to select all the values from the \"city\" column for the rows there the \"country\" column is \"us\" (for \"United States\"). \n",
    "\n",
    "> **What's up with the triple quotation marks (\"\"\")?** These tell Python that everything inside them is a single string, even though we have line breaks in it. The line breaks aren't necessary, but they do make it much easier to read your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query to select all the items from the \"city\" column where the\n",
    "# \"country\" column is \"us\"\n",
    "query = \"\"\"SELECT city\n",
    "            FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "            WHERE country = 'US'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important:**  Note that the argument we pass to FROM is *not* in single or double quotation marks (' or \"). It is in backticks (\\`). If you use quotation marks instead of backticks, you'll get this error when you try to run the query: `Syntax error: Unexpected string literal` \n",
    "\n",
    "Now I can use this query to get information from our open_aq dataset. I'm using the `BigQueryHelper.query_to_pandas_safe()` method here because it won't run a query if it's larger than 1 gigabyte, which helps me avoid accidentally running a very large query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the query_to_pandas_safe will only return a result if it's less\n",
    "# than one gigabyte (by default)\n",
    "us_cities = open_aq.query_to_pandas_safe(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I've got a dataframe called us_cities, which I can use like I would any other dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What five cities have the most measurements taken there?\n",
    "us_cities.city.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Check the size of your query before you run it\n",
    "\n",
    "BigQuery datasets can be very large, and there are some restrictions on how much data you can access. \n",
    "\n",
    "**Each Kaggle user can scan 5TB every 30 days for free.  Once you hit that limit, you'll have to wait for it to reset.**\n",
    "\n",
    "Don't worry: we'll show you how to use that allotment efficiently so you don't hit your limit.\n",
    "\n",
    "The [biggest dataset currently on Kaggle](https://www.kaggle.com/github/github-repos) is 3 terabytes, so you can easily go past your 30-day limit in a couple queries.\n",
    "\n",
    "> **What's a query?** A query is small piece of SQL code that specifies what data would you like to scan from a databases, and how much of that data you would like returned. (Note that your quota is on data *scanned*, not the amount of data returned.)\n",
    "\n",
    "One way to help avoid this is to estimate how big your query will be before you actually execute it. You can do this with the `BigQueryHelper.estimate_query_size()` method. For the rest of this notebook, I'll be using an example query that finding the scores for every Hacker News post of the type \"job\". Let's see how much data it will scan if we actually ran it.\n",
    "\n",
    "---\n",
    "## Run the Query\n",
    "\n",
    "Now that we know how to check the size of the query (and make sure we're not scanning several terabytes of data!) we're ready to run our first query. You have two methods available to help you do this:\n",
    "\n",
    "* *`BigQueryHelper.query_to_pandas(query)`*: This method takes a query and returns a Pandas dataframe.\n",
    "* *`BigQueryHelper.query_to_pandas_safe(query, max_gb_scanned=1)`*: This method takes a query and returns a Pandas dataframe only if the size of the query is less than the upperSizeLimit (1 gigabyte by default). \n",
    "\n",
    "Here's an example of a query that is larger than the specified upper limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this query looks in the full table in the hacker_news\n",
    "# dataset, then gets the score column from every row where \n",
    "# the type column has \"job\" in it.\n",
    "query = \"\"\"SELECT score\n",
    "            FROM `bigquery-public-data.hacker_news.full`\n",
    "            WHERE type = \"job\" \"\"\"\n",
    "\n",
    "# check how big this query will be\n",
    "hacker_news.estimate_query_size(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this query if it's less than 100 MB\n",
    "hacker_news.query_to_pandas_safe(query, max_gb_scanned=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's an example where the same query returns a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the scores of job postings (if the \n",
    "# query is smaller than 1 gig)\n",
    "job_post_scores = hacker_news.query_to_pandas_safe(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this has returned a dataframe, we can work with it as we would any other dataframe. For example, we can get the mean of the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average score for job posts\n",
    "job_post_scores.score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Common Mistakes when Querying Big Datasets\n",
    "____\n",
    "\n",
    "\n",
    "* *Avoid using the asterisk *(**) in your queries.* The asterisk means “everything”. This may be okay with smaller datasets, but getting everything from a 4 terabyte dataset takes a long time and eats into your monthly usage limit.\n",
    "* *For initial exploration, look at just part of the table instead of the whole thing.* If you're just curious to see what data's in a table, preview it instead of scanning the whole table. The `BigQueryHelper.head()` method in our helper package does this. Like `head()` in Pandas or R, it returns just the first few rows for you to look at.\n",
    "* *Double-check the size of complex queries.* If you're planning on running what might be a large query, either estimate the size first or run it using the `BigQueryHelper.query_to_pandas_safe()` method.\n",
    "* *Be cautious about joining tables.* In particular, avoid joining a table with itself (i.e. a self-join) and try to avoid joins that return a table that's larger than the ones you're joining together. (You can double-check yourself by joining just the heads of the tables.)\n",
    "* *Don't rely on LIMIT*: One of the things that can be confusing when working with BigQuery datasets is the difference between the data you *scan* and the data you actually *get back* especially since it's the first one that actually counts against your quota. When you do something like select a column with LIMIT = 10, you'll only get 10 results back, but you'll scan the whole column (and that counts against your monthly usage limit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "\n",
    "Write your first SQL query and run it in BigQuery in **[this hands-on exercise](https://www.kaggle.com/dansbecker/exercise-using-select-from-where).**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "*This tutorial is part of the [SQL Series](https://www.kaggle.com/learn/sql) on Kaggle Learn.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
