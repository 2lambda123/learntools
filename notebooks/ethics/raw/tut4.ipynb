{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004205,
     "end_time": "2021-02-17T20:43:06.853100",
     "exception": false,
     "start_time": "2021-02-17T20:43:06.848895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "There are many different ways of defining what we might look for in a fair machine learning (ML) model.  For instance, say we're working with a model that approves (or denies) credit card applications.  Is it:\n",
    "- more fair if the approval rate is equal across genders, or is it \n",
    "- better if gender is removed from the dataset and hidden from the model?\n",
    "\n",
    "In this tutorial, we'll cover four criteria that we can use to decide if a model is fair.  Then, you'll apply what you've learned in a **[hands-on exercise](#$NEXT_NOTEBOOK_URL$)** where you'll run code to train several models and analyze fairness of each.  (*As for every lesson in this course, no prior coding experience is required.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.003012,
     "end_time": "2021-02-17T20:43:06.859709",
     "exception": false,
     "start_time": "2021-02-17T20:43:06.856697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Four fairness criteria\n",
    "\n",
    "These four fairness criteria are a useful starting point, but it's important to note that there are many more ways of formalizing fairness, which you are encouraged to [explore](https://arxiv.org/pdf/1710.03184.pdf).  \n",
    "\n",
    "Assume we're working with a model that selects individuals to receive some outcome.  For instance, the model could select people who should be approved for loan, accepted to a university, or offered a job opportunity.  (*So, we don't consider models that perform tasks like facial recognition or text generation, among other things.*)\n",
    "\n",
    "## 1. Demographic parity / statistical parity\n",
    "\n",
    "**Demographic parity** says the model is fair, if the composition of people who are selected by the model matches the group membership percentages of the applicants.  For instance, if 60% of university applicants are women, then 60% of the acceptances should be extended to women.\n",
    "\n",
    "## 2. Equal accuracy\n",
    "\n",
    "Alternatively, we could check that the model has **equal accuracy** for each group.  That is, the percentage of correct classifications (people who should be denied and are denied, and people who should be approved who are approved) should be same for each group.  If the model is 98% accurate for individuals in one group, it should be 98% accurate for other groups.  \n",
    "\n",
    "## 3. Equal opportunity\n",
    "\n",
    "**Equal opportunity** fairness ensures that the percentage of people who should be approved and are approved is the same for each group.  We refer to the proportion of people who should be approved by the model (\"positives\") that are correctly approved by the model as the **true positive rate** (TPR) or **sensitivity** of the model.\n",
    "\n",
    "## 4. Group unaware / \"Fairness through unawareness\"\n",
    "\n",
    "**Group unaware** fairness removes all group membership information from the dataset.  For instance, to ensure the model is fair to all genders, we can remove gender data.  To make the model treat different ages and ethnicities fairly, we can remove that information, too.\n",
    "\n",
    "One difficulty of applying this approach in practice is that one has to be careful to identify and remove proxies for the group membership data.  For instance, in cities that are racially segregated, zip code is a strong proxy for race.  That is, when the race data is removed, the zip code data should also be removed, or else the ML application may still be able to infer an individual's race from the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.002805,
     "end_time": "2021-02-17T20:43:06.865652",
     "exception": false,
     "start_time": "2021-02-17T20:43:06.862847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Example\n",
    "\n",
    "We'll work with a very small example to illustrate the differences between the four different types of fairness.  We'll use a **confusion matrix**, which is a common tool used to understand the performance of a ML model.  This tool is depicted in the example below, which depicts a model with 80% accuracy (since 8/10 people were correctly classified) and has an 83% true positive rate (since 5/6 \"positives\" were correctly classified).\n",
    "\n",
    "![](https://i.imgur.com/xFZG5fF.png)\n",
    "\n",
    "To understand how a model's performance varies across groups, we can construct a different confusion matrix for each group. \n",
    "\n",
    "The next image shows what the confusion matrices should look like, if the model satisfies **demographic parity** fairness. 10 people from each group applied (50% from Group A, and 50% from Group B). 14 people, also equally split across groups (50% from Group A, and 50% from Group B) were approved by the model.\n",
    "\n",
    "![](https://i.imgur.com/e32gcDh.png)\n",
    "\n",
    "Next, we can see how the confusion matrices might look for ****equal accuracy**** fairness. For each group, the model was 80% accurate.\n",
    "\n",
    "![](https://i.imgur.com/fIOJovc.png)\n",
    "\n",
    "For ****equal opportunity**** fairness, the TPR for each group should be the same; in the example below, it is 66% in each case.\n",
    "\n",
    "![](https://i.imgur.com/aInWboA.png)\n",
    "\n",
    "Note that ****group unaware**** fairness cannot be detected from the confusion matrix, and is more concerned with removing group membership information from the dataset.\n",
    "\n",
    "Take the time now to study these toy examples, and use it to build your intuition for the differences between the different types of fairness. How does the example change if Group A has double the number of applicants as Group B?\n",
    "\n",
    "Also note that none of the examples satisfy more than one type of fairness. For instance, the demographic parity example does not satisfy equal accuracy or equal opportunity. Take the time to verify this now. In practice, it is usually not possible to optimize a model for more than one type of fairness.  _So which fairness criterion should you select, if you can only satisfy one?_  As with most ethical questions, the correct answer is usually not straightforward, and picking a criterion should be a long conversation involving everyone on your team.\n",
    "\n",
    "\n",
    "# Learn more\n",
    "\n",
    "- Explore different types of fairness with an [interactive tool](http://research.google.com/bigpicture/attacking-discrimination-in-ml/).\n",
    "- You can read more about equal opportunity in [this blog post](https://ai.googleblog.com/2016/10/equality-of-opportunity-in-machine.html).\n",
    "- Analyze ML fairness with [this walkthrough](https://pair-code.github.io/what-if-tool/learn/tutorials/walkthrough/) of the What-If Tool, created by the People and AI Research (PAIR) team at Google. This tool allows you to quickly amend an ML model, once you've picked the fairness criterion that is best for your use case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.002878,
     "end_time": "2021-02-17T20:43:06.871617",
     "exception": false,
     "start_time": "2021-02-17T20:43:06.868739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Your turn\n",
    "\n",
    "Continue to the exercise to **[run code](#$NEXT_NOTEBOOK_URL$)** to train models and determine fairness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.26116,
   "end_time": "2021-02-17T20:43:07.485248",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-17T20:43:00.224088",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
